<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chatglm on Svtter's Blog</title><link>https://svtter.cn/tags/chatglm/</link><description>Recent content in Chatglm on Svtter's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 02 Nov 2025 15:26:10 +0800</lastBuildDate><atom:link href="https://svtter.cn/tags/chatglm/index.xml" rel="self" type="application/rss+xml"/><item><title>一个划算的 kilocode 使用方法</title><link>https://svtter.cn/p/%E4%B8%80%E4%B8%AA%E5%88%92%E7%AE%97%E7%9A%84-kilocode-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</link><pubDate>Sun, 02 Nov 2025 15:26:10 +0800</pubDate><guid>https://svtter.cn/p/%E4%B8%80%E4%B8%AA%E5%88%92%E7%AE%97%E7%9A%84-kilocode-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</guid><description>&lt;p&gt;众所周不知，我已经购买了 &lt;a class="link" href="https://svtter.cn/p/%E6%99%BA%E8%B0%B1-glm-4.5-%E5%9C%A8%E7%BC%96%E7%A8%8B%E6%96%B9%E9%9D%A2%E7%9A%84%E8%8B%A5%E5%B9%B2%E9%97%AE%E9%A2%98/" &gt;glm4.6 年付版本&lt;/a&gt;，但是体验下来编程能力着实一般。但是转念一想，呵，实际上可以用来读代码呀。&lt;/p&gt;
&lt;p&gt;因此使用 kilocode ask 模式用于读代码，节省了不少时间，速度还很快，也算是另一种回本。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E4%B8%80%E4%B8%AA%E5%88%92%E7%AE%97%E7%9A%84-kilocode-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/pics/blog-img.png"
width="924"
height="312"
srcset="https://svtter.cn/p/%E4%B8%80%E4%B8%AA%E5%88%92%E7%AE%97%E7%9A%84-kilocode-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/pics/blog-img_hu_ab26343998d848de.png 480w, https://svtter.cn/p/%E4%B8%80%E4%B8%AA%E5%88%92%E7%AE%97%E7%9A%84-kilocode-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/pics/blog-img_hu_aee8231ca3eb1da.png 1024w"
loading="lazy"
alt="ask mode"
class="gallery-image"
data-flex-grow="296"
data-flex-basis="710px"
&gt;&lt;/p&gt;
&lt;p&gt;在 ask mode 下，模型无法操作文件（偶尔也会有 kilocode 会有 bug, bug 可能导致 LLM 可以修改文件。但是近期未出现），因此不必担心 glm 瞎搞导致仓库出现问题。&lt;/p&gt;
&lt;p&gt;基于此，更进一步思考下，如果 claude code 用完了，怎么进一步 vibe coding 呢？&lt;/p&gt;
&lt;h2 id="有效利用弱模型的方法---在-ask-mode-中使用"&gt;有效利用弱模型的方法 - 在 ask mode 中使用
&lt;/h2&gt;&lt;p&gt;这就引出了我这两天尝试的一种新思路——效果很好—— &lt;a class="link" href="https://kilocode.ai/docs/basic-usage/using-modes#ask-mode" target="_blank" rel="noopener"
&gt;ask mode&lt;/a&gt; 采用 glm4.6，coding mode 采用 deepseek-chat。&lt;/p&gt;
&lt;p&gt;deepseek 能够较为严格的遵从指令，glm4.6 运行速度有保证。因此整个效率提升了。&lt;/p&gt;
&lt;p&gt;此外，由于 ask 模式下，glm4.6 可以更好的提取信息，不至于浪费。如果理解错误了，切换成 deepseek-reasoner 也不迟。&lt;/p&gt;
&lt;h3 id="关于-ask-mode"&gt;关于 Ask Mode
&lt;/h3&gt;&lt;p&gt;&lt;a class="link" href="https://kilocode.ai/docs/basic-usage/using-modes#ask-mode" target="_blank" rel="noopener"
&gt;Ask mode&lt;/a&gt; 限制了大模型对文件的操作。也就是说大模型只能 read file 但是不能 write file。&lt;/p&gt;
&lt;p&gt;通过并行读取，kilocode 可以一次性将多个文件提供给 LLM 进行代码阅读，从而加速开发者对代码进行理解。&lt;/p&gt;
&lt;p&gt;这些 ask 的内容可以作为 context 给模型。也就是说，如果你希望让模型更加了解代码，通过几次 ask，然后切换到 coding mode 来修改代码，可能会得到更好的结果。&lt;/p&gt;
&lt;h2 id="一次性达成率-once-finished-rate-ofr"&gt;一次性达成率 (Once Finished Rate, OFR)
&lt;/h2&gt;&lt;p&gt;我觉得对于一个 llm+coding agent 组合来说，一次性达成率 (OFR )至关重要。&lt;/p&gt;
&lt;p&gt;一次性达成率定义为：&lt;/p&gt;
$$ OFR = \frac{n_{f}}{N_{total}} $$&lt;p&gt;其中，$n_{f}$ 是一次性完成任务的数量，$N_{total}$是提出任务的总数量。在 coding benchmark 中，可以考虑观测 Pass@1 来衡量这个能力。&lt;/p&gt;
&lt;p&gt;也就是说，当我们提出问题的时候，coding agent 能够一次性解决问题的比例。&lt;/p&gt;
&lt;p&gt;比如 claude code + sonnet 组合，基本上是 OFR 是最高的。OFR 较高意味着用户不需要反复调试代码，只需要生成代码然后就可以投入到下一个 feature 的开发中。这是真正的高效和提效。&lt;/p&gt;
&lt;p&gt;相比之下，GLM4.6 的 OFR 可能不到其一半的程度。用户需要不停的针对生成的代码进行优化。如果使用 coding agent 本身进行优化，可能来来回回也解决不了问题。这种现象我称之为 echo（回音壁），或者中文形式“鬼打墙”。绕来绕去，仿佛不具备智能一般。如果是其他的模型，例如 deepseek，在经过几次提示之后，可以走出 echo 状态。但是 glm 不太行。&lt;/p&gt;
&lt;h3 id="为什么-glm-不行"&gt;为什么 glm 不行？
&lt;/h3&gt;&lt;p&gt;浅见：该问题是 glm 4.6 过量的使用了生成数据造成的，即模型过拟合了。这在视觉模型上较为常见——通过生成数据来拔高在某些评测指标上的性能，但却因为生成数据过多导致在实际问题上无法得到较高的性能。&lt;/p&gt;
&lt;h2 id="好文推荐ai-创造了更多的价值"&gt;好文推荐：AI 创造了更多的价值
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="link" href="https://a16z.com/the-trillion-dollar-ai-software-development-stack/" target="_blank" rel="noopener"
&gt;The Trillion Dollar AI Software Development Stack&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>[过期] 我现在更多的使用 GLM 4.6 了</title><link>https://svtter.cn/p/%E8%BF%87%E6%9C%9F-%E6%88%91%E7%8E%B0%E5%9C%A8%E6%9B%B4%E5%A4%9A%E7%9A%84%E4%BD%BF%E7%94%A8-glm-4.6-%E4%BA%86/</link><pubDate>Thu, 09 Oct 2025 15:36:00 +0800</pubDate><guid>https://svtter.cn/p/%E8%BF%87%E6%9C%9F-%E6%88%91%E7%8E%B0%E5%9C%A8%E6%9B%B4%E5%A4%9A%E7%9A%84%E4%BD%BF%E7%94%A8-glm-4.6-%E4%BA%86/</guid><description>&lt;img src="https://svtter.cn/p/%E8%BF%87%E6%9C%9F-%E6%88%91%E7%8E%B0%E5%9C%A8%E6%9B%B4%E5%A4%9A%E7%9A%84%E4%BD%BF%E7%94%A8-glm-4.6-%E4%BA%86/glm-vs-deepseek.svg" alt="Featured image of post [过期] 我现在更多的使用 GLM 4.6 了" /&gt;&lt;blockquote&gt;
&lt;p&gt;updated at: 2025-10-27
只有在很简单的任务上我才会使用 glm4.6。实际体验上，经常会出现小问题。例如使用 claude code 的时候无法更新文件。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;span class="lnt"&gt;2
&lt;/span&gt;&lt;span class="lnt"&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;● Update&lt;span class="o"&gt;(&lt;/span&gt;content/post/2025-10-24-我又买了-kimi-coding-plan/pics/bg.svg&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ⎿  Error editing file
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ⎿  Interrupted · What should Claude &lt;span class="k"&gt;do&lt;/span&gt; instead?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;这是近期使用 code agent 一些体会。&lt;/p&gt;
&lt;h2 id="模型对比"&gt;模型对比
&lt;/h2&gt;&lt;p&gt;在我的实际使用体验下来，相比于 DeepSeek v3.2，GLM 4.6 还是要更强一些。&lt;/p&gt;
&lt;p&gt;例如，我在 nextjs 的项目中，创建了配置 nextjs config -&amp;gt; baseUrl 192.168.2.14:8080，GLM 4.6 在不提供明确上下文的前提下，能识别这个预先配置，但是 DeepSeek v3.2 无法做到。&lt;/p&gt;
&lt;p&gt;但 GLM 4.6 并非全面领先的。在问题相对不确定的情况下，DeepSeek v3.2 更加保守，不会去突破我在任务完成时预先提出的限制。GLM 4.6 则视我的限制于无物，大胆的修改然后改崩。&lt;/p&gt;
&lt;h2 id="工具"&gt;工具
&lt;/h2&gt;&lt;p&gt;相比于在 claude code / cline 中使用 GLM 4.6，在 kilo code 中使用体验是最好的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kilo code 可以并行读取文件，cc 只能串行一个个读取&lt;/li&gt;
&lt;li&gt;kilo code 强制要求生成 plan，相比于 cc 对 big model 限制更多。&lt;/li&gt;
&lt;li&gt;可视化界面更好操作。我可以直接 ban 掉 python 命令（我需要执行 uv run 而不是直接执行 python 命令）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是 kilo code 本身也有问题。无法使用 input; http 类型的 mcpServer。这使得 web-search-prime 在 kilo code 上无法使用。&lt;/p&gt;
&lt;h2 id="相关阅读"&gt;相关阅读
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="link" href="https://atbug.com/budget-efficiency-kilo-code-choice/" target="_blank" rel="noopener"
&gt;预算有限，效率拉满：为什么 Kilo Code 成了我的首选 Coding Agent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://kilocode.ai/" target="_blank" rel="noopener"
&gt;kilo code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item></channel></rss>