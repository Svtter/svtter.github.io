<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on Svtter's Blog</title><link>https://svtter.cn/tags/llm/</link><description>Recent content in LLM on Svtter's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 23 Jan 2026 11:52:52 +0800</lastBuildDate><atom:link href="https://svtter.cn/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>大模型 Coding Plan 套餐的数学陷阱：并发限制下的承诺量能否兑现？</title><link>https://svtter.cn/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B-coding-plan-%E5%A5%97%E9%A4%90%E7%9A%84%E6%95%B0%E5%AD%A6%E9%99%B7%E9%98%B1%E5%B9%B6%E5%8F%91%E9%99%90%E5%88%B6%E4%B8%8B%E7%9A%84%E6%89%BF%E8%AF%BA%E9%87%8F%E8%83%BD%E5%90%A6%E5%85%91%E7%8E%B0/</link><pubDate>Fri, 23 Jan 2026 11:52:52 +0800</pubDate><guid>https://svtter.cn/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B-coding-plan-%E5%A5%97%E9%A4%90%E7%9A%84%E6%95%B0%E5%AD%A6%E9%99%B7%E9%98%B1%E5%B9%B6%E5%8F%91%E9%99%90%E5%88%B6%E4%B8%8B%E7%9A%84%E6%89%BF%E8%AF%BA%E9%87%8F%E8%83%BD%E5%90%A6%E5%85%91%E7%8E%B0/</guid><description>&lt;h2 id="前言"&gt;前言
&lt;/h2&gt;&lt;p&gt;最近，国内多家大模型厂商纷纷推出面向开发者的 Coding Plan 订阅套餐，主打&amp;quot;低价享受海量用量&amp;quot;，宣称每月仅需几十到几百元，即可获得&amp;quot;数百亿 tokens&amp;quot;的使用额度。&lt;/p&gt;
&lt;p&gt;听起来很美好，但作为一个习惯用数据说话的开发者，我决定算一笔账：&lt;strong&gt;在并发限制下，这些承诺的用量真的能用完吗？&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="典型套餐结构"&gt;典型套餐结构
&lt;/h2&gt;&lt;p&gt;以市面上常见的三档套餐为例：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;套餐&lt;/th&gt;
&lt;th&gt;月费&lt;/th&gt;
&lt;th&gt;承诺用量（每 5 小时）&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Lite&lt;/td&gt;
&lt;td&gt;~20 元&lt;/td&gt;
&lt;td&gt;约 120 次 prompts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pro&lt;/td&gt;
&lt;td&gt;~100 元&lt;/td&gt;
&lt;td&gt;约 600 次 prompts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Max&lt;/td&gt;
&lt;td&gt;~200 元&lt;/td&gt;
&lt;td&gt;约 2,400 次 prompts&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;官方还会补充说明：&amp;ldquo;每次 prompt 预计可调用模型 15-20 次，每月总计可用总量高达几十亿到数百亿 tokens。&amp;rdquo;&lt;/p&gt;
&lt;p&gt;看起来性价比爆表，但魔鬼藏在细节里。&lt;/p&gt;
&lt;h2 id="关键限制并发数"&gt;关键限制：并发数
&lt;/h2&gt;&lt;p&gt;大多数厂商的文档中会轻描淡写地提到：&amp;ldquo;套餐使用受到并发数（在途请求任务数量）的限制。&amp;rdquo;&lt;/p&gt;
&lt;p&gt;但具体是多少？往往不会明确告知。根据社区反馈和实测，典型的并发限制如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;套餐&lt;/th&gt;
&lt;th&gt;并发数（在途请求）&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Lite&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pro&lt;/td&gt;
&lt;td&gt;~4-5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Max&lt;/td&gt;
&lt;td&gt;~7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这个数字，直接决定了你的实际吞吐量上限。&lt;/p&gt;
&lt;h2 id="数学时间max-套餐能否用完-2400-prompts"&gt;数学时间：Max 套餐能否用完 2,400 prompts？
&lt;/h2&gt;&lt;p&gt;让我们以最高档的 Max 套餐为例，做一个简单的计算。&lt;/p&gt;
&lt;h3 id="已知条件"&gt;已知条件
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;承诺用量&lt;/strong&gt;：每 5 小时 2,400 次 prompts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;并发限制&lt;/strong&gt;：7&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;每 prompt 触发的模型调用次数&lt;/strong&gt;：15-20 次（官方数据）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型生成速度&lt;/strong&gt;：约 50-60 tokens/秒&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;5 小时 = 18,000 秒&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="计算过程"&gt;计算过程
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Step 1：估算单次 API 调用耗时&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一次完整的 API 调用包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入处理：~1 秒&lt;/li&gt;
&lt;li&gt;模型推理生成（假设输出 500 tokens）：500 ÷ 55 ≈ 9 秒&lt;/li&gt;
&lt;li&gt;网络往返延迟：~1 秒&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;合计：约 10-12 秒/次调用&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2：计算 5 小时内的最大调用次数&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;span class="lnt"&gt;2
&lt;/span&gt;&lt;span class="lnt"&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;最大调用次数 = 并发数 × (总时间 ÷ 单次耗时)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; = 7 × (18,000 ÷ 10)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; = 12,600 次
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Step 3：换算为 prompts 数量&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;按官方说法，每 prompt 触发 15-20 次调用：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;可完成 prompts = 12,600 ÷ 17.5 ≈ 720 次
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id="结论"&gt;结论
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;指标&lt;/th&gt;
&lt;th&gt;官方承诺&lt;/th&gt;
&lt;th&gt;并发上限&lt;/th&gt;
&lt;th&gt;达成率&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;每 5 小时 prompts&lt;/td&gt;
&lt;td&gt;2,400&lt;/td&gt;
&lt;td&gt;~720&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;30%&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;即使在理想条件下，Max 套餐的实际可用量也只有承诺的 30% 左右。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="更残酷的现实agent-模式下的调用膨胀"&gt;更残酷的现实：Agent 模式下的调用膨胀
&lt;/h2&gt;&lt;p&gt;上面的计算还是基于官方宣称的&amp;quot;每 prompt 15-20 次调用&amp;quot;。但在实际的 AI Coding Agent（如 Claude Code、Cline 等）场景中，情况要糟糕得多。&lt;/p&gt;
&lt;h3 id="agent-模式的工作方式"&gt;Agent 模式的工作方式
&lt;/h3&gt;&lt;p&gt;当你给 AI 编程助手一个任务时，它通常会：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;分析需求，制定计划&lt;/li&gt;
&lt;li&gt;读取相关文件（每个文件可能触发一次调用）&lt;/li&gt;
&lt;li&gt;编写代码&lt;/li&gt;
&lt;li&gt;运行测试&lt;/li&gt;
&lt;li&gt;发现错误，修复&lt;/li&gt;
&lt;li&gt;重复 3-5 直到成功&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一个看似简单的 prompt，在 Agent 循环中可能触发 &lt;strong&gt;50-100+ 次模型调用&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id="实测案例"&gt;实测案例
&lt;/h3&gt;&lt;p&gt;有用户反馈：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;2 条简单 prompt，80 秒，消耗 38M Tokens，用掉 97% 的 5 小时限额&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;反推计算：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个 prompt 消耗约 19M tokens&lt;/li&gt;
&lt;li&gt;如果按 128K 上下文计算，相当于 &lt;strong&gt;~127 次模型调用/prompt&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这比官方说的&amp;quot;15-20 次&amp;quot;高出 &lt;strong&gt;6-8 倍&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id="修正后的实际可用量"&gt;修正后的实际可用量
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;场景&lt;/th&gt;
&lt;th&gt;每 prompt 调用次数&lt;/th&gt;
&lt;th&gt;5 小时可用 prompts&lt;/th&gt;
&lt;th&gt;达成率&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;官方理想值&lt;/td&gt;
&lt;td&gt;17.5&lt;/td&gt;
&lt;td&gt;720&lt;/td&gt;
&lt;td&gt;30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;轻度使用&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;252&lt;/td&gt;
&lt;td&gt;10.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;中度使用&lt;/td&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;168&lt;/td&gt;
&lt;td&gt;7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;重度 Agent&lt;/td&gt;
&lt;td&gt;100+&lt;/td&gt;
&lt;td&gt;&amp;lt;126&lt;/td&gt;
&lt;td&gt;&amp;lt;5%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="为什么会这样"&gt;为什么会这样？
&lt;/h2&gt;&lt;h3 id="1-token-计算包含-context"&gt;1. Token 计算包含 Context
&lt;/h3&gt;&lt;p&gt;大模型的 token 消耗不仅仅是输出，还包括输入。在 Coding 场景下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每次调用都要发送完整的对话历史&lt;/li&gt;
&lt;li&gt;代码项目的上下文动辄几十 K tokens&lt;/li&gt;
&lt;li&gt;128K 上下文窗口意味着每次调用可能消耗 100K+ tokens&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="2-并发是硬约束"&gt;2. 并发是硬约束
&lt;/h3&gt;&lt;p&gt;无论你的套餐额度有多大，并发数决定了单位时间内的最大吞吐量。这是一个&lt;strong&gt;物理瓶颈&lt;/strong&gt;，不是商业策略能绕过的。&lt;/p&gt;
&lt;h3 id="3-承诺基于理想假设"&gt;3. 承诺基于理想假设
&lt;/h3&gt;&lt;p&gt;厂商的宣传数字，往往基于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每次调用只用很小的 context&lt;/li&gt;
&lt;li&gt;每个 prompt 只触发少量调用&lt;/li&gt;
&lt;li&gt;用户不会连续高强度使用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但这些假设在真实的 AI Coding 场景中几乎不成立。&lt;/p&gt;
&lt;h2 id="一张表看清真相"&gt;一张表看清真相
&lt;/h2&gt;&lt;p&gt;以 Max 套餐（~200 元/月）为例：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;指标&lt;/th&gt;
&lt;th&gt;官方宣传&lt;/th&gt;
&lt;th&gt;理论上限&lt;/th&gt;
&lt;th&gt;实际预期&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;每 5 小时 prompts&lt;/td&gt;
&lt;td&gt;2,400&lt;/td&gt;
&lt;td&gt;720&lt;/td&gt;
&lt;td&gt;150-400&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;每月 prompts&lt;/td&gt;
&lt;td&gt;345,600&lt;/td&gt;
&lt;td&gt;103,680&lt;/td&gt;
&lt;td&gt;21,600-57,600&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;每月 tokens&lt;/td&gt;
&lt;td&gt;&amp;ldquo;数百亿&amp;rdquo;&lt;/td&gt;
&lt;td&gt;~100 亿&lt;/td&gt;
&lt;td&gt;10-30 亿&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;达成率&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;30%&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;5-17%&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="给开发者的建议"&gt;给开发者的建议
&lt;/h2&gt;&lt;h3 id="1-别被数百亿-tokens忽悠"&gt;1. 别被&amp;quot;数百亿 tokens&amp;quot;忽悠
&lt;/h3&gt;&lt;p&gt;Token 数量是一个极具误导性的指标。在 Coding Agent 场景下，context 占了大头，真正有效的输出 tokens 可能只有 1-5%。&lt;/p&gt;
&lt;h3 id="2-关注并发数"&gt;2. 关注并发数
&lt;/h3&gt;&lt;p&gt;这才是决定实际体验的核心指标。如果厂商不公开并发限制，大概率是因为数字不好看。&lt;/p&gt;
&lt;h3 id="3-计算单-prompt-成本"&gt;3. 计算单 prompt 成本
&lt;/h3&gt;&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;实际单 prompt 成本 = 月费 ÷ 实际可用 prompts
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;以 Max 套餐为例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;官方宣传：200 ÷ 345,600 = 0.0006 元/prompt&lt;/li&gt;
&lt;li&gt;实际情况：200 ÷ 30,000 = &lt;strong&gt;0.007 元/prompt&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;差了 10 倍。&lt;/p&gt;
&lt;h3 id="4-考虑按量付费"&gt;4. 考虑按量付费
&lt;/h3&gt;&lt;p&gt;如果你的使用量不大，按量付费可能比包月更划算。至少不会为&amp;quot;用不完的额度&amp;quot;买单。&lt;/p&gt;
&lt;h2 id="结语"&gt;结语
&lt;/h2&gt;&lt;p&gt;大模型 Coding Plan 套餐的出现本身是好事，降低了开发者使用 AI 编程助手的门槛。但在选择套餐时，请务必：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;要求厂商公开并发限制&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自己动手算一算吞吐量上限&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不要被&amp;quot;数百亿 tokens&amp;quot;的大数字迷惑&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;毕竟，&lt;strong&gt;承诺的用量用不完，就等于变相涨价&lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;本文基于公开信息和数学推导，具体数值可能因厂商调整而变化。建议读者以实测为准。&lt;/em&gt;&lt;/p&gt;</description></item><item><title>高效省钱：我的 AI Agent 工作流选择</title><link>https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/</link><pubDate>Mon, 05 Jan 2026 16:00:00 +0800</pubDate><guid>https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/</guid><description>&lt;img src="https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/featured-image.jpg" alt="Featured image of post 高效省钱：我的 AI Agent 工作流选择" /&gt;&lt;p&gt;Claude Code 一个月 $100 费用有些高，很多朋友都有点扛不住。为了解决问题，我实践了一套工作流。&lt;/p&gt;
&lt;p&gt;模型方面，我的建议是，选择 &lt;code&gt;Gemini 3 Flash&lt;/code&gt; 的按需用量作为替代。&lt;/p&gt;
&lt;p&gt;原因：Gemini 3 Flash 性价比极高；响应速度快、处理效率高，价格仅为 Opus 和 Sonnet 的几分之一。对于大多数任务，Flash 版本已经足够使用。&lt;/p&gt;
&lt;h2 id="省钱的-workflow"&gt;省钱的 Workflow
&lt;/h2&gt;&lt;p&gt;一个经济实惠的工作流：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;制定计划&lt;/strong&gt;：使用 Gemini 3 Flash&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;执行构建&lt;/strong&gt;：使用 OpenCode 提供的免费 GLM 4.7（或 MiniMax M2.1）。亦或者你已经购买了 &lt;a class="link" href="https://svtter.cn/p/%E6%99%BA%E8%B0%B1-glm-4.5-%E5%9C%A8%E7%BC%96%E7%A8%8B%E6%96%B9%E9%9D%A2%E7%9A%84%E8%8B%A5%E5%B9%B2%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener"
&gt;Zhipu Coding Plan&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;提到 Gemini 3 就不能不提 GPT-5.2。&lt;/p&gt;
&lt;p&gt;首先，部分工程师不使用 coding agent，而是使用直接使用 ChatGPT.com。这种使用方式且不论是否高效，可靠性就令人担忧。从实际体验来看，GPT-5.2 的回复语气做了拟人化微调，过于迎合用户。虽然可以调整语气，但对于专业开发者来说可能不是最佳选择。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/pics/image_1767597061665_0.png"
width="1023"
height="930"
srcset="https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/pics/image_1767597061665_0_hu_c015f16c1d892d52.png 480w, https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/pics/image_1767597061665_0_hu_474d285f9e3620e9.png 1024w"
loading="lazy"
alt="GPT-5.2 回复语气截图"
class="gallery-image"
data-flex-grow="110"
data-flex-basis="264px"
&gt;&lt;/p&gt;
&lt;p&gt;此外，尽管 GPT-5.2 在 SWE-bench veried 上得到了较好的性能，但是我实际体验不好。这就不得不说一下 SWE-bench 的历史：&lt;/p&gt;
&lt;p&gt;SWE-bench 最初由&lt;strong&gt;普林斯顿大学&lt;/strong&gt;团队提出（ICLR 2024），用于评估语言模型解决真实 GitHub 问题的能力。&lt;/p&gt;
&lt;p&gt;但问题在于：2024年8月，OpenAI 的 Preparedness 团队与原作者合作，推出了 &lt;strong&gt;SWE-bench Verified&lt;/strong&gt;（500个经过人工验证的问题子集）。由于 OpenAI 参与了这个新版本 benchmark 的&lt;strong&gt;设计&lt;/strong&gt;，因此在这个 benchmark 下测试的 OpenAI 模型性能值得怀疑。这并不一定是主观引入性能优化，但是 Bias 在这种情况下，极有可能存在。&lt;/p&gt;
&lt;p&gt;还是那句话：从实际使用上来看，codex 总不能带来特别理想的结果。&lt;/p&gt;
&lt;h2 id="一些-opencode-技巧"&gt;一些 opencode 技巧
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;通过 OpenCode 使用 Agent&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;OpenCode 支持启动 SubAgent。调试前后端代码时，可以让 OpenCode 在不同目录启动 Agent，有效避免权限问题。&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;OpenSpec：跨 Agent 共享规范&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;span class="lnt"&gt;2
&lt;/span&gt;&lt;span class="lnt"&gt;3
&lt;/span&gt;&lt;span class="lnt"&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;1. OpenCode + Gemini 3 Flash → 生成 proposal
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;2. Codex → 代码审查
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;3. Claude Code → 再次审查
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;4. OpenSpec Apply → 最终执行
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;OpenSpec 生成 spec 是靠谱的，但是有时候便宜模型代码质量不行。这个时候可以多生成几次，使用 spec 生成多次，从中挑选最好的结果。&lt;/p&gt;
&lt;p&gt;最后，如果从 PR 看到确实不行，仍可以继续采用 sonnet 4.5 来实现代码。&lt;/p&gt;
&lt;h2 id="思考"&gt;思考
&lt;/h2&gt;&lt;p&gt;作为 Agent 工程师，我们需要基于以下趋势做决策：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;模型会越来越聪明&lt;/li&gt;
&lt;li&gt;执行速度会越来越快&lt;/li&gt;
&lt;li&gt;价格会越来越便宜&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;虽然这是大趋势，但在实际任务中仍需平衡计算速度、成本和最终效果。也许很快就会出现能够自动平衡这些因素的 Agent 系统，但是现阶段考虑这些问题，没错。&lt;/p&gt;</description></item><item><title>编码性能与模型性价比分析</title><link>https://svtter.cn/p/%E7%BC%96%E7%A0%81%E6%80%A7%E8%83%BD%E4%B8%8E%E6%A8%A1%E5%9E%8B%E6%80%A7%E4%BB%B7%E6%AF%94%E5%88%86%E6%9E%90/</link><pubDate>Sat, 03 Jan 2026 00:00:00 +0000</pubDate><guid>https://svtter.cn/p/%E7%BC%96%E7%A0%81%E6%80%A7%E8%83%BD%E4%B8%8E%E6%A8%A1%E5%9E%8B%E6%80%A7%E4%BB%B7%E6%AF%94%E5%88%86%E6%9E%90/</guid><description>&lt;img src="https://svtter.cn/p/%E7%BC%96%E7%A0%81%E6%80%A7%E8%83%BD%E4%B8%8E%E6%A8%A1%E5%9E%8B%E6%80%A7%E4%BB%B7%E6%AF%94%E5%88%86%E6%9E%90/pics/bg-new-v2.jpg" alt="Featured image of post 编码性能与模型性价比分析" /&gt;&lt;p&gt;这是我对几个模型的编码性能与性价比分析报告，用于对比不同模型在编码任务上的表现和成本效益，以便选择最合适的模型。&lt;/p&gt;
&lt;iframe src="model-comparison.pdf" style="width:100%; height:85vh; border:0;"&gt;&lt;/iframe&gt;
&lt;p&gt;中文显然使用 GLM 4.7 是比较划算的。 2000 人民币的价格，基本上包年处理了。
缺点是高峰期使用，即便是企业 MAX 版本也会很慢。&lt;/p&gt;
&lt;p&gt;从我的实际体验上看，minimax m2.1 的能力是要远远超过 GLM 4.7 的。&lt;/p&gt;</description></item><item><title>第三方客户端与大模型 API 结合 -- 性能小测</title><link>https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/</link><pubDate>Wed, 19 Nov 2025 17:03:18 +0800</pubDate><guid>https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/</guid><description>&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/bg.jpg" alt="Featured image of post 第三方客户端与大模型 API 结合 -- 性能小测" /&gt;&lt;p&gt;最近一年里，我尝试使用 &lt;a class="link" href="https://github.com/ThinkInAIXYZ/deepchat" target="_blank" rel="noopener"
&gt;deepchat&lt;/a&gt; 和 大模型 API（例如 k2 thinking turbo) 来构成一个相对私有化的聊天工具（或者说 agent 助手），来处理一些私有化的数据。但是，总的来说体验不是很好。大模型答不对。&lt;/p&gt;
&lt;p&gt;搜索方面，我使用了 bocha api，重置了 10 块，来为大模型提供搜索能力。&lt;/p&gt;
&lt;h2 id="测试的问题"&gt;测试的问题
&lt;/h2&gt;&lt;p&gt;我感觉上下文能力（单一聊天框内）还是有点问题。我简单测试了这个问题：&lt;code&gt;硅基流动上，最贵的模型是哪一个？&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;答案是：&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/answ.png"
width="1200"
height="832"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/answ_hu_b24caa1450c981f6.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/answ_hu_c39dd9490283c1be.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
&gt;&lt;/p&gt;
&lt;h2 id="kimi-k2-thinking-turbo"&gt;Kimi k2 thinking turbo
&lt;/h2&gt;&lt;p&gt;首先是 deepchat:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171204_032.png"
width="2091"
height="1587"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171204_032_hu_7db667b7f9d9df7b.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171204_032_hu_88d3f5743cb65223.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="131"
data-flex-basis="316px"
&gt;&lt;/p&gt;
&lt;p&gt;emm，不对。&lt;/p&gt;
&lt;p&gt;然后是 kimi official:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171256_940.png"
width="2163"
height="1911"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171256_940_hu_c84b877eeaffa92d.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171256_940_hu_434f4701e97c5241.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="271px"
&gt;&lt;/p&gt;
&lt;p&gt;也不对。&lt;/p&gt;
&lt;h2 id="试试-deepseek"&gt;试试 deepseek
&lt;/h2&gt;&lt;p&gt;先试试客户端。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds-dc.png"
width="2001"
height="1509"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds-dc_hu_4b496410369e3b3a.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds-dc_hu_e711c6410482af7e.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="132"
data-flex-basis="318px"
&gt;&lt;/p&gt;
&lt;p&gt;不对。&lt;/p&gt;
&lt;p&gt;再试试 deepseek official。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds.png"
width="1260"
height="1538"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds_hu_449973d3bc8fb8ab.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds_hu_a4b89e87924b34eb.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="81"
data-flex-basis="196px"
&gt;&lt;/p&gt;
&lt;p&gt;很接近，答案也靠谱了。但是可惜，也不对。&lt;/p&gt;
&lt;h2 id="如果直接问-chatgpt"&gt;如果直接问 chatgpt
&lt;/h2&gt;&lt;p&gt;嘶，有点离谱。让我们试试 gpt-5。&lt;/p&gt;
&lt;p&gt;prompt:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;span class="lnt"&gt;2
&lt;/span&gt;&lt;span class="lnt"&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;硅基流动上，最贵的模型是哪一个？
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;我指的是 siliconflow.cn
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;帮我看看
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/wechat_2025-11-19_171536_131.png"
width="1275"
height="1616"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/wechat_2025-11-19_171536_131_hu_479ab0fea5b53f5f.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/wechat_2025-11-19_171536_131_hu_33adf58eab061c65.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="78"
data-flex-basis="189px"
&gt;&lt;/p&gt;
&lt;h2 id="推断-性能不好的原因"&gt;推断-性能不好的原因
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;搜索能力不足。博查 API 背锅。&lt;/li&gt;
&lt;li&gt;不同模型，最佳表现的超参数可能不一样。我调用的是硅基流动的大模型 API。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="结论"&gt;结论
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;单从这个问题上看，还是 chatgpt 强一些。相比之前，官方的搜索+模型，似乎性能也会更好一些。因此，如果不是特别隐私的数据，还是用官方的比较好。&lt;/li&gt;
&lt;li&gt;本文仅供参考，看个乐子。&lt;/li&gt;
&lt;/ul&gt;</description></item></channel></rss>