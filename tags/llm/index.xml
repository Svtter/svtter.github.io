<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on Svtter's Blog</title><link>https://svtter.cn/tags/llm/</link><description>Recent content in LLM on Svtter's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 10 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://svtter.cn/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Swarm Coding：大模型编程的下一步</title><link>https://svtter.cn/p/swarm-coding%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BC%96%E7%A8%8B%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/</link><pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate><guid>https://svtter.cn/p/swarm-coding%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BC%96%E7%A8%8B%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/</guid><description>&lt;blockquote&gt;
&lt;p&gt;大模型编程的下一步，是 swarm coding。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id="工具体验"&gt;工具体验
&lt;/h2&gt;&lt;p&gt;目前使用过的工具中，oh my opencode 是一个典型的 swarm 框架，基于 opencode。花了约 $400 体验下来，效果一般——对任务的拆分粒度和跨 agent 的分发调度做得不够好。Agentless [3] 从反面印证了这一体感：简单的三阶段方法有时可以媲美复杂的 agent 架构，复杂未必更好。&lt;/p&gt;
&lt;p&gt;claude team 还没尝试，暂时不打算吃这个螃蟹。&lt;/p&gt;
&lt;h2 id="核心观点控制交付"&gt;核心观点：控制交付
&lt;/h2&gt;&lt;p&gt;随着大模型能力的提升，写代码的基本能力已经可以淘汰 2024 年的初级程序员。那么下一步是什么？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;大模型的新阶段，说白了，就是&lt;strong&gt;控制交付&lt;/strong&gt;。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;想让大模型更好地运作，靠的不是给它更多自由度，而是用&lt;strong&gt;确定性的流程&lt;/strong&gt;去约束它的行为，做强制检查。MetaGPT [1] 将标准化操作流程（SOP）编码为 prompt 序列来编排多角色协作，正是这一思路的学术实践；CodeR [8] 提出的 task graph 也证明了前置规划优于即时决策。&lt;/p&gt;
&lt;p&gt;具体做法包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过 &lt;a class="link" href="https://svtter.cn/p/%E4%BD%BF%E7%94%A8-opencode--glm-4.7-%E5%AE%9E%E7%8E%B0-github-pr-%E8%87%AA%E5%8A%A8%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5/" target="_blank" rel="noopener"
&gt;opencode 自动化检查&lt;/a&gt; 隔离上下文&lt;/li&gt;
&lt;li&gt;使用 ruff、pytest 等工具控制代码质量——AgentCoder [10] 的实验表明，将代码生成与测试执行分离到不同 agent 后，质量显著提升&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外，&lt;strong&gt;大规模强力小模型 + 聪明的 orchestrator 大模型&lt;/strong&gt;来规划任务和隔离上下文，也是一个很有潜力的方向。SwarmAgentic [5] 将粒子群优化改造为文本符号更新，实现了全自动的多 agent 架构生成；AgentCoder [10] 的程序员+测试设计+测试执行三 agent 框架也是典型实例。&lt;/p&gt;
&lt;h2 id="相关研究"&gt;相关研究
&lt;/h2&gt;&lt;p&gt;&lt;a class="link" href="https://hy.tencent.com/research/100025?langVersion=zh" target="_blank" rel="noopener"
&gt;姚顺雨提出的 context 学习&lt;/a&gt;很有启发，本质上也是迁移学习的一部分。SWE-agent [11] 在此基础上定义了 Agent-Computer Interface，确立了 SWE-bench 作为编码 agent 的评测标准。&lt;/p&gt;
&lt;p&gt;在多 agent 协同编程领域，ChatDev [2] 通过 &amp;ldquo;chat chain&amp;rdquo; 让 agent 在软件生命周期各阶段对话协作；MapCoder [9] 用四个 agent 模拟人类编程周期，HumanEval pass@1 达到 93.9%；MAGIS [4] 将多 agent 应用于 GitHub Issue 解决，分辨率比直接用 GPT-4 高 8 倍。&lt;/p&gt;
&lt;p&gt;在蜂群智能方向，Model Swarms [6] 让多个 LLM 在权重空间中协同搜索，仅需 200 个样本即可免微调适应；Multi-scale Swarm [7] 提出多尺度蜂群框架用于代码生成，HumanEval 达 86.6%。&lt;/p&gt;
&lt;p&gt;He et al. [15] 的综述系统审视了 41 篇 LMA（LLM-based Multi-Agent）论文，提出产品负责人/开发者/QA/管理者四角色框架，是该领域最全面的文献梳理。&lt;/p&gt;
&lt;h2 id="如何评价蜂群能力"&gt;如何评价蜂群能力？
&lt;/h2&gt;&lt;p&gt;目前大部分 swarm 产品还处于比较初级的阶段，尚未看到公开的、明确的评测标准。MultiAgentBench [12] 已开始用里程碑式 KPI 评测多 agent 协作质量，但整体仍不成熟；KDD 2025 的评测综述 [13] 提出了二维分类法（评测目标 × 评测流程），覆盖了企业级可靠性与合规性。&lt;/p&gt;
&lt;p&gt;我认为蜂群能力的评价应包含以下维度：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;任务交付能力&lt;/strong&gt;——最终产出是否满足需求&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;交付时间&lt;/strong&gt;——端到端完成耗时&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;任务拆分程度&lt;/strong&gt;——子任务划分的合理性&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;并行效果&lt;/strong&gt;——多 agent 并发带来的加速比&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;token 消耗（cost）&lt;/strong&gt;——完成任务的总成本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;尤其关键的是：在用户尚未给出明确交付指标时，蜂群系统能否自行分析任务并推导出合理的交付标准。&lt;/p&gt;
&lt;p&gt;当然，现有产品内部也不见得没有这些标准，可能只是尚未公开。&lt;/p&gt;
&lt;h2 id="参考文献"&gt;参考文献
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;[1] Hong et al. &lt;em&gt;MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework.&lt;/em&gt; ICLR 2024 (Oral). &lt;a class="link" href="https://arxiv.org/abs/2308.00352" target="_blank" rel="noopener"
&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[2] Qian et al. &lt;em&gt;ChatDev: Communicative Agents for Software Development.&lt;/em&gt; ACL 2024. &lt;a class="link" href="https://aclanthology.org/2024.acl-long.810/" target="_blank" rel="noopener"
&gt;ACL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[3] Xia et al. &lt;em&gt;Agentless: Demystifying LLM-Based Software Engineering Agents.&lt;/em&gt; FSE 2025. &lt;a class="link" href="https://arxiv.org/abs/2407.01489" target="_blank" rel="noopener"
&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[4] Tao et al. &lt;em&gt;MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution.&lt;/em&gt; NeurIPS 2024. &lt;a class="link" href="https://proceedings.neurips.cc/paper_files/paper/2024/file/5d1f02132ef51602adf07000ca5b6138-Paper-Conference.pdf" target="_blank" rel="noopener"
&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[5] Zhang et al. &lt;em&gt;SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence.&lt;/em&gt; EMNLP 2025. &lt;a class="link" href="https://aclanthology.org/2025.emnlp-main.93/" target="_blank" rel="noopener"
&gt;ACL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[6] Feng et al. &lt;em&gt;Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence.&lt;/em&gt; ICML 2025. &lt;a class="link" href="https://arxiv.org/abs/2410.11163" target="_blank" rel="noopener"
&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[7] Tan et al. &lt;em&gt;Multi-scale Swarm of Large Language Models for Python Code Generation.&lt;/em&gt; ICSI 2025. &lt;a class="link" href="https://link.springer.com/chapter/10.1007/978-981-95-0982-9_14" target="_blank" rel="noopener"
&gt;Springer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[8] Chen et al. &lt;em&gt;CodeR: Issue Resolving with Multi-Agent and Task Graphs.&lt;/em&gt; arXiv 2024. &lt;a class="link" href="https://arxiv.org/abs/2406.01304" target="_blank" rel="noopener"
&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[9] Islam et al. &lt;em&gt;MapCoder: Multi-Agent Code Generation for Competitive Problem Solving.&lt;/em&gt; ACL 2024. &lt;a class="link" href="https://aclanthology.org/2024.acl-long.269/" target="_blank" rel="noopener"
&gt;ACL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[10] Huang et al. &lt;em&gt;AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation.&lt;/em&gt; arXiv 2024. &lt;a class="link" href="https://arxiv.org/abs/2312.13010" target="_blank" rel="noopener"
&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[11] Yang et al. &lt;em&gt;SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering.&lt;/em&gt; NeurIPS 2024. &lt;a class="link" href="https://arxiv.org/abs/2405.15793" target="_blank" rel="noopener"
&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[12] &lt;em&gt;MultiAgentBench: Evaluating the Collaboration and Competition of LLM Agents.&lt;/em&gt; arXiv 2025. &lt;a class="link" href="https://arxiv.org/abs/2503.01935" target="_blank" rel="noopener"
&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[13] Mohammadi et al. &lt;em&gt;Evaluation and Benchmarking of LLM Agents: A Survey.&lt;/em&gt; KDD 2025. &lt;a class="link" href="https://arxiv.org/abs/2507.21504" target="_blank" rel="noopener"
&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[14] &lt;em&gt;The Potential of LLMs in Automating Software Testing: From Generation to Reporting.&lt;/em&gt; arXiv 2025. &lt;a class="link" href="https://arxiv.org/abs/2501.00217" target="_blank" rel="noopener"
&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[15] He et al. &lt;em&gt;LLM-Based Multi-Agent Systems for Software Engineering: Literature Review, Vision, and the Road Ahead.&lt;/em&gt; ACM TOSEM 2025. &lt;a class="link" href="https://arxiv.org/abs/2404.04834" target="_blank" rel="noopener"
&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>大模型 Coding Plan 套餐的数学陷阱：并发限制下的承诺量能否兑现？</title><link>https://svtter.cn/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B-coding-plan-%E5%A5%97%E9%A4%90%E7%9A%84%E6%95%B0%E5%AD%A6%E9%99%B7%E9%98%B1%E5%B9%B6%E5%8F%91%E9%99%90%E5%88%B6%E4%B8%8B%E7%9A%84%E6%89%BF%E8%AF%BA%E9%87%8F%E8%83%BD%E5%90%A6%E5%85%91%E7%8E%B0/</link><pubDate>Fri, 23 Jan 2026 11:52:52 +0800</pubDate><guid>https://svtter.cn/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B-coding-plan-%E5%A5%97%E9%A4%90%E7%9A%84%E6%95%B0%E5%AD%A6%E9%99%B7%E9%98%B1%E5%B9%B6%E5%8F%91%E9%99%90%E5%88%B6%E4%B8%8B%E7%9A%84%E6%89%BF%E8%AF%BA%E9%87%8F%E8%83%BD%E5%90%A6%E5%85%91%E7%8E%B0/</guid><description>&lt;img src="https://svtter.cn/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B-coding-plan-%E5%A5%97%E9%A4%90%E7%9A%84%E6%95%B0%E5%AD%A6%E9%99%B7%E9%98%B1%E5%B9%B6%E5%8F%91%E9%99%90%E5%88%B6%E4%B8%8B%E7%9A%84%E6%89%BF%E8%AF%BA%E9%87%8F%E8%83%BD%E5%90%A6%E5%85%91%E7%8E%B0/cover.png" alt="Featured image of post 大模型 Coding Plan 套餐的数学陷阱：并发限制下的承诺量能否兑现？" /&gt;&lt;h2 id="前言"&gt;前言
&lt;/h2&gt;&lt;p&gt;最近，国内多家大模型厂商纷纷推出面向开发者的 Coding Plan 订阅套餐，主打&amp;quot;低价享受海量用量&amp;quot;，宣称每月仅需几十到几百元，即可获得&amp;quot;数百亿 tokens&amp;quot;的使用额度。&lt;/p&gt;
&lt;p&gt;听起来很美好，但作为一个习惯用数据说话的开发者，我决定算一笔账：&lt;strong&gt;在并发限制下，这些承诺的用量真的能用完吗？&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="典型套餐结构"&gt;典型套餐结构
&lt;/h2&gt;&lt;p&gt;以市面上常见的三档套餐为例：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;套餐&lt;/th&gt;
&lt;th&gt;月费&lt;/th&gt;
&lt;th&gt;承诺用量（每 5 小时）&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Lite&lt;/td&gt;
&lt;td&gt;~20 元&lt;/td&gt;
&lt;td&gt;约 120 次 prompts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pro&lt;/td&gt;
&lt;td&gt;~100 元&lt;/td&gt;
&lt;td&gt;约 600 次 prompts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Max&lt;/td&gt;
&lt;td&gt;~200 元&lt;/td&gt;
&lt;td&gt;约 2,400 次 prompts&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;官方还会补充说明：&amp;ldquo;每次 prompt 预计可调用模型 15-20 次，每月总计可用总量高达几十亿到数百亿 tokens。&amp;rdquo;&lt;/p&gt;
&lt;p&gt;看起来性价比爆表，但魔鬼藏在细节里。&lt;/p&gt;
&lt;h2 id="关键限制并发数"&gt;关键限制：并发数
&lt;/h2&gt;&lt;p&gt;大多数厂商的文档中会轻描淡写地提到：&amp;ldquo;套餐使用受到并发数（在途请求任务数量）的限制。&amp;rdquo;&lt;/p&gt;
&lt;p&gt;但具体是多少？往往不会明确告知。根据社区反馈和实测，典型的并发限制如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;套餐&lt;/th&gt;
&lt;th&gt;并发数（在途请求）&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Lite&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pro&lt;/td&gt;
&lt;td&gt;~4-5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Max&lt;/td&gt;
&lt;td&gt;~7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这个数字，直接决定了你的实际吞吐量上限。&lt;/p&gt;
&lt;h2 id="数学时间max-套餐能否用完-2400-prompts"&gt;数学时间：Max 套餐能否用完 2,400 prompts？
&lt;/h2&gt;&lt;p&gt;让我们以最高档的 Max 套餐为例，做一个简单的计算。&lt;/p&gt;
&lt;h3 id="已知条件"&gt;已知条件
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;承诺用量&lt;/strong&gt;：每 5 小时 2,400 次 prompts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;并发限制&lt;/strong&gt;：7&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;每 prompt 触发的模型调用次数&lt;/strong&gt;：15-20 次（官方数据）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型生成速度&lt;/strong&gt;：约 50-60 tokens/秒&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;5 小时 = 18,000 秒&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="计算过程"&gt;计算过程
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Step 1：估算单次 API 调用耗时&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一次完整的 API 调用包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入处理：~1 秒&lt;/li&gt;
&lt;li&gt;模型推理生成（假设输出 500 tokens）：500 ÷ 55 ≈ 9 秒&lt;/li&gt;
&lt;li&gt;网络往返延迟：~1 秒&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;合计：约 10-12 秒/次调用&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2：计算 5 小时内的最大调用次数&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;span class="lnt"&gt;2
&lt;/span&gt;&lt;span class="lnt"&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;最大调用次数 = 并发数 × (总时间 ÷ 单次耗时)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; = 7 × (18,000 ÷ 10)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; = 12,600 次
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Step 3：换算为 prompts 数量&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;按官方说法，每 prompt 触发 15-20 次调用：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;可完成 prompts = 12,600 ÷ 17.5 ≈ 720 次
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id="结论"&gt;结论
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;指标&lt;/th&gt;
&lt;th&gt;官方承诺&lt;/th&gt;
&lt;th&gt;并发上限&lt;/th&gt;
&lt;th&gt;达成率&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;每 5 小时 prompts&lt;/td&gt;
&lt;td&gt;2,400&lt;/td&gt;
&lt;td&gt;~720&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;30%&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;即使在理想条件下，Max 套餐的实际可用量也只有承诺的 30% 左右。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="更残酷的现实agent-模式下的调用膨胀"&gt;更残酷的现实：Agent 模式下的调用膨胀
&lt;/h2&gt;&lt;p&gt;上面的计算还是基于官方宣称的&amp;quot;每 prompt 15-20 次调用&amp;quot;。但在实际的 AI Coding Agent（如 Claude Code、Cline 等）场景中，情况要糟糕得多。&lt;/p&gt;
&lt;h3 id="agent-模式的工作方式"&gt;Agent 模式的工作方式
&lt;/h3&gt;&lt;p&gt;当你给 AI 编程助手一个任务时，它通常会：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;分析需求，制定计划&lt;/li&gt;
&lt;li&gt;读取相关文件（每个文件可能触发一次调用）&lt;/li&gt;
&lt;li&gt;编写代码&lt;/li&gt;
&lt;li&gt;运行测试&lt;/li&gt;
&lt;li&gt;发现错误，修复&lt;/li&gt;
&lt;li&gt;重复 3-5 直到成功&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一个看似简单的 prompt，在 Agent 循环中可能触发 &lt;strong&gt;50-100+ 次模型调用&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id="实测案例"&gt;实测案例
&lt;/h3&gt;&lt;p&gt;有用户反馈：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;2 条简单 prompt，80 秒，消耗 38M Tokens，用掉 97% 的 5 小时限额&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;反推计算：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个 prompt 消耗约 19M tokens&lt;/li&gt;
&lt;li&gt;如果按 128K 上下文计算，相当于 &lt;strong&gt;~127 次模型调用/prompt&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这比官方说的&amp;quot;15-20 次&amp;quot;高出 &lt;strong&gt;6-8 倍&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id="修正后的实际可用量"&gt;修正后的实际可用量
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;场景&lt;/th&gt;
&lt;th&gt;每 prompt 调用次数&lt;/th&gt;
&lt;th&gt;5 小时可用 prompts&lt;/th&gt;
&lt;th&gt;达成率&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;官方理想值&lt;/td&gt;
&lt;td&gt;17.5&lt;/td&gt;
&lt;td&gt;720&lt;/td&gt;
&lt;td&gt;30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;轻度使用&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;252&lt;/td&gt;
&lt;td&gt;10.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;中度使用&lt;/td&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;168&lt;/td&gt;
&lt;td&gt;7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;重度 Agent&lt;/td&gt;
&lt;td&gt;100+&lt;/td&gt;
&lt;td&gt;&amp;lt;126&lt;/td&gt;
&lt;td&gt;&amp;lt;5%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="为什么会这样"&gt;为什么会这样？
&lt;/h2&gt;&lt;h3 id="1-token-计算包含-context"&gt;1. Token 计算包含 Context
&lt;/h3&gt;&lt;p&gt;大模型的 token 消耗不仅仅是输出，还包括输入。在 Coding 场景下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每次调用都要发送完整的对话历史&lt;/li&gt;
&lt;li&gt;代码项目的上下文动辄几十 K tokens&lt;/li&gt;
&lt;li&gt;128K 上下文窗口意味着每次调用可能消耗 100K+ tokens&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="2-并发是硬约束"&gt;2. 并发是硬约束
&lt;/h3&gt;&lt;p&gt;无论你的套餐额度有多大，并发数决定了单位时间内的最大吞吐量。这是一个&lt;strong&gt;物理瓶颈&lt;/strong&gt;，不是商业策略能绕过的。&lt;/p&gt;
&lt;h3 id="3-承诺基于理想假设"&gt;3. 承诺基于理想假设
&lt;/h3&gt;&lt;p&gt;厂商的宣传数字，往往基于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每次调用只用很小的 context&lt;/li&gt;
&lt;li&gt;每个 prompt 只触发少量调用&lt;/li&gt;
&lt;li&gt;用户不会连续高强度使用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但这些假设在真实的 AI Coding 场景中几乎不成立。&lt;/p&gt;
&lt;h2 id="一张表看清真相"&gt;一张表看清真相
&lt;/h2&gt;&lt;p&gt;以 Max 套餐（~200 元/月）为例：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;指标&lt;/th&gt;
&lt;th&gt;官方宣传&lt;/th&gt;
&lt;th&gt;理论上限&lt;/th&gt;
&lt;th&gt;实际预期&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;每 5 小时 prompts&lt;/td&gt;
&lt;td&gt;2,400&lt;/td&gt;
&lt;td&gt;720&lt;/td&gt;
&lt;td&gt;150-400&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;每月 prompts&lt;/td&gt;
&lt;td&gt;345,600&lt;/td&gt;
&lt;td&gt;103,680&lt;/td&gt;
&lt;td&gt;21,600-57,600&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;每月 tokens&lt;/td&gt;
&lt;td&gt;&amp;ldquo;数百亿&amp;rdquo;&lt;/td&gt;
&lt;td&gt;~100 亿&lt;/td&gt;
&lt;td&gt;10-30 亿&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;达成率&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;30%&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;5-17%&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="给开发者的建议"&gt;给开发者的建议
&lt;/h2&gt;&lt;h3 id="1-别被数百亿-tokens忽悠"&gt;1. 别被&amp;quot;数百亿 tokens&amp;quot;忽悠
&lt;/h3&gt;&lt;p&gt;Token 数量是一个极具误导性的指标。在 Coding Agent 场景下，context 占了大头，真正有效的输出 tokens 可能只有 1-5%。&lt;/p&gt;
&lt;h3 id="2-关注并发数"&gt;2. 关注并发数
&lt;/h3&gt;&lt;p&gt;这才是决定实际体验的核心指标。如果厂商不公开并发限制，大概率是因为数字不好看。&lt;/p&gt;
&lt;h3 id="3-计算单-prompt-成本"&gt;3. 计算单 prompt 成本
&lt;/h3&gt;&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;实际单 prompt 成本 = 月费 ÷ 实际可用 prompts
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;以 Max 套餐为例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;官方宣传：200 ÷ 345,600 = 0.0006 元/prompt&lt;/li&gt;
&lt;li&gt;实际情况：200 ÷ 30,000 = &lt;strong&gt;0.007 元/prompt&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;差了 10 倍。&lt;/p&gt;
&lt;h3 id="4-考虑按量付费"&gt;4. 考虑按量付费
&lt;/h3&gt;&lt;p&gt;如果你的使用量不大，按量付费可能比包月更划算。至少不会为&amp;quot;用不完的额度&amp;quot;买单。&lt;/p&gt;
&lt;h2 id="结语"&gt;结语
&lt;/h2&gt;&lt;p&gt;大模型 Coding Plan 套餐的出现本身是好事，降低了开发者使用 AI 编程助手的门槛。但在选择套餐时，请务必：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;要求厂商公开并发限制&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自己动手算一算吞吐量上限&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不要被&amp;quot;数百亿 tokens&amp;quot;的大数字迷惑&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;毕竟，&lt;strong&gt;承诺的用量用不完，就等于变相涨价&lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;本文基于公开信息和数学推导，具体数值可能因厂商调整而变化。建议读者以实测为准。&lt;/em&gt;&lt;/p&gt;</description></item><item><title>高效省钱：我的 AI Agent 工作流选择</title><link>https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/</link><pubDate>Mon, 05 Jan 2026 16:00:00 +0800</pubDate><guid>https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/</guid><description>&lt;img src="https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/featured-image.jpg" alt="Featured image of post 高效省钱：我的 AI Agent 工作流选择" /&gt;&lt;p&gt;Claude Code 一个月 $100 费用有些高，很多朋友都有点扛不住。为了解决问题，我实践了一套工作流。&lt;/p&gt;
&lt;p&gt;模型方面，我的建议是，选择 &lt;code&gt;Gemini 3 Flash&lt;/code&gt; 的按需用量作为替代。&lt;/p&gt;
&lt;p&gt;原因：Gemini 3 Flash 性价比极高；响应速度快、处理效率高，价格仅为 Opus 和 Sonnet 的几分之一。对于大多数任务，Flash 版本已经足够使用。&lt;/p&gt;
&lt;h2 id="省钱的-workflow"&gt;省钱的 Workflow
&lt;/h2&gt;&lt;p&gt;一个经济实惠的工作流：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;制定计划&lt;/strong&gt;：使用 Gemini 3 Flash&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;执行构建&lt;/strong&gt;：使用 OpenCode 提供的免费 GLM 4.7（或 MiniMax M2.1）。亦或者你已经购买了 &lt;a class="link" href="https://svtter.cn/p/%E6%99%BA%E8%B0%B1-glm-4.5-%E5%9C%A8%E7%BC%96%E7%A8%8B%E6%96%B9%E9%9D%A2%E7%9A%84%E8%8B%A5%E5%B9%B2%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener"
&gt;Zhipu Coding Plan&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;提到 Gemini 3 就不能不提 GPT-5.2。&lt;/p&gt;
&lt;p&gt;首先，部分工程师不使用 coding agent，而是使用直接使用 ChatGPT.com。这种使用方式且不论是否高效，可靠性就令人担忧。从实际体验来看，GPT-5.2 的回复语气做了拟人化微调，过于迎合用户。虽然可以调整语气，但对于专业开发者来说可能不是最佳选择。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/pics/image_1767597061665_0.png"
width="1023"
height="930"
srcset="https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/pics/image_1767597061665_0_hu_c015f16c1d892d52.png 480w, https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/pics/image_1767597061665_0_hu_474d285f9e3620e9.png 1024w"
loading="lazy"
alt="GPT-5.2 回复语气截图"
class="gallery-image"
data-flex-grow="110"
data-flex-basis="264px"
&gt;&lt;/p&gt;
&lt;p&gt;此外，尽管 GPT-5.2 在 SWE-bench veried 上得到了较好的性能，但是我实际体验不好。这就不得不说一下 SWE-bench 的历史：&lt;/p&gt;
&lt;p&gt;SWE-bench 最初由&lt;strong&gt;普林斯顿大学&lt;/strong&gt;团队提出（ICLR 2024），用于评估语言模型解决真实 GitHub 问题的能力。&lt;/p&gt;
&lt;p&gt;但问题在于：2024年8月，OpenAI 的 Preparedness 团队与原作者合作，推出了 &lt;strong&gt;SWE-bench Verified&lt;/strong&gt;（500个经过人工验证的问题子集）。由于 OpenAI 参与了这个新版本 benchmark 的&lt;strong&gt;设计&lt;/strong&gt;，因此在这个 benchmark 下测试的 OpenAI 模型性能值得怀疑。这并不一定是主观引入性能优化，但是 Bias 在这种情况下，极有可能存在。&lt;/p&gt;
&lt;p&gt;还是那句话：从实际使用上来看，codex 总不能带来特别理想的结果。&lt;/p&gt;
&lt;h2 id="一些-opencode-技巧"&gt;一些 opencode 技巧
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;通过 OpenCode 使用 Agent&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;OpenCode 支持启动 SubAgent。调试前后端代码时，可以让 OpenCode 在不同目录启动 Agent，有效避免权限问题。&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;OpenSpec：跨 Agent 共享规范&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;span class="lnt"&gt;2
&lt;/span&gt;&lt;span class="lnt"&gt;3
&lt;/span&gt;&lt;span class="lnt"&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;1. OpenCode + Gemini 3 Flash → 生成 proposal
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;2. Codex → 代码审查
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;3. Claude Code → 再次审查
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;4. OpenSpec Apply → 最终执行
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;OpenSpec 生成 spec 是靠谱的，但是有时候便宜模型代码质量不行。这个时候可以多生成几次，使用 spec 生成多次，从中挑选最好的结果。&lt;/p&gt;
&lt;p&gt;最后，如果从 PR 看到确实不行，仍可以继续采用 sonnet 4.5 来实现代码。&lt;/p&gt;
&lt;h2 id="思考"&gt;思考
&lt;/h2&gt;&lt;p&gt;作为 Agent 工程师，我们需要基于以下趋势做决策：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;模型会越来越聪明&lt;/li&gt;
&lt;li&gt;执行速度会越来越快&lt;/li&gt;
&lt;li&gt;价格会越来越便宜&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;虽然这是大趋势，但在实际任务中仍需平衡计算速度、成本和最终效果。也许很快就会出现能够自动平衡这些因素的 Agent 系统，但是现阶段考虑这些问题，没错。&lt;/p&gt;</description></item><item><title>编码性能与模型性价比分析</title><link>https://svtter.cn/p/%E7%BC%96%E7%A0%81%E6%80%A7%E8%83%BD%E4%B8%8E%E6%A8%A1%E5%9E%8B%E6%80%A7%E4%BB%B7%E6%AF%94%E5%88%86%E6%9E%90/</link><pubDate>Sat, 03 Jan 2026 00:00:00 +0000</pubDate><guid>https://svtter.cn/p/%E7%BC%96%E7%A0%81%E6%80%A7%E8%83%BD%E4%B8%8E%E6%A8%A1%E5%9E%8B%E6%80%A7%E4%BB%B7%E6%AF%94%E5%88%86%E6%9E%90/</guid><description>&lt;img src="https://svtter.cn/p/%E7%BC%96%E7%A0%81%E6%80%A7%E8%83%BD%E4%B8%8E%E6%A8%A1%E5%9E%8B%E6%80%A7%E4%BB%B7%E6%AF%94%E5%88%86%E6%9E%90/pics/bg-new-v2.jpg" alt="Featured image of post 编码性能与模型性价比分析" /&gt;&lt;p&gt;这是我对几个模型的编码性能与性价比分析报告，用于对比不同模型在编码任务上的表现和成本效益，以便选择最合适的模型。&lt;/p&gt;
&lt;iframe src="model-comparison.pdf" style="width:100%; height:85vh; border:0;"&gt;&lt;/iframe&gt;
&lt;p&gt;中文显然使用 GLM 4.7 是比较划算的。 2000 人民币的价格，基本上包年处理了。
缺点是高峰期使用，即便是企业 MAX 版本也会很慢。&lt;/p&gt;
&lt;p&gt;从我的实际体验上看，minimax m2.1 的能力是要远远超过 GLM 4.7 的。&lt;/p&gt;</description></item><item><title>第三方客户端与大模型 API 结合 -- 性能小测</title><link>https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/</link><pubDate>Wed, 19 Nov 2025 17:03:18 +0800</pubDate><guid>https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/</guid><description>&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/bg.jpg" alt="Featured image of post 第三方客户端与大模型 API 结合 -- 性能小测" /&gt;&lt;p&gt;最近一年里，我尝试使用 &lt;a class="link" href="https://github.com/ThinkInAIXYZ/deepchat" target="_blank" rel="noopener"
&gt;deepchat&lt;/a&gt; 和 大模型 API（例如 k2 thinking turbo) 来构成一个相对私有化的聊天工具（或者说 agent 助手），来处理一些私有化的数据。但是，总的来说体验不是很好。大模型答不对。&lt;/p&gt;
&lt;p&gt;搜索方面，我使用了 bocha api，重置了 10 块，来为大模型提供搜索能力。&lt;/p&gt;
&lt;h2 id="测试的问题"&gt;测试的问题
&lt;/h2&gt;&lt;p&gt;我感觉上下文能力（单一聊天框内）还是有点问题。我简单测试了这个问题：&lt;code&gt;硅基流动上，最贵的模型是哪一个？&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;答案是：&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/answ.png"
width="1200"
height="832"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/answ_hu_b24caa1450c981f6.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/answ_hu_c39dd9490283c1be.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
&gt;&lt;/p&gt;
&lt;h2 id="kimi-k2-thinking-turbo"&gt;Kimi k2 thinking turbo
&lt;/h2&gt;&lt;p&gt;首先是 deepchat:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171204_032.png"
width="2091"
height="1587"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171204_032_hu_7db667b7f9d9df7b.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171204_032_hu_88d3f5743cb65223.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="131"
data-flex-basis="316px"
&gt;&lt;/p&gt;
&lt;p&gt;emm，不对。&lt;/p&gt;
&lt;p&gt;然后是 kimi official:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171256_940.png"
width="2163"
height="1911"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171256_940_hu_c84b877eeaffa92d.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171256_940_hu_434f4701e97c5241.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="271px"
&gt;&lt;/p&gt;
&lt;p&gt;也不对。&lt;/p&gt;
&lt;h2 id="试试-deepseek"&gt;试试 deepseek
&lt;/h2&gt;&lt;p&gt;先试试客户端。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds-dc.png"
width="2001"
height="1509"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds-dc_hu_4b496410369e3b3a.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds-dc_hu_e711c6410482af7e.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="132"
data-flex-basis="318px"
&gt;&lt;/p&gt;
&lt;p&gt;不对。&lt;/p&gt;
&lt;p&gt;再试试 deepseek official。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds.png"
width="1260"
height="1538"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds_hu_449973d3bc8fb8ab.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds_hu_a4b89e87924b34eb.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="81"
data-flex-basis="196px"
&gt;&lt;/p&gt;
&lt;p&gt;很接近，答案也靠谱了。但是可惜，也不对。&lt;/p&gt;
&lt;h2 id="如果直接问-chatgpt"&gt;如果直接问 chatgpt
&lt;/h2&gt;&lt;p&gt;嘶，有点离谱。让我们试试 gpt-5。&lt;/p&gt;
&lt;p&gt;prompt:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;span class="lnt"&gt;2
&lt;/span&gt;&lt;span class="lnt"&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;硅基流动上，最贵的模型是哪一个？
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;我指的是 siliconflow.cn
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;帮我看看
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/wechat_2025-11-19_171536_131.png"
width="1275"
height="1616"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/wechat_2025-11-19_171536_131_hu_479ab0fea5b53f5f.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/wechat_2025-11-19_171536_131_hu_33adf58eab061c65.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="78"
data-flex-basis="189px"
&gt;&lt;/p&gt;
&lt;h2 id="推断-性能不好的原因"&gt;推断-性能不好的原因
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;搜索能力不足。博查 API 背锅。&lt;/li&gt;
&lt;li&gt;不同模型，最佳表现的超参数可能不一样。我调用的是硅基流动的大模型 API。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="结论"&gt;结论
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;单从这个问题上看，还是 chatgpt 强一些。相比之前，官方的搜索+模型，似乎性能也会更好一些。因此，如果不是特别隐私的数据，还是用官方的比较好。&lt;/li&gt;
&lt;li&gt;本文仅供参考，看个乐子。&lt;/li&gt;
&lt;/ul&gt;</description></item></channel></rss>