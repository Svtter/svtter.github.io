<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on Svtter's Blog</title><link>https://svtter.cn/tags/deep-learning/</link><description>Recent content in Deep Learning on Svtter's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Wed, 19 Mar 2025 13:23:50 +0800</lastBuildDate><atom:link href="https://svtter.cn/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Read Code of CLIP.md</title><link>https://svtter.cn/p/read-code-of-clip.md/</link><pubDate>Wed, 19 Mar 2025 13:23:50 +0800</pubDate><guid>https://svtter.cn/p/read-code-of-clip.md/</guid><description>&lt;img src="https://svtter.cn/p/read-code-of-clip.md/image.png" alt="Featured image of post Read Code of CLIP.md" />&lt;p>Contrastive Language-Image Pre-Training (CLIP) 是 openai 的经典工作之一。出自论文&lt;a class="link" href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener"
>&lt;/a>&lt;/p>
&lt;p>为了能够在 CLIP 上完成我的新 idea，我尝试阅读 &lt;a class="link" href="https://github.com/openai/CLIP" target="_blank" rel="noopener"
>openai/clip&lt;/a> 来理解 clip 在 classifier 上的基本工作原理。&lt;/p>
&lt;p>这是 &lt;a class="link" href="https://github.com/openai/CLIP" target="_blank" rel="noopener"
>openai/clip&lt;/a> 给出的 python 样例代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">clip&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">PIL&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Image&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">device&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;cuda&amp;#34;&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_available&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="s2">&amp;#34;cpu&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">preprocess&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">clip&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;ViT-B/32&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">preprocess&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Image&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;CLIP.png&amp;#34;&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">clip&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tokenize&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="s2">&amp;#34;a diagram&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;a dog&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;a cat&amp;#34;&lt;/span>&lt;span class="p">])&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">no_grad&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">image_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode_image&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">text_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode_text&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits_per_image&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">logits_per_text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">probs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">logits_per_image&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cpu&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">numpy&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Label probs:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">probs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># prints: [[0.9927937 0.00421068 0.00299572]]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>load 函数用于加载特定的 openai 模型。这里是基于&lt;code>ViT-B/32&lt;/code>，一个 Vision Transformer 32B。&lt;/p>
&lt;p>可以看到，如果 openai 支持的 vision encoder 大概有如下几种：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">_MODELS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;RN50&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;RN101&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;RN50x4&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;RN50x16&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;RN50x64&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;ViT-B/32&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;ViT-B/16&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;ViT-L/14&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;ViT-L/14@336px&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们假设模型已经下载完成，让我们看看 _tranform 预处理工作是如何进行的：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">_transform&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_px&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">Compose&lt;/span>&lt;span class="p">([&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Resize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_px&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">interpolation&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">BICUBIC&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">CenterCrop&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_px&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_convert_image_to_rgb&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ToTensor&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Normalize&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="mf">0.48145466&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.4578275&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.40821073&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mf">0.26862954&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.26130258&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.27577711&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>也不是很复杂，预处理&lt;code>Normalize&lt;/code>参数虽然不太明白。似乎是用的 ViT 同样的预处理参数。&lt;/p>
&lt;p>然后进入模型加载阶段，我们可以看到，如果不是 &lt;a class="link" href="https://chenglu.me/blogs/pytorch-jit" target="_blank" rel="noopener"
>jit 加载&lt;/a> ，那么模型会选择 state_dict 的模式。
通过加载 state_dict 的过程，我们可以看到 build_model 函数用于加载权重，将权重赋值给已有的模型结构。&lt;/p>
&lt;p>这个模型结构的文件是&lt;a class="link" href="https://github.com/openai/CLIP/blob/main/clip/model.py" target="_blank" rel="noopener"
>model.py&lt;/a>。
因此，CLIP 的主要代码位于&lt;a class="link" href="https://github.com/openai/CLIP/blob/main/clip/model.py#L243" target="_blank" rel="noopener"
>model.py#L243&lt;/a>。&lt;/p>
&lt;p>image_encoder 和 text_encoder 的输出，分别是两个不同的特征 tensor。&lt;/p>
&lt;p>将两个 tensor 进行矩阵乘积，分别得到一个相似性矩阵。这个相似性矩阵的大小是 &lt;code>(batch_size, batch_size)&lt;/code>。&lt;/p>
&lt;blockquote>
&lt;p>TIPS: 如果说 batch-size 太小，为1，那么对比学习的性能可能就大打折扣了。&lt;/p>&lt;/blockquote>
&lt;p>这两个 tensor 使用 symmetric cross-entropy loss 进行计算，来用于更新网络权重。&lt;/p>
&lt;p>专门做智能指标的提升，不太在意计算量。不追求最新最高的智能指标，更加关注模型计算的运行效率。&lt;/p>
&lt;blockquote>
&lt;p>trick: 给参数加一个 log 来使得权重更新没有那么剧烈，计算起来没有那么大。&lt;/p>&lt;/blockquote>
&lt;p>&lt;a class="link" href="https://github.com/openai/CLIP" target="_blank" rel="noopener"
>CLIP&lt;/a> 代码中没有给出能够直接进行训练的代码。下一篇文章，尝试阅读一下 openclip。&lt;/p></description></item><item><title>Reimplment MAE Work Path.md</title><link>https://svtter.cn/p/reimplment-mae-work-path.md/</link><pubDate>Wed, 12 Mar 2025 11:12:03 +0800</pubDate><guid>https://svtter.cn/p/reimplment-mae-work-path.md/</guid><description>&lt;img src="https://svtter.cn/p/reimplment-mae-work-path.md/arch.png" alt="Featured image of post Reimplment MAE Work Path.md" />&lt;p>这两天我一直在尝试复现 MAE，与我的数据进行结合，来做出一点新的东西。&lt;/p>
&lt;h2 id="offical-mae">Offical MAE
&lt;/h2>&lt;p>我首先从 meta 的&lt;a class="link" href="https://github.com/facebookresearch/mae" target="_blank" rel="noopener"
>官方仓库&lt;/a>开始。我成功跑通了 demo，但是发现仍然有些问题。&lt;a class="link" href="https://ai.meta.com/blog/open-sourcing-submitit-a-lightweight-tool-for-slurm-cluster-computation/" target="_blank" rel="noopener"
>submitit&lt;/a> 是 meta 开源的集群任务提交工具。这个代码使用的应该是 meta 内部使用的训练集群，基于 &lt;a class="link" href="https://slurm.schedmd.com/quickstart.html" target="_blank" rel="noopener"
>slurm&lt;/a>。我无法访问这个集群。而且&lt;a class="link" href="https://slurm.schedmd.com/quickstart_admin.html" target="_blank" rel="noopener"
>部署slurm&lt;/a> 集群可能很麻烦。因此作罢。&lt;/p>
&lt;h3 id="tips">Tips
&lt;/h3>&lt;p>使用开源工具是对的。内部的私有工具无法迁移到下一个团队，重新学习的成本比较高。开源工具一般没有这个限制，可以打通切换团队后的协作。&lt;/p>
&lt;h3 id="dockerfile">Dockerfile
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span class="line">&lt;span class="cl">&lt;span class="k">FROM&lt;/span>&lt;span class="s"> pytorch/pytorch:1.11.0-cuda11.3-cudnn8-devel&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> pip install jupyterlab &lt;span class="s2">&amp;#34;numpy&amp;lt;1.20&amp;#34;&lt;/span> &lt;span class="s2">&amp;#34;timm==0.4.5&amp;#34;&lt;/span> matplotlib -i https://mirrors.sustech.edu.cn/pypi/simple&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> rm /etc/apt/sources.list.d/cuda.list &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> rm /etc/apt/sources.list.d/nvidia-ml.list &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> apt-get update &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> apt-get install -y wget&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">CMD&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;jupyter&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;lab&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--ip=0.0.0.0&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--port=8888&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--no-browser&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--allow-root&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--NotebookApp.token=&amp;#39;abcd&amp;#39;&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>使用上面这个 Dockerfile 可以跑通环境。&lt;/p>
&lt;h2 id="mae-pytorch">MAE-pytorch
&lt;/h2>&lt;p>之后我想使用 pytorch lightning 来跑一下 MAE。现在训练的显卡够用，但是 pytorch 直接写 DDP 比较麻烦。于是我参考了 &lt;a class="link" href="https://github.com/mtrazzak/mae-pytorch" target="_blank" rel="noopener"
>mae-pytorch&lt;/a>。&lt;/p>
&lt;p>但是训练之后发现，似乎对 lightning 的适配不是很好。我调整了 &lt;code>num_gpus&lt;/code> 参数，训练仍然不顺。&lt;/p>
&lt;p>&lt;a class="link" href="https://gist.github.com/Svtter/f4416107d99d69d3a3c48a6eae918411" target="_blank" rel="noopener"
>https://gist.github.com/Svtter/f4416107d99d69d3a3c48a6eae918411&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">File&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">workspace&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">mae&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">py&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">145&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">MaskedAutoencoderLIT&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">generate_grid_of_samples&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">true_images&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pred_patch&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mask&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">144&lt;/span> &lt;span class="n">pred_images&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">sample_indices&lt;/span>&lt;span class="p">][:,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">--&amp;gt;&lt;/span> &lt;span class="mi">145&lt;/span> &lt;span class="n">true_image_grid&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">true_images&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">sample_indices&lt;/span>&lt;span class="p">][:,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">147&lt;/span> &lt;span class="n">grid_images&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">true_image_grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pred_images&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="ne">RuntimeError&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">CUDA&lt;/span> &lt;span class="n">error&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">side&lt;/span> &lt;span class="k">assert&lt;/span> &lt;span class="n">triggered&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">CUDA&lt;/span> &lt;span class="n">kernel&lt;/span> &lt;span class="n">errors&lt;/span> &lt;span class="n">might&lt;/span> &lt;span class="n">be&lt;/span> &lt;span class="n">asynchronously&lt;/span> &lt;span class="n">reported&lt;/span> &lt;span class="n">at&lt;/span> &lt;span class="n">some&lt;/span> &lt;span class="n">other&lt;/span> &lt;span class="n">API&lt;/span> &lt;span class="n">call&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">so&lt;/span> &lt;span class="n">the&lt;/span> &lt;span class="n">stacktrace&lt;/span> &lt;span class="n">below&lt;/span> &lt;span class="n">might&lt;/span> &lt;span class="n">be&lt;/span> &lt;span class="n">incorrect&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">For&lt;/span> &lt;span class="n">debugging&lt;/span> &lt;span class="n">consider&lt;/span> &lt;span class="n">passing&lt;/span> &lt;span class="n">CUDA_LAUNCH_BLOCKING&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Compile&lt;/span> &lt;span class="k">with&lt;/span> &lt;span class="err">`&lt;/span>&lt;span class="n">TORCH_USE_CUDA_DSA&lt;/span>&lt;span class="err">`&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">enable&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">side&lt;/span> &lt;span class="n">assertions&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="imagenet-的问题">ImageNet 的问题
&lt;/h3>&lt;p>而且 &lt;strong>ImageNet 我本地没有相关的数据&lt;/strong>，重新下载也是比较麻烦。（虽然可以用 pretrain weights）。&lt;/p>
&lt;p>我下载了 &lt;a class="link" href="https://www.kaggle.com/c/tiny-imagenet" target="_blank" rel="noopener"
>tiny-imagenet&lt;/a>，但是没能起到作用。&lt;/p>
&lt;p>我尝试了一下，能跑通 MNIST + resize，也就是 &lt;code>Tutorial.ipynb&lt;/code> 的内容。因为并行训练的重新调优比较麻烦，我暂时放下，再去看看其他的库。&lt;/p>
&lt;p>ImageNet 体积比较大，大家一般会使用小型 imagenet 来做一些测试。例如 &lt;a class="link" href="https://github.com/ultralytics/yolov5/issues/11028" target="_blank" rel="noopener"
>yolo-v5 issue&lt;/a> 中所表述的。&lt;/p>
&lt;h4 id="imagenet-100">imagenet-100
&lt;/h4>&lt;p>imagenet 往往太大了，我们可用一些小型的数据集去验证模型；但是 shape 和文件结构需要与 imagenet 一致。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">kagglehub&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Download latest version&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">path&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">kagglehub&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dataset_download&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;ambityga/imagenet100&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Path to dataset files:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">path&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="dockerfile-1">Dockerfile
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span class="line">&lt;span class="cl">&lt;span class="k">FROM&lt;/span>&lt;span class="s"> pytorch/pytorch:2.4.1-cuda11.8-cudnn9-devel&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">WORKDIR&lt;/span>&lt;span class="s"> /workspace&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">COPY&lt;/span> requirements.txt .&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> pip install -r requirements.txt -i https://mirrors.sustech.edu.cn/pypi/web/simple &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> pip install jupyterlab matplotlib&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">CMD&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;jupyter&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;lab&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--ip=0.0.0.0&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--port=8888&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--no-browser&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--allow-root&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--NotebookApp.token=&amp;#39;abcd&amp;#39;&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="solo-learn">solo learn
&lt;/h2>&lt;p>&lt;a class="link" href="https://github.com/vturrisi/solo-learn?tab=readme-ov-file" target="_blank" rel="noopener"
>solo-learn&lt;/a> 是发表在 IJML 的一个工作。主要实现了 self-regression 的相关方法。他们使用了一个我之前没有使用过的库&lt;a class="link" href="https://github.com/NVIDIA/DALI" target="_blank" rel="noopener"
>NVIDIA/DALI&lt;/a>&lt;/p>
&lt;p>我使用了 MAE_pytorch 的 Dockerfile，做了一些调整。&lt;/p>
&lt;h3 id="start">Start
&lt;/h3>&lt;p>做分类任务，参考文档 &lt;a class="link" href="https://solo-learn.readthedocs.io/en/latest/tutorials/offline_linear_eval.html" target="_blank" rel="noopener"
>offline-linear-eval&lt;/a>.&lt;/p>
&lt;p>首先是文档有点问题，&lt;code>from solo.utils ...&lt;/code> 应该改为 &lt;code>from solo.data...&lt;/code>。&lt;/p>
&lt;p>这个库使用了 &lt;a class="link" href="https://hydra.cc/docs/intro/" target="_blank" rel="noopener"
>hydra&lt;/a> 以及 &lt;a class="link" href="https://omegaconf.readthedocs.io/en/2.3_branch/" target="_blank" rel="noopener"
>omegaconf&lt;/a>。&lt;/p>
&lt;p>&lt;a class="link" href="https://hydra.cc/docs/intro/" target="_blank" rel="noopener"
>hydra&lt;/a> 可以比较有效的去处理配置重叠的情况。我之前写类似的东西，都是一个一个类，然后不停的嵌套嵌套。这样看，hydra 确实方便很多。&lt;/p>
&lt;p>&lt;a class="link" href="https://omegaconf.readthedocs.io/en/2.3_branch/" target="_blank" rel="noopener"
>omegaconf&lt;/a> 是一个使用 hydra 配置文件的工具；由于 hydra 配置返回的是 DictConfig，因此读取配置的时候需要使用 dict 数据结构进行访问。&lt;/p>
&lt;p>不过我觉得实际上用处不是很大。除非项目很庞大，否则增加了上手的成本。&lt;/p>
&lt;p>&lt;a class="link" href="https://github.com/vturrisi/solo-learn?tab=readme-ov-file" target="_blank" rel="noopener"
>solo-learn&lt;/a> 基本上已经不太维护了。我尝试下载了 imagenet-100，正在测试中。&lt;/p>
&lt;p>尝试执行了&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="cp">#!/bin/bash
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">python3 main_pretrain.py &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --config-path scripts/pretrain/imagenet-100/ &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --config-name barlow.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># add new arguments (e.g. those not defined in the yaml files)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># by doing ++new_argument=VALUE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># pytorch lightning&amp;#39;s arguments can be added here as well.&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>遇到 cuda 内存不够用的情况。放弃了。&lt;/p>
&lt;h3 id="dockerfile-2">Dockerfile
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span class="line">&lt;span class="cl">&lt;span class="k">FROM&lt;/span>&lt;span class="s"> pytorch/pytorch:2.4.1-cuda11.8-cudnn9-devel&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">WORKDIR&lt;/span>&lt;span class="s"> /workspace&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">COPY&lt;/span> requirements.txt .&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> pip install -r requirements.txt -i https://mirrors.sustech.edu.cn/pypi/web/simple &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> pip install jupyterlab matplotlib&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="c"># RUN pip3 install .[dali,umap,h5] --extra-index-url https://developer.download.nvidia.com/compute/redist&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">CMD&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;jupyter&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;lab&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--ip=0.0.0.0&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--port=8888&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--no-browser&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--allow-root&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;--NotebookApp.token=&amp;#39;abcd&amp;#39;&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="tips-1">Tips
&lt;/h3>&lt;p>直接使用 checkpoints 可以节省大量的训练时间。手里的卡更多的应该去用来 fine tune，或者训练自己的数据集。从头开始训练（我之前的做法）往往是不经济且速度慢的。&lt;/p>
&lt;h2 id="huggingface-transformers">huggingface &lt;a class="link" href="https://huggingface.co/docs/transformers/en/index" target="_blank" rel="noopener"
>transformers&lt;/a>
&lt;/h2>&lt;p>很出名的一个库。很多朋友都使用这个库来尝试了构建了自己的大型模型。&lt;/p>
&lt;p>我想 MAE 应该是组成部分之一，我想借着这个机会使用一下看。&lt;/p>
&lt;p>我尝试了一下 open sources models by huggingface。&lt;/p>
&lt;p>transformers 使用起来比较简单。zero-shot image classification 入门速度很快。&lt;/p>
&lt;!-- 但是对于多目标的复杂任务，还需要进一步探索一下。 -->
&lt;p>使用 vit_mae，可以通过这个&lt;a class="link" href="https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/vit_mae#overview" target="_blank" rel="noopener"
>官方文档&lt;/a>。&lt;/p>
&lt;h3 id="使用预训练权重的-mae">使用预训练权重的 MAE
&lt;/h3>&lt;p>这是 huggingface 官方提供的源代码，我直接贴在这里了。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">transformers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">ViTFeatureExtractor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ViTForImageClassification&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">datasets&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">load_dataset&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dataset&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">load_dataset&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;huggingface/cats-image&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dataset&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;test&amp;#34;&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="s2">&amp;#34;image&amp;#34;&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">feature_extractor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ViTFeatureExtractor&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">from_pretrained&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;google/vit-base-patch16-224&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ViTForImageClassification&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">from_pretrained&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;google/vit-base-patch16-224&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">feature_extractor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">return_tensors&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;pt&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">no_grad&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">logits&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># model predicts one of the 1000 ImageNet classes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">predicted_label&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">logits&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id2label&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">predicted_label&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Dataset 和 DataLoader 的区别是什么</title><link>https://svtter.cn/p/dataset-%E5%92%8C-dataloader-%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88/</link><pubDate>Mon, 24 Feb 2025 22:01:57 +0800</pubDate><guid>https://svtter.cn/p/dataset-%E5%92%8C-dataloader-%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88/</guid><description>&lt;p>在 &lt;code>torch.utils.data&lt;/code> 中，有两个类，一个是&lt;code>Dataset&lt;/code>，另一个是&lt;code>DataLoader&lt;/code>。&lt;/p>
&lt;p>这两个类的主要区别是什么？&lt;/p>
&lt;p>Dataset 一般用于读取数据集的基础数据。例如，在 torch 给出的&lt;a class="link" href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html" target="_blank" rel="noopener"
>官网说明&lt;/a>中，用于展示数据集的数量，以及用于神经网络训练的单个样本。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">FaceLandmarkDataset&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Dataset&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__len__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;表示数据集的数量&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__getitem__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">idx&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;返回某个下标的数据组合。例如，如果是图像和标签，应该是 {&amp;#39;img&amp;#39;: img, &amp;#39;label&amp;#39;: label} &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>与之相对的，DataLoader 则是一个可以并行读取数据的类。一般情况下，不需要进行继承然后改写。&lt;/p>
&lt;p>所以我们主要说说怎么用。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">dataloader&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">DataLoader&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">transformed_dataset&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">shuffle&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_workers&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们可以看到，因为 DataLoader 的存在，因此实际上我们在实现 Dataset 的时候:&lt;/p>
&lt;ol>
&lt;li>不需要关注 shuffle: 为了神经网络训练的 batch不聚集在一个地方&lt;/li>
&lt;li>不需要考虑并行读取，因为有 &lt;code>num_worker&lt;/code>&lt;/li>
&lt;li>不需要考虑 &lt;code>batch_size&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>因此，DataLoader 可以比较容易的完成一些数据集处理前的必要工作。&lt;/p>
&lt;p>如果使用 lightning，那么还需要进一步了解 DataModule。我会在另外一篇博客中说明。&lt;/p></description></item><item><title>Where to Put Your Data Folder.md</title><link>https://svtter.cn/p/where-to-put-your-data-folder.md/</link><pubDate>Mon, 24 Feb 2025 14:34:56 +0800</pubDate><guid>https://svtter.cn/p/where-to-put-your-data-folder.md/</guid><description>&lt;p>在训练模型时，我们应该尽可能的把数据和代码放在同一个位置。&lt;/p>
&lt;p>放在同一个位置可以避免路径的问题，例如给出数据的绝对路径等。&lt;/p>
&lt;p>比如，如果我将路径设置为，&lt;code>./data/&lt;/code>，那么我只需要把数据放在&lt;code>./data&lt;/code>目录下即可。&lt;/p>
&lt;p>我可以通过&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ln -s &lt;span class="k">$(&lt;/span>source-path-of-dataset&lt;span class="k">)&lt;/span> ./data
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>来将数据从其他位置链接过来。&lt;/p>
&lt;p>如果在相同的主机上，git 是可以自动同步这些 link 的。&lt;/p>
&lt;p>但如果在不同的主机上，就需要自己做好管理了。&lt;/p></description></item><item><title>图片数据集的浏览</title><link>https://svtter.cn/p/%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%B5%8F%E8%A7%88/</link><pubDate>Sun, 12 Jan 2025 18:31:12 +0800</pubDate><guid>https://svtter.cn/p/%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%B5%8F%E8%A7%88/</guid><description>&lt;p>数据集浏览是个比较麻烦的事情。尤其是数据集比较大的时候。&lt;/p>
&lt;p>npy (numpy array) ，h5 文件是两种常见的数据存储方式。
h5 文件的缺点是很容易产生数据损坏。笔者使用的时候多次遇见 h5 文件打不开的问题。
npy 文件在读取速度，文件传输方面具有很明显的优势。缺点是一次性加载到内存中，如果服务器不行，很容易爆掉。&lt;/p>
&lt;p>常见的图像数据集一般是将 label 和 image 分开放。例如 COCO 等。这样一来，也可以用文件浏览器去查看图片，可以快速的观察图片的特点。但一般情况下，我们不会在本地的电脑上查看图片，而是更多的在服务上操作数据集。&lt;/p>
&lt;p>2024，结合 torch，我感觉还是 matplotlib 直接绘图会方便一些。matplotlib 直接绘图一般是展示单张图片。但是如果利用 subplot，可以同时展示更多图片。如果用了 opencv，可以将部分标签值打印上去。不过也有缺点：如果使用的是远程服务器，生成图片的传输过程需要占用较多的带宽。
具体采用什么方式，还得自己做判断呐！&lt;/p></description></item><item><title>CNN Size Computing</title><link>https://svtter.cn/p/cnn-size-computing/</link><pubDate>Fri, 11 Oct 2024 16:51:14 +0800</pubDate><guid>https://svtter.cn/p/cnn-size-computing/</guid><description>&lt;p>CNNs (Convolutional Neural Networks) is a amazing component of neural network theory. However, to use it efficiently, we need to compute the output shape.&lt;/p>
&lt;h3 id="ask-for-chatgpt">ask for chatgpt
&lt;/h3>&lt;ul>
&lt;li>&lt;a class="link" href="https://chatgpt.com/share/19be811d-e750-45de-b5bd-ad391c9dba80" target="_blank" rel="noopener"
>https://chatgpt.com/share/19be811d-e750-45de-b5bd-ad391c9dba80&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="input-shape">input shape
&lt;/h3>&lt;p>Ensure that the input to your network is of the correct shape. The input tensor should have the dimensions &lt;code>[batch_size, channels, height, width]&lt;/code>. For example, if you&amp;rsquo;re using a batch size of 1, the input should be &lt;code>[1, 32, 32, 300]&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>1 is batch size.&lt;/li>
&lt;li>&lt;code>input_data = torch.randn(1, 32, 32, 300) # Example input tensor with the correct shape&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="compute-of-cnn-output">Compute of CNN output
&lt;/h3>&lt;ul>
&lt;li>reference: &lt;a class="link" href="https://www.baeldung.com/cs/convolutional-layer-size" target="_blank" rel="noopener"
>baeldung link&lt;/a>
&lt;img src="https://svtter.cn/assets/image_1717928679511_0.png"
loading="lazy"
alt="image.png"
>&lt;/li>
&lt;/ul>
&lt;p>The channels number is the CNN&amp;rsquo;s filters number.&lt;/p></description></item><item><title>Easy LSTM Training Tricks</title><link>https://svtter.cn/p/easy-lstm-training-tricks/</link><pubDate>Mon, 12 Aug 2024 15:35:03 +0800</pubDate><guid>https://svtter.cn/p/easy-lstm-training-tricks/</guid><description>&lt;p>This post introduces how to train the LSTM networks to get correct outputs.&lt;/p>
&lt;p>If you use this way, not work.&lt;/p>
&lt;ul>
&lt;li>Just use the last width, like &lt;code>x[-1, :, :]&lt;/code>, select the last piece of width.&lt;/li>
&lt;li>It&amp;rsquo;s normal way. However, sometimes it will &lt;strong>NOT WORK&lt;/strong>.&lt;/li>
&lt;li>&lt;img src="https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-1.png"
width="744"
height="1140"
srcset="https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-1_hu_ac7f1b95cef41c2a.png 480w, https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-1_hu_90a5c97aafe0bd87.png 1024w"
loading="lazy"
alt="image.png"
class="gallery-image"
data-flex-grow="65"
data-flex-basis="156px"
>&lt;/li>
&lt;/ul>
&lt;p>This way will work.&lt;/p>
&lt;ul>
&lt;li>Flatten all the output&lt;/li>
&lt;li>&lt;img src="https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-2.png"
width="702"
height="1140"
srcset="https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-2_hu_15f30d5916691925.png 480w, https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-2_hu_30536e8b9ab4dc3b.png 1024w"
loading="lazy"
alt="image.png"
class="gallery-image"
data-flex-grow="61"
data-flex-basis="147px"
>&lt;/li>
&lt;li>It always works.&lt;/li>
&lt;/ul>
&lt;p>Have fun.&lt;/p>
&lt;p>Besides, I found &lt;a class="link" href="https://marimo.io/use-cases#examples" target="_blank" rel="noopener"
>marimo&lt;/a>, which is a replacement for jupyter notebooks.&lt;/p></description></item><item><title>My Keras tricks</title><link>https://svtter.cn/p/my-keras-tricks/</link><pubDate>Mon, 07 Jan 2019 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/my-keras-tricks/</guid><description>&lt;p>记录了一些使用 keras 的技巧。&lt;/p>
&lt;h2 id="categorical_crossentropy-vs-sparse_categorical_crossentropy">categorical_crossentropy vs sparse_categorical_crossentropy.
&lt;/h2>&lt;h4 id="3-the-answerin-a-nutshell">3. The Answer, In a Nutshell
&lt;/h4>&lt;ul>
&lt;li>
&lt;p>If your targets are one-hot encoded, use categorical_crossentropy.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Examples of &lt;a href="https://jovianlin.io/keras-one-hot-encode-decode-sequence-data/" target="_blank" rel="noreferrer noopener">one-hot encodings&lt;/a>:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[1,0,0]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[0,1,0]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[0,0,1]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>But if your targets are integers, use sparse_categorical_crossentropy.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Examples of integer encodings (&lt;em>for the sake of completion&lt;/em>):&lt;/p>
&lt;ul>
&lt;li>1, 2, 3&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="clip-norm">&lt;strong>clip norm&lt;/strong>
&lt;/h2>&lt;p style="text-align:center">
&lt;br />&lt;a rel="noreferrer noopener" href="https://wulc.me/2018/05/01/%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA%E5%8F%8A%E5%85%B6%E4%BD%9C%E7%94%A8/" target="_blank">https://wulc.me/2018/05/01/%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA%E5%8F%8A%E5%85%B6%E4%BD%9C%E7%94%A8/&lt;/a>&lt;br />可以加速 RNN 训练
&lt;/p>
&lt;h2 id="multiple-gpu">&lt;strong>Multiple GPU&lt;/strong>
&lt;/h2>&lt;p># &lt;a class="link" href="https://keras.io/utils/#multi" target="_blank" rel="noopener"
>https://keras.io/utils/#multi&lt;/a>_gpu_model#&lt;/p>
&lt;p>使用多GPU，注意使用 save 的时候，传参传 model (multi_gpu_model的model参数)&lt;/p>
&lt;h2 id="encode-labels">&lt;strong>Encode Labels&lt;/strong>
&lt;/h2>&lt;p>可以把不同的字符[‘aa’, ‘bb’, ‘cc’, ‘aa’] 编码成 [0, 1, 2, 0]&lt;/p>
&lt;table class="wp-block-table">
&lt;tr>
&lt;td>
1&lt;br />2&lt;br />3&lt;br />4&lt;br />5&lt;br />6
&lt;/td>
&lt;pre>&lt;code>&amp;lt;td&amp;gt;
# encode class values as integers&amp;lt;br /&amp;gt;encoder&amp;amp;nbsp;=&amp;amp;nbsp;LabelEncoder()&amp;lt;br /&amp;gt;encoder.fit(Y)&amp;lt;br /&amp;gt;encoded_Y&amp;amp;nbsp;=&amp;amp;nbsp;encoder.transform(Y)&amp;lt;br /&amp;gt;# convert integers to dummy variables (i.e. one hot encoded)&amp;lt;br /&amp;gt;dummy_y&amp;amp;nbsp;=&amp;amp;nbsp;np_utils.to_categorical(encoded_Y)
&amp;lt;/td&amp;gt;
&lt;/code>&lt;/pre>
&lt;/tr>
&lt;/table>
&lt;h2 id="训练中存在的问题">训练中存在的问题
&lt;/h2>&lt;p>训练性能低了别急着调参，首先看看数据预处理有没有问题，评价指标是不是写错了。再一个，batch norm 要勤快点加上。&lt;/p>
&lt;p>相似的文章还有：&lt;a class="link" href="https://svtter.github.io/2018/02/01/keras%e5%9d%91/" target="_blank" rel="noopener"
>https://svtter.github.io/2018/02/01/keras%e5%9d%91/&lt;/a>&lt;/p></description></item><item><title>我应该学哪个深度学习框架？</title><link>https://svtter.cn/p/%E6%88%91%E5%BA%94%E8%AF%A5%E5%AD%A6%E5%93%AA%E4%B8%AA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/</link><pubDate>Wed, 02 Jan 2019 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/%E6%88%91%E5%BA%94%E8%AF%A5%E5%AD%A6%E5%93%AA%E4%B8%AA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/</guid><description>&lt;img src="https://i0.wp.com/ws1.sinaimg.cn/large/c53b1907ly1fysk81yoinj21wa0zgael.jpg?w=625&amp;#038;ssl=1" data-recalc-dims="1" />
&lt;p>答案似乎显然意见 😉&lt;/p></description></item><item><title>beam search – 一个搜索策略</title><link>https://svtter.cn/p/beam-search-%E4%B8%80%E4%B8%AA%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5/</link><pubDate>Fri, 23 Nov 2018 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/beam-search-%E4%B8%80%E4%B8%AA%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5/</guid><description>&lt;p>这篇文章不建议读，2018年写的，不知所云。&lt;/p>
&lt;p>beam search 是一个近似搜索策略，用于在候选可能中选择最好的结果。&lt;a class="link" href="https://hackernoon.com/beam-search-a-search-strategy-5d92fb7817f" target="_blank" rel="noopener"
>原文链接&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://i2.wp.com/cdn-images-1.medium.com/max/1760/1*HmPwWf0VJV3TeaGKoly4Xw.png?w=625&amp;amp;#038;ssl=1"
loading="lazy"
alt="img"
>&lt;/p>
&lt;!-- &lt;img src="" alt="" data-recalc-dims="1" />&lt;/figure> -->
&lt;p>一个常用例子，BS(beam search) 用于获得与机器翻译等价的结果。对于那些不了解机器翻译的人，也肯定知道 Google Translate。&lt;/p>
&lt;p>这就是为啥要讲这个。这些系统都用 BS 技术来找到与结果最等价的翻译。阅读这个 Wiki 来了解相同文件的定义。&lt;/p>
&lt;p>让我们讨论一下这个使用机器翻译案例的策略。如果你是一个喜欢研究现象背后原理的人，一定要读一下 google encoder-decoder 网络架构。这个东西我就不讲了，有很多人讲。例如，如果你不知道这个架构，看看这个 quora 上的回答。&lt;/p>
&lt;h2 id="一个视角">一个视角
&lt;/h2>&lt;p>机器翻译模型可以被认为是一种 “条件语言模型”，对于…&lt;/p>
&lt;h2 id="让我们看一下8230">让我们看一下…
&lt;/h2>&lt;p>BS B(beam 宽度) 是唯一一个调整翻译结果的超参。 B 在一般情况决定了，在每一步，要记忆的单词的个数，来变换概率。&lt;/p>
&lt;h2 id="不翻译了这里有更直接的结果">不翻译了。。这里有更直接的结果
&lt;/h2>&lt;p>beam search 时在每一个时间点选择 beam_width 个最大的可能类别，然后在每个时间点 beam_width 个类别组成的空间里寻找整体概率最大的一条路径，得到最后得识别输出。而 greedy search 则直接在每个时间点寻找概率最大的类别，然后依次组成这个路径。也就是说，greedy search 是 beam_width=1 版本的 beam search。上图是 CTC 论文里 greedy search 示意图。&lt;/p></description></item><item><title>对于 CTC 的一个直观理解与解释</title><link>https://svtter.cn/p/%E5%AF%B9%E4%BA%8E-ctc-%E7%9A%84%E4%B8%80%E4%B8%AA%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E4%B8%8E%E8%A7%A3%E9%87%8A/</link><pubDate>Thu, 22 Nov 2018 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/%E5%AF%B9%E4%BA%8E-ctc-%E7%9A%84%E4%B8%80%E4%B8%AA%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E4%B8%8E%E8%A7%A3%E9%87%8A/</guid><description>&lt;blockquote>
&lt;p>这篇文章是一个翻译：&lt;a class="link" href="https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c" target="_blank" rel="noopener"
>towardsdatascience-ctc&lt;/a>&lt;/p>&lt;/blockquote>
&lt;p>通过 CTC loss 以及编码操作进行文字识别。&lt;/p>
&lt;p>如果想用使用计算机识别文字，神经网络很好用。使用一些列 CNN 从序列中提取特征，使用 RNN 来传播需略的信息。它会输出字符得分，给每个序列元素，通过一个简单的矩阵表示。现在，有两个事情我们想要对矩阵进行处理。&lt;/p>
&lt;ol>
&lt;li>训练：计算损失值来训练神经网络&lt;/li>
&lt;li>推理：解码矩阵来获得图片中的字符&lt;/li>
&lt;/ol>
&lt;p>两个任务可以同时被 CTC 操作完成。对于手写数字系统的描述，可以参见图像 1.&lt;/p>
&lt;p>我们更进一步看看 CTC 操作，并且讨论一下它如完成的，以及它背后的公式是如此巧妙。最后，我将会指点你来找到 Python 代码以及不复杂的公式，如果你感兴趣的话。&lt;/p>
&lt;p>&lt;img src="https://i0.wp.com/cdn-images-1.medium.com/max/1760/1*i2OG4hu9EjsyWcVMc4OOvA.png?w=625&amp;amp;#038;ssl=1"
loading="lazy"
alt="img"
>&lt;/p>
&lt;h2 id="为什么我们使用-ctc">为什么我们使用 CTC
&lt;/h2>&lt;p>当然我们可以创建一个数据集，这个数据集有文本行，然后指出每列属于哪一个字符，就像图 2 中展示的那样。然后，我们可以训练一个神经网络来输出每一列的得分。而然，对于这个简单的解法，这里有两个问题。&lt;/p>
&lt;ol>
&lt;li>这个十分的耗时（以及无聊）来在字符层面上标注数据&lt;/li>
&lt;li>我们仅仅能够得到字符的得分，因此还需要一些操作来获取最终的文本。一个简单地字符可以跨越多个位置，比如，我们得到 “ttooo”，是因为 “o” 是一个比较宽的字符。我们已经删除了多余的 “t” and “o”，但是，如果要识别的字符是 “too”，我们应该怎么办？如果删除了多余的 “o”，将会给我们错误的答案。我们应该如何处理呢？&lt;/li>
&lt;/ol>
&lt;p>CTC 解决了几个问题&lt;/p>
&lt;ol>
&lt;li>我们只需要告诉 CTC loss function，文本在图像中出现了，因此我们忽略位置和宽度文中在图像中。&lt;/li>
&lt;li>不需要更多的文本识别处理&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://i0.wp.com/cdn-images-1.medium.com/max/1760/1*rfeNj9aNfUSqA-SXB_i4Bw.png"
loading="lazy"
alt="img"
>&lt;/p>
&lt;h2 id="ctc-如何工作的">CTC 如何工作的
&lt;/h2>&lt;p>就像是我们已经讨论的，我们不希望在图像的每一列标注数据（这曾经被我们成为时间步）。神经网络的训练将会被 CTC 损失函数所指引。我们只需要把数据矩阵给 CTC 函数，以及对应的真实值即可。但是它是怎么知道每一个字符出现的呢？他不知道。相对而言，它尝试了图片中所有的真实文本，以及计算了所有的加和。通过这个方式， This way, the score of a GT text is high if the sum over the alignment-scores has a high value.&lt;/p>
&lt;h2 id="编码文本">编码文本
&lt;/h2>&lt;p>如何编码重复的文本曾经是一个问题。这个问题通过引入一个虚假字符来解决了（称为空，但是不要把它和真正的 space 混淆。）。这个特殊的字符被标记为 “-”，在下面的文本中。我们使用了一个聪明的编码策略来解决重复字符的问题。当编码一个文本的时候，我们可以随机加入许多空在任何位置中，当我们解码的时候，我们将会把的这些删除。但是，我们必须在重复字符串中加入空，例如 “he&lt;strong>ll&lt;/strong>o”，如此一来，重复字符就不是问题了。&lt;/p>
&lt;p>Let’s look at some examples:&lt;/p>
&lt;p>“to” → “—ttttttooo”, or “-t-o-”, or “to” “too” → “—ttttto-o”, or “-t-o-o-”, or “to-o”, but not “too”&lt;/p>
&lt;p>正如你所见，这些模式也允许我们简单的创建一些相同文本串的不同对取，比如 “t-o”, 以及 “too”，以及 “-to”，所有的表示都是同一个文本 “to”，但是通过对图片不同对其获得的。神经网络被训练于输出一个编码的文本（在神经网络的矩阵中编码）。&lt;/p>
&lt;h2 id="损失计算">损失计算
&lt;/h2>&lt;p>我们需要计算每一个损失值，这个损失值是由图像核真实文本给出来训练 NN 的。你已经知道 NN 输出一个矩阵，包含一个得分，为每个文本在每个时间步上。一个小矩阵在图三中展示：有许多的时间步（t0, t1），以及三个字符（”a”, “b”, 以及 blank “-“).&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/cdn-images-1.medium.com/max/1760/1*BFQYgGofh6HOxnGdkJnO-w.png?w=625&amp;amp;#038;ssl=1"
loading="lazy"
alt="img"
>&lt;/p>
&lt;p>此外，你已经知道，loss 是通过加和所有积分来进行计算的，通过这个方式，字符出现在图片的哪个位置不重要。&lt;/p>
&lt;p>对于一个 alignment 的得分（或者 path；在文学中一般这么称呼）通过将相应的字符相乘。在上面的例子中，path”aa” 的得分是：0.4*0.4=0.16，”a-” 的得分是 0.4*0.6=0.24，”-a” 的得分是 0.6*0.4=0.24. 为了获得 GT 文本的得分，我们加和这个文本的所有 path 的得分。我们假设，GT 文本是 “a”，我们已经计算了所有长度为 “2” 的 path，分别是 “aa”，“a-”，“-a”，我们已经计算了这些 path 所有的得分，所以我们只需要把他们加起来，得到 0.4×0.4 + 0.4×0.6 + 0.6×0.4 = 0.64。如 GT 文本可能是 “”，我们可以看到只有一种相关的 path，那么就是 “–”，获得的得分是 0.6×0.6 =0.36.&lt;/p>
&lt;p>如果仔细看，你已经发现我们计算了 GT 的可能性，但是不是 loss 值，而然，loss 知识概率的负对数。这个 loss 值是反向传播算法以及 NN 的参数更新使用的，我这里没有进行详细的讨论。&lt;/p>
&lt;h2 id="解码">解码
&lt;/h2>&lt;p>当我们训练一个 NN，我们想要使用它来识别那些之前没有看到的图像。或者更多在更多的技术术语：我们想要计算，NN 输出的矩阵最可能是什么。你已经知道一个方法来计算给出文本的得分，但是现在，我们没有被给出任何文本，事实上，它正是我们正在寻找的文本。尝试所有可能的文本，如果他们只有很少的时间步以及字符，但是对于练习用例而言，这不可行。&lt;/p>
&lt;p>一个简单而快速的算法，是最佳 path 解码，包含两个步骤：&lt;/p>
&lt;ol>
&lt;li>它计算了最佳 path，通过获取最可能的每一个时间步的字符&lt;/li>
&lt;li>首先删除重复的字符，然后删除 path 里面所有的空。这仍然表示了识别的文本。&lt;/li>
&lt;/ol>
&lt;p>正如 FIG4 所展示的，字符是 “a”，“b” 以及 “-”（空），一共有 5 个时间步。让我们应用最佳 path decoder 来处理这个矩阵。在 t0，最可能的是“a”，同样应用于 t1，t2. 空字符在 t3 是最可能的。最后，“b” 是 t4 时刻最可能的。这将给出我们一个 path“aaa-b”，我们删除了重复的字符，这将会返回“a-b”，然后我们删除 path 中的空，这给我们一个“ab”，作为我们输出的识别结果。&lt;/p>
&lt;p>&lt;img src="https://i0.wp.com/cdn-images-1.medium.com/max/1600/1*1_5KnLvaTkGUFoyat2jHcQ.png?w=625&amp;amp;#038;ssl=1"
loading="lazy"
alt="img"
>&lt;/p>
&lt;p>最佳 path 解析是，当容纳，仅仅是一个近似。构建样例容易给出错误的结果，比如用这个方法构建 FIG3，将会得到 “”，作为识别文本。但是，我们已经知道“” 结果的概率是 0.36，而 “a” 的概率是 0.64。而然，近似算法经常在练习的情景下给出比较好的结果。也有许多其他的比较好的 decoder，例如 beam-search，prefix-search 以及 token passing，这些关于语言结构的方法，都有利于提升结果。&lt;/p>
&lt;h2 id="结论以及展望">结论以及展望
&lt;/h2>&lt;p>首先我们看得是，神经网络如何解决这个问题；然后，我们展示了 CTC 如何解决这些问题，然后，我们解释了 CTC 为啥能够工作，如何计算的 loss，以及如何解码 CTC 训练的 NN。&lt;/p>
\[1] introduce the CTC operation, the paper also shows all the relevant math. If you are interested in how to improve&amp;nbsp;decoding,&amp;nbsp;take a look&amp;nbsp;at the articles about beam search&amp;nbsp;decoding&amp;nbsp;[2\]\[3\]\[4\]\[5\]&lt;p>. Finally, if you want to look at the bigger picture of how to recognize (handwritten) text, look at my article on how to build a handwritten text recognition system [6].&lt;/p></description></item><item><title>使用主动学习加速机器学习</title><link>https://svtter.cn/p/%E4%BD%BF%E7%94%A8%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link><pubDate>Tue, 20 Nov 2018 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/%E4%BD%BF%E7%94%A8%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</guid><description>&lt;blockquote class="wp-block-quote">
&lt;p>
一篇 medium 文章的渣翻
&lt;/p>
&lt;p>&lt;cite>&lt;a class="link" href="https://becominghuman.ai/accelerate-machine-learning-with-active-learning-96cea4b72fdb" target="_blank" rel="noopener"
>https://becominghuman.ai/accelerate-machine-learning-with-active-learning-96cea4b72fdb&lt;/a>&lt;/cite>&lt;/p>
&lt;/blockquote>
&lt;p>让我们讨论一下主动学习。我相信这个方法可以极大的增速，以及减少许多机器学习工程的花费。这篇文章我将从两个部分说明这个问题。在第一部分，我给出了一个极高的层级的主动学习的说明，以及如何把它利用到机器学习工程中。在第二部分，深入到一个主动学习 demo 中。&lt;/p>
&lt;h2 id="第一部分">第一部分
&lt;/h2>&lt;h3 id="主动学习是如何工作的">主动学习是如何工作的
&lt;/h3>&lt;p>让我们通过一个很简单的概览，来看看机器学习是如何工作地。&lt;figure class="wp-block-image">&lt;/p>
&lt;p>&lt;img src="https://i2.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdd3awts2j218g0j4wj2.jpg?w=625" alt="Repeat thousands of times and you get a trained model!" data-recalc-dims="1" />&lt;/figure>&lt;/p>
&lt;p>许多机器学习模型是巨大的猜疑机器——他们看了许多数据，计算出一个猜测的结果，检查他们的答案，微调一下，然后再试试。在许多数据之后，模型将会变得十分准确。&lt;/p>
&lt;h3 id="标记数据">标记数据
&lt;/h3>&lt;p>…&lt;/p>
&lt;h3 id="主动学习">主动学习
&lt;/h3>&lt;p>主动学习是一种方法，有时可以极大减少标记样本的数量。它通过专家标记样本来完成这个工作。&lt;/p>
&lt;p>不使用全部的数据一次标注所有数据，主动学习优先处理那些让模型感到困惑的数据，并且仅仅需要好那些数据的标签。模型在小样本数据上进行训练，然后根据那些最令模型疑惑的数据，请求更多的标签。&lt;/p>
&lt;p>通过优先处理那些最迷惑的样本，模型可以专注于提供一些最有价值的信息。这帮助模型训练的更快，并且让专家跳过那些对于模型帮助不是很大的数据。结果是，我们可以很大程度上减少标记样本的数量，并且我们仍然得到一个很好的模型。这意味着节省时间和金钱！&lt;/p>
&lt;h2 id="第二部分">第二部分
&lt;/h2>&lt;h3 id="mnist-例子">MNIST 例子
&lt;/h3>&lt;p>让我们看一下实际的主动学习样本。&lt;/p>
&lt;p>使用文档良好的 MNIST 数据集，以及经典的 Tensorflow 卷积神经完了过。一个聪明的模型和架构可以做的更好，但是我们想要直接使用这个模型。&lt;/p>
&lt;p>MNIST 数据集是公开可获取得的数据集，包含了大量的手写数字，以及数值标签。它经常被使用于机器学习入门教程，因为他的标记数据质量很高，并且简单地模型也可以表现的不错。&lt;figure class="wp-block-image">&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdd4dewm6j20ho04fq38.jpg?w=625" alt="" data-recalc-dims="1" />&lt;/figure>&lt;/p>
&lt;h3 id="设计">设计
&lt;/h3>&lt;p>这个工程包含两个部分：&lt;/p>
&lt;ol>
&lt;li>在训练模型的时候，模仿主动学习&lt;/li>
&lt;li>在严格的模型上确定主动学习的效率&lt;/li>
&lt;/ol>
&lt;h3 id="训练一个模型">训练一个模型
&lt;/h3>&lt;p>我们使用 mini-batch 训练。这个模型仅仅在训练集中，看一个小数量样本，通过小数量样本进行学习。&lt;/p>
&lt;p>这里，我们可以看到一个正常的（非 – 主动学习）的训练过程，模型在一个随机结合的小批次上进行训练。每在小 – 批次训练中的迭代，都在测试记上运行模型（不作为训练集的一部分）来追踪模型是怎样增长的。我提供了准确率以及 cross-entropy 损失（就像是平均误差一样）。在这里，每一个小批次有个 10 个例子，我运行了 2000 批次（20000 个标注）。&lt;figure class="wp-block-image">&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdd4pv3b2j218g0fhjtx.jpg?w=625" alt="" data-recalc-dims="1" />&lt;/figure>&lt;/p>
&lt;p>对于这个分类任务，我们试图把 0~9 的数字进行分类，意味着随机猜测仅有 10% 的准确率。简单的神经网络已经做的不错了。&lt;/p>
&lt;h3 id="模拟主动学习">模拟主动学习
&lt;/h3>&lt;p>获得主动学习结果有一点小技巧。我们不在数据集中的随机选择数据，相反，模型将会评估许多在训练集中的例子，然后将置信度最小的数据作为小 – 批次（在这个工程中，我查看了 1000 个在训练集中的随机样本，来确定置信度至少为 10）。在那里，模型将会像处理小 – 批次数据一样处理进行训练过程，它将会重复这个过程来更新模型。就像是在 “非 – 主动学习” 样例中，每经过一些迭代，我将会在测试记上运行模型，追踪模型的训练过程。&lt;figure class="wp-block-image">&lt;/p>
&lt;p>&lt;img src="https://i2.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdd5z4a18j218g0ljjwd.jpg?w=625" alt="" data-recalc-dims="1" />&lt;/figure>&lt;/p>
&lt;p>有许多很好的文章来说明如何实现 主动学习。在这里，我仅仅想要把事情做的简单一些。这个模型使用一个 “softmax” 来生成概率——在这个例子中，是数字 0~9。” 置信度” 通过选择” 最大的概率减去最小的概率 “。模型越自信，这个差值越大。（置信度不意味着准确率）。&lt;figure class="wp-block-image">&lt;/p>
&lt;p>&lt;img src="https://i0.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdd96i561j218g0ejte0.jpg?w=625" alt="" data-recalc-dims="1" />&lt;/figure>&lt;/p>
&lt;p>主动学习过程，使用了那些置信度比较低的数据，并且在上面进行训练。并且当然的，当模型改变了，它的置信度也一样会改变。&lt;/p>
&lt;p>MNIST 数据集已经有了我们需要的标签，但是这个过程，在 mini-batch 中，模拟询问了专家，来获取标签。在通常情况下，专家会随机被提问，来获取数据。在主动学习的例子中，模型会选择那些数据，希望专家进行标注。&lt;/p>
&lt;p>让我们来看一下主动学习结果 VS 一般的结果。注意 y-axis。&lt;figure class="wp-block-image">&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxddharrn6j218g0h378k.jpg?w=625" alt="" data-recalc-dims="1" />&lt;/figure>&lt;/p>
&lt;p>通过 mini-batch（8000 标签），主动学习的方法匹配了 2000 mini-batch（20000label）数据的准确率。所以，使用一接近一半的数据，主动学习可以达到同样地准确率。&lt;/p>
&lt;h3 id="倾斜数据的二分类任务">倾斜数据的二分类任务
&lt;/h3>&lt;p>主动学习可以大施拳脚的地方是，数据的强烈偏差。&lt;/p>
&lt;p>训练一个模型的时候，重要的不仅仅是标记的数据，还有不同数据的，合理的不同表示。如果我们尝试在 MNIST 上训练一个模型，而没有任何包含 3 的数据，收集多少数据并不重要，重要的是我们的模型不可能区分 3。如果我们仅仅含有一小部分 3，我们仍然会面临一个问题，就是模型仅仅会准确的区分其他数字，也就是那些有更好表达的数据。&lt;/p>
&lt;p>数据的偏差，不均衡对于 MNIST 数据集中不存在，但是它的确是一个真实世界的问题。如果我们训练一个模型来识别 CT 中的脑瘤，大多数 CT 图像不会含有肿瘤图像，所以标注 “肿瘤” 的数据将会远不均衡于 “非肿瘤” 的样本数据。因为主动学习优先考虑的例子不那么自信，因此主动学习可能有助于识别 “异常 “或代表性不足的数据并且确定优先级。&lt;/p>
&lt;p>我们在 MNIST 上模拟一下 skew 的问题。重新定义 MNIST 的问题，定义成 3 或者非 3，然后，非 3 的数据有 90%，而 3 的数据仅有 10%。所以愚蠢的策略将会在” 非 3“上达到 90% 的准确率，让我们看一下主动学习是如何做的：&lt;figure class="wp-block-image">&lt;/p>
&lt;p>&lt;img src="https://i2.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdf9177f8j218g0hdwkb.jpg?w=625" alt="" data-recalc-dims="1" />&lt;/figure>&lt;/p>
&lt;p>在使用主动学习的时候，在 500mini-batch（5000labels）我们就达到，甚至更好地准确率。相比之下，cross-entropy 算法，通过 2000 mini-batch。主动学习减少了 4 倍的数据量。主动学习是如何做到的？看下图。&lt;figure class="wp-block-image">&lt;/p>
&lt;p>&lt;img src="https://i2.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdfe5ahvnj20ua0nkjv0.jpg?w=625" alt="" data-recalc-dims="1" />&lt;/figure>&lt;/p>
&lt;h2 id="后续">后续
&lt;/h2>&lt;p>未完，后面翻不翻看心情。。也不知道工业界玩 active learning 的多不多。&lt;/p></description></item><item><title>Keras坑</title><link>https://svtter.cn/p/keras%E5%9D%91/</link><pubDate>Thu, 01 Feb 2018 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/keras%E5%9D%91/</guid><description>&lt;p>使用Keras做分类的时候踩了一个坑，也是拿来主义的锅，估计也有不少同志遇到。&lt;/p>
&lt;p>在进行分类的时候，往往使用&lt;code>categorical_crossentropy&lt;/code>，有时候萌新（像我）会用&lt;code>binary_crossentropy&lt;/code>，虽然结果可能上浮30%，但是这个结果是不对的。&lt;code>model.fit&lt;/code>以及&lt;code>model.evaluate&lt;/code>给出的&lt;code>acc&lt;/code>的值都是有问题的，正确的计算方法应该是：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Actual accuracy calculated manually:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y_pred&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_test&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">acc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">sum&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_test&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">])&lt;/span>&lt;span class="o">==&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_pred&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10000&lt;/span>&lt;span class="p">)])&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">10000&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">numpy&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 返回沿轴axis最大值的索引。&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>使用测试样本的数量代替&lt;code>10000&lt;/code>，输出的acc才是正确的结果。&lt;/p>
&lt;p>这个方法是使用二分类的时候才能使用的，label的个数多于2就不能使用。&lt;/p>
&lt;h2 id="参考">参考
&lt;/h2>&lt;p>&lt;a class="link" href="https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance" target="_blank" rel="noopener"
>https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance&lt;/a>&lt;/p></description></item></channel></rss>