<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>VLM on Svtter's Blog</title><link>https://svtter.cn/tags/vlm/</link><description>Recent content in VLM on Svtter's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 30 Sep 2025 11:54:06 +0800</lastBuildDate><atom:link href="https://svtter.cn/tags/vlm/index.xml" rel="self" type="application/rss+xml"/><item><title>Why Agent</title><link>https://svtter.cn/p/why-agent/</link><pubDate>Tue, 30 Sep 2025 11:54:06 +0800</pubDate><guid>https://svtter.cn/p/why-agent/</guid><description>&lt;p&gt;一直以来我都有一个问题，为什么我们需要 agent framework，大模型不够用吗？&lt;/p&gt;
&lt;p&gt;最近深度使用了几个工具，以及参与几个 agent 项目后，我得到了结论。&lt;/p&gt;
&lt;h2 id="llm-的问题"&gt;LLM 的问题
&lt;/h2&gt;&lt;p&gt;之所以要有 agent，最大的问题是，llm 本身的局限性。&lt;/p&gt;
&lt;p&gt;首先，我认为最大的问题是 context 长度。虽然现在很多模型 context 都变长了很多（GPT-4 Turbo 128K，Claude-3.5 Sonnet 200K，Gemini-1.5 Pro 最高 2M），但是对于真正复杂的任务来说，还是不够长。比如你要处理一个大型代码库，或者分析很多文档，这点 context 还是不够用。而且这些长 context 处理起来费用很高，也很慢。&lt;/p&gt;
&lt;p&gt;此外，则是其他能力的问题。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对于视觉能力，现在的 VLM 虽然已经很强了，但是在一些特定场景下，传统的 CV 模型还是会更好一些。而且部分模型（如 &lt;a class="link" href="https://api-docs.deepseek.com/zh-cn/news/news250929" target="_blank" rel="noopener"
&gt;deepseek-v3&lt;/a&gt;）本身就没有视觉能力。&lt;/li&gt;
&lt;li&gt;LLM 本身不能直接访问数据库、文件系统、网络服务等外部资源。&lt;/li&gt;
&lt;li&gt;一些专门的工具，比如代码执行、数学计算、数据分析等，需要通过 MCP 等协议才能给 LLM 用。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="agent-可以做什么"&gt;agent 可以做什么
&lt;/h2&gt;&lt;p&gt;除了上述原因之外，应该还有其他原因，但是我目前没有在实践中遇到。欢迎大家补充。但以上原因足够表明我们需要 agent。下文是几个常见的 agent。&lt;/p&gt;
&lt;h3 id="分领域的文本处理"&gt;分领域的文本处理
&lt;/h3&gt;&lt;p&gt;将不同文本块（context）使用不同的 agent 进行处理。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;context 节省：通过 agent，我们可以将部分 context 进行压缩，或者干脆不给 context。这样一来，context 的长度就被节省下来了。&lt;/li&gt;
&lt;li&gt;性能优势：进而，agent 中的 llm 可以聚焦在某一件具体的事情上，从而获得较好的性能。例如，如果我们将大段的文本给与 LLM，他可能无法找到关键信息。但是如果我们减少给于 LLM 的文本数量，找到关键信息就变得相对容易了。&lt;/li&gt;
&lt;li&gt;LLM 是使用相对通用的文本训练的。因此，如果我们要 agent 掌握领域知识，就需要在 context 中添加这些领域知识。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="视觉能力集成"&gt;视觉能力集成
&lt;/h3&gt;&lt;p&gt;通过 agent，我们可以集成一些传统的视觉模型，来做一些之前视觉上不好做的事情。例如，我们可以通过 mcp 与 agent 配合，增加视觉能力。&lt;/p&gt;
&lt;p&gt;常见的是&lt;a class="link" href="https://docs.bigmodel.cn/cn/coding-plan/mcp/vision-mcp-server" target="_blank" rel="noopener"
&gt;智谱的视觉 MCP&lt;/a&gt;。通过使用这个 MCP + Agent，可以增强视觉能力。&lt;/p&gt;
&lt;p&gt;我想这也是部分完全整合其他 mcp 服务的 mcp server 存在的意义。&lt;/p&gt;
&lt;h2 id="agent-框架"&gt;agent 框架
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="link" href="https://ai.pydantic.dev/" target="_blank" rel="noopener"
&gt;pydantic ai&lt;/a&gt;: 我认为是比较好用的。其优势在于将 pydantic basemodel 引入了 agent 框架，比较容易调试。我测试了其与 &lt;a class="link" href="https://ai.pydantic.dev/" target="_blank" rel="noopener"
&gt;qwen3 的集成&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;a class="link" href="https://www.langchain.com/" target="_blank" rel="noopener"
&gt;langchain&lt;/a&gt;: 没有在生产环境中使用过。开发环境中简单调试过。其 api 变化比较大。一个小的问题是 prompt 的处理，&lt;a class="link" href="https://svtter.cn/p/string-template-in-prompt.md/" &gt;我使用了 jinja 来解决&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Poor Performance of Large Models on Specific Tasks</title><link>https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/</link><pubDate>Thu, 19 Jun 2025 16:34:32 +0800</pubDate><guid>https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/</guid><description>&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/bg.png" alt="Featured image of post Poor Performance of Large Models on Specific Tasks" /&gt;&lt;p&gt;视觉大模型在一些具体任务上比较糟糕，对于格式化的文本比较友好。这里我以仪表识别区域的定位作为例子，展示大模型的效果。&lt;/p&gt;
&lt;h2 id="源代码"&gt;源代码
&lt;/h2&gt;&lt;p&gt;&lt;a class="link" href="https://github.com/Svtter/vl-model/pull/4" target="_blank" rel="noopener"
&gt;https://github.com/Svtter/vl-model/pull/4&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="测试的任务"&gt;测试的任务
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;将图片中的文本 boxes 提取出来。&lt;/li&gt;
&lt;li&gt;将图片中的仪表读数区域提取出来。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="测试的文件"&gt;测试的文件
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/meter-2.jpg"
width="1280"
height="1707"
srcset="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/meter-2_hu_c88be12cb06d2945.jpg 480w, https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/meter-2_hu_791a35b85a8e04bb.jpg 1024w"
loading="lazy"
alt="Original Meter"
class="gallery-image"
data-flex-grow="74"
data-flex-basis="179px"
&gt;&lt;/p&gt;
&lt;p&gt;我们可以从这个测试结果中看出不同模型的表现差异：&lt;/p&gt;
&lt;h2 id="测试结果对比"&gt;测试结果对比
&lt;/h2&gt;&lt;h3 id="bounding-boxes-作为提示词的结果"&gt;bounding boxes 作为提示词的结果
&lt;/h3&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image.png"
width="1280"
height="1707"
srcset="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_hu_21b7440b53011ad6.png 480w, https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_hu_e002cc344d0e25da.png 1024w"
loading="lazy"
alt="整体测试结果"
class="gallery-image"
data-flex-grow="74"
data-flex-basis="179px"
&gt;&lt;/p&gt;
&lt;h3 id="各模型详细表现"&gt;各模型详细表现
&lt;/h3&gt;&lt;h4 id="anthropic-claude-35-sonnet"&gt;Anthropic Claude 3.5 Sonnet
&lt;/h4&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_anthropic_claude-3.5-sonnet.png"
width="187"
height="56"
srcset="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_anthropic_claude-3.5-sonnet_hu_21e838496ec13e0c.png 480w, https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_anthropic_claude-3.5-sonnet_hu_98d3433b78049bde.png 1024w"
loading="lazy"
alt="Claude 3.5 Sonnet 测试结果"
class="gallery-image"
data-flex-grow="333"
data-flex-basis="801px"
&gt;&lt;/p&gt;
&lt;h4 id="google-gemini-25-pro"&gt;Google Gemini 2.5 Pro
&lt;/h4&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_google_gemini-2.5-pro.png"
width="690"
height="142"
srcset="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_google_gemini-2.5-pro_hu_eb7a36a86550fa2e.png 480w, https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_google_gemini-2.5-pro_hu_c6c68f3c0be13424.png 1024w"
loading="lazy"
alt="Gemini 2.5 Pro 测试结果"
class="gallery-image"
data-flex-grow="485"
data-flex-basis="1166px"
&gt;&lt;/p&gt;
&lt;h4 id="openai-gpt-4o"&gt;OpenAI GPT-4o
&lt;/h4&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_openai_gpt-4o.png"
width="120"
height="60"
srcset="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_openai_gpt-4o_hu_66da02220ce9dd8e.png 480w, https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_openai_gpt-4o_hu_895477318e72aaf7.png 1024w"
loading="lazy"
alt="GPT-4o 测试结果"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
&gt;&lt;/p&gt;
&lt;h2 id="分析总结"&gt;分析总结
&lt;/h2&gt;&lt;p&gt;从这些测试结果可以看出：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;视觉识别能力差异&lt;/strong&gt;：不同模型在处理相同视觉任务时表现出明显的性能差异&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;格式化文本处理&lt;/strong&gt;：相比视觉任务，模型在处理结构化文本时表现更加稳定&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型特性&lt;/strong&gt;：每个模型都有其独特的优势和局限性&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些结果提醒我们在选择 AI 模型时需要根据具体任务类型来评估其适用性。&lt;/p&gt;</description></item></channel></rss>