<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Beam Search on Svtter&#39;s Blog</title>
    <link>https://svtter.cn/tags/beam-search/</link>
    <description>Recent content in Beam Search on Svtter&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 23 Nov 2018 01:00:00 +0800</lastBuildDate><atom:link href="https://svtter.cn/tags/beam-search/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>beam search – 一个搜索策略</title>
      <link>https://svtter.cn/2018/11/23/beam-search-%E4%B8%80%E4%B8%AA%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5/</link>
      <pubDate>Fri, 23 Nov 2018 01:00:00 +0800</pubDate>
      
      <guid>https://svtter.cn/2018/11/23/beam-search-%E4%B8%80%E4%B8%AA%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5/</guid>
      <description>一个常用例子，BS(beam search) 用于获得与机器翻译等价的结果。对于那些不了解机器翻译的人，也肯定知道 Google Translate。
这就是为啥要讲这个。这些系统都用 BS 技术来找到与结果最等价的翻译。阅读这个 Wiki 来了解相同文件的定义。
让我们讨论一下这个使用机器翻译案例的策略。如果你是一个喜欢研究现象背后原理的人，一定要读一下 google encoder-decoder 网络架构。这个东西我就不讲了，有很多人讲。例如，如果你不知道这个架构，看看这个 quora 上的回答。
一个视角 机器翻译模型可以被认为是一种 “条件语言模型”，对于…
让我们看一下… BS B(beam 宽度) 是唯一一个调整翻译结果的超参。 B 在一般情况决定了，在每一步，要记忆的单词的个数，来变换概率。
不翻译了。。这里有更直接的结果 beam search 时在每一个时间点选择 beam_width 个最大的可能类别，然后在每个时间点 beam_width 个类别组成的空间里寻找整体概率最大的一条路径，得到最后得识别输出。而 greedy search 则直接在每个时间点寻找概率最大的类别，然后依次组成这个路径。也就是说，greedy search 是 beam_width=1 版本的 beam search。上图是 CTC 论文里 greedy search 示意图。</description>
    </item>
    
  </channel>
</rss>
