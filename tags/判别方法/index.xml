<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>判别方法 on Svtter's Blog</title><link>https://svtter.cn/tags/%E5%88%A4%E5%88%AB%E6%96%B9%E6%B3%95/</link><description>Recent content in 判别方法 on Svtter's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Mon, 04 Sep 2023 14:07:19 +0800</lastBuildDate><atom:link href="https://svtter.cn/tags/%E5%88%A4%E5%88%AB%E6%96%B9%E6%B3%95/index.xml" rel="self" type="application/rss+xml"/><item><title>生成模型和判别模型</title><link>https://svtter.cn/p/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 04 Sep 2023 14:07:19 +0800</pubDate><guid>https://svtter.cn/p/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/</guid><description>&lt;p>在李航的书中讲到，监督学习方法可以分为生成方法（generative approach）和判别方法 (discriminative approach) 。所学习到的模型分别称为生成模型和判别模型。&lt;/p>
&lt;p>之前一直搞不清楚这俩模型区别到底是什么，课程中也只是学习了一下条件概率和联合概率分布，理解总是有些偏差。
对于生成方法，模型通过对数据进行学习，得到联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型。
也就是说:&lt;/p>
$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$&lt;p>这个方法之所以称之为生成方法，是因为模型表示了&lt;strong>给定输入$X$产生输出$Y$的生成关系&lt;/strong>。
典型的生成模型有朴素贝叶斯法和隐马尔可夫模型。&lt;/p>
&lt;p>判别方法由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测模型，即判别模型。
判别方法关心的是给定输入$X$，应该预测什么样的输出$Y$。&lt;/p>
&lt;p>典型的判别模型包括：k近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。&lt;/p>
&lt;p>隐变量是不可观测的随机变量。&lt;/p>
&lt;h2 id="区别">区别
&lt;/h2>&lt;ol>
&lt;li>对生成方法而言，学习收敛的速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型。如果存在隐变量，判别方法就不能用。&lt;/li>
&lt;li>对于判别方法而言，直接学习条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率更高。&lt;/li>
&lt;/ol>
&lt;p>我的理解：区别是一个是学习$P(X,Y)$，因此如果输入的样本发生变化，即$P(X)$发生变化，那么$P(Y|X)$只需要进行简单计算。相比之下，
直接使用$P(Y|X)$的模型则可能需要重新训练。&lt;/p>
&lt;h2 id="附录">附录
&lt;/h2>&lt;p>决策函数指的是应用监督学习的方法：$Y=f(X)$，或者条件概率分布$P(Y|X)$。&lt;/p></description></item></channel></rss>