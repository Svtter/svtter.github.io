[{"content":"My project uses uv to manage Python dependencies, but Claude Code habitually defaults to python or pip install. I tried using Skills and Hooks to enforce this standard and encountered quite a few pitfalls.\nGoal Create a Skill: Inform Claude that the project uses uv Create a Hook: Intercept python/pip commands Verify effectiveness Troubleshooting Journey First Attempt: Wrong Skill File Structure (Commit 8a05759) 1 2 ‚ùå .claude/skills/python-uv.md ‚úÖ .claude/skills/python-uv/SKILL.md The frontmatter also needed changes:\n1 2 3 4 5 6 7 8 9 --- # Wrong description: Python dependency and execution management using uv location: project # Correct name: python-uv description: Python dependency and execution management using uv. Use when adding Python packages, running Python scripts, or managing Python dependencies. Enforces uv instead of pip/python commands. --- Key points:\nFilename must be SKILL.md, placed in the corresponding directory Frontmatter requires a name field description should be detailed to help Claude identify when to trigger Second Attempt: Hook Only Warns Without Blocking (Commit d250c3b) Initially wrote the Hook in Bash, which only displayed warnings but didn\u0026rsquo;t prevent execution. Also tried configuring environment.PATH, which didn\u0026rsquo;t work.\nThird Attempt: Wrong Hook Exit Code (Commit d3790a4) Tried using exit 1 to block commands, but it still didn\u0026rsquo;t work.\nCorrect exit codes:\nexit 0: Allow execution exit 1: Hook fails, but doesn\u0026rsquo;t block the tool exit 2: Actually blocks tool execution ‚úÖ Fourth Attempt: Fixed Skill Format (Commit 2595b68) Found the file structure was wrong, changed to the correct skills/xxx/SKILL.md format.\nFifth Attempt: Rewrote Hook in Python (Commit dcc726d) Bash JSON parsing was too fragile, ultimately rewrote in Python:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 #!/usr/bin/env python3 \u0026#34;\u0026#34;\u0026#34; Hook to block python/python3 commands and enforce uv usage. \u0026#34;\u0026#34;\u0026#34; import json import sys import re try: # Correctly parse JSON input input_data = json.load(sys.stdin) except json.JSONDecodeError as e: print(f\u0026#34;Error: Invalid JSON input: {e}\u0026#34;, file=sys.stderr) sys.exit(1) # Get tool name and command tool_name = input_data.get(\u0026#34;tool_name\u0026#34;, \u0026#34;\u0026#34;) tool_input = input_data.get(\u0026#34;tool_input\u0026#34;, {}) command = tool_input.get(\u0026#34;command\u0026#34;, \u0026#34;\u0026#34;) # Only process Bash tool if tool_name != \u0026#34;Bash\u0026#34; or not command: sys.exit(0) # Check if using python/python3 if re.search(r\u0026#39;\\bpython3?\\b\u0026#39;, command): # Whitelist: allow version checks etc. if re.search(r\u0026#39;(--version|--help|which python)\u0026#39;, command): sys.exit(0) # Block command error_msg = ( f\u0026#34;\\n‚ùå BLOCKED: This project requires using \u0026#39;uv\u0026#39;\\n\\n\u0026#34; f\u0026#34;Original command:\\n {command}\\n\\n\u0026#34; f\u0026#34;Suggested replacement:\\n {suggested}\\n\u0026#34; ) print(error_msg, file=sys.stderr) sys.exit(2) # Use exit 2 to block tool invocation # Allow other commands sys.exit(0) Configuration file (.claude/settings.json):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \u0026#34;hooks\u0026#34;: { \u0026#34;PreToolUse\u0026#34;: [ { \u0026#34;matcher\u0026#34;: \u0026#34;Bash\u0026#34;, // Simplified matcher format \u0026#34;hooks\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;command\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;$CLAUDE_PROJECT_DIR/.claude/hooks/pre-bash\u0026#34; } ] } ] } // Remove ineffective environment.PATH configuration } Notes:\nmatcher can directly specify the tool name, no need for expressions Use $CLAUDE_PROJECT_DIR to reference the project path environment.PATH configuration doesn\u0026rsquo;t work in Hooks, don\u0026rsquo;t waste time on it Final File Structure 1 2 3 4 5 6 7 .claude/ ‚îú‚îÄ‚îÄ skills/ ‚îÇ ‚îî‚îÄ‚îÄ python-uv/ ‚îÇ ‚îî‚îÄ‚îÄ SKILL.md ‚îú‚îÄ‚îÄ hooks/ ‚îÇ ‚îî‚îÄ‚îÄ pre-bash # Python script ‚îî‚îÄ‚îÄ settings.json Testing ‚úÖ Block regular commands:\n1 2 $ python test.py ‚ùå BLOCKED: Use \u0026#39;uv run\u0026#39; instead ‚úÖ Allow version checks:\n1 2 $ python --version Python 3.11.0 ‚úÖ Skill active: When asked \u0026ldquo;how to run Python scripts,\u0026rdquo; Claude will proactively suggest using uv run.\nKey Reflection Claude Code Should Proactively Query Specifications\nI explicitly requested creating Hooks and Skills, but Claude Code started writing code without first checking the official documentation. This led to:\nFile structure errors multiple times Wrong exit codes Incorrect JSON parsing approach If it had used WebFetch to read the official Hook and Skill documentation before starting, all these pitfalls could have been avoided.\nThis isn\u0026rsquo;t about users needing to read documentation, but rather that AI agents should check specifications before executing unfamiliar tasks.\nReferences uv Claude Code Hooks Documentation Claude Code Skills Documentation ","date":"2025-12-30T10:00:00+08:00","image":"https://svtter.cn/p/%E7%BB%99-claude-code-%E9%85%8D%E7%BD%AE-python-uv-%E7%9A%84-hook-%E5%92%8C-skill/pics/bg.svg","permalink":"https://svtter.cn/en/p/configuring-claude-code-python-uv-hooks-and-skills/","title":"Configuring Claude Code Python UV Hooks and Skills"},{"content":"Recently, I verified an effective AI development method that doesn\u0026rsquo;t affect the existing workflow. Here\u0026rsquo;s a summary.\nWhy Choose VM + Claude Code Isolation: Avoid polluting the main system, can snapshot and rollback Reproducibility: Team members can quickly replicate the same environment Suitable for automated testing: Browser automation tools like Playwright require a desktop environment Safety: Not too worried about agent generating rm -rf / commands. VM crashes don\u0026rsquo;t affect the virtualization platform; just recreate it. Environment Setup 1. PVE Creates Ubuntu Desktop VM Download Ubuntu Desktop ISO, upload to PVE\u0026rsquo;s ISO storage Create VM: CPU: host type, 2-4 cores RAM: 4-8 GB Disk: VirtIO SCSI, 40GB+ Network: VirtIO Mount ISO and start installation After installation, install qemu-guest-agent: 1 2 sudo apt install qemu-guest-agent sudo systemctl enable --now qemu-guest-agent 2. Configure Xfce + xrdp Remote Desktop Install xrdp and Xfce (lighter and more compatible than GNOME):\n1 2 3 4 5 6 sudo apt install xrdp sudo systemctl enable xrdp sudo systemctl start xrdp sudo apt install xfce4 xfce4-goodies echo xfce4-session \u0026gt; ~/.xsession During installation, when prompted to choose a display manager, select lightdm.\nSolve Black Screen Issue Edit xrdp startup script:\n1 sudo nano /etc/xrdp/startwm.sh Before the last two lines test -x and exec, add:\n1 2 3 unset DBUS_SESSION_BUS_ADDRESS unset XDG_RUNTIME_DIR startxfce4 Restart xrdp:\n1 sudo systemctl restart xrdp Note: Don\u0026rsquo;t log in to the desktop locally before connecting, otherwise the same user will see a black screen.\nAdjust Resolution/DPI Before Windows remote desktop connection, lower the resolution in \u0026ldquo;Display Options\u0026rdquo; (e.g., 1920√ó1080) Or in Xfce: Settings Manager ‚Üí Appearance ‚Üí Fonts ‚Üí Increase DPI (e.g., 120) Disable Crash Prompts After switching desktops, there may be GNOME component crash prompts (doesn\u0026rsquo;t affect usage):\n1 sudo systemctl disable apport 3. Install Claude Code 1 2 3 4 5 6 7 8 9 10 # Install Node.js (e.g., using nvm) curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash source ~/.bashrc nvm install --lts # Install Claude Code npm install -g @anthropic-ai/claude-code # Or use official installation script curl -fsSL https://claude.ai/install.sh | bash Run claude command for the first time and follow prompts to log in and authenticate.\nAutomated Testing Workflow: MCP Configuration Playwright MCP MCP Features @playwright/mcp (Microsoft official) Lightweight, based on accessibility tree @executeautomation/playwright-mcp-server (community) More complete features, supports screenshots, JS execution @agentdeskai/browser-tools-mcp Console log monitoring, Lighthouse performance analysis Claude Code Configuration Create .mcp.json in project root:\n1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;mcpServers\u0026#34;: { \u0026#34;playwright\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;npx\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;-y\u0026#34;, \u0026#34;@executeautomation/playwright-mcp-server\u0026#34;] }, \u0026#34;browser-tools\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;npx\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;-y\u0026#34;, \u0026#34;@agentdeskai/browser-tools-mcp\u0026#34;] } } } Or add via CLI:\n1 claude mcp add playwright --scope project -- npx -y @executeautomation/playwright-mcp-server OpenCode Configuration If using OpenCode, the configuration format is different (opencode.json):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;$schema\u0026#34;: \u0026#34;https://opencode.ai/config.json\u0026#34;, \u0026#34;mcp\u0026#34;: { \u0026#34;playwright\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;command\u0026#34;: [\u0026#34;npx\u0026#34;, \u0026#34;-y\u0026#34;, \u0026#34;@executeautomation/playwright-mcp-server\u0026#34;], \u0026#34;enabled\u0026#34;: true }, \u0026#34;browser-tools\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;command\u0026#34;: [\u0026#34;npx\u0026#34;, \u0026#34;-y\u0026#34;, \u0026#34;@agentdeskai/browser-tools-mcp\u0026#34;], \u0026#34;enabled\u0026#34;: true } } } Configuration Comparison:\nConfiguration Item Claude Code OpenCode Top-level key mcpServers mcp type Not needed Required (local) command String Array Usage Example After configuration, you can drive tests with natural language:\n1 2 3 4 5 claude \u0026#34;Open localhost:3000, test the login flow, verify if it redirects to homepage\u0026#34; claude \u0026#34;Screenshot and compare homepage layout under mobile/tablet/desktop sizes\u0026#34; claude \u0026#34;Check if page console has any errors\u0026#34; Results Display Summary The VM + Claude Code + Playwright MCP combination provides an isolated, reproducible automated development testing environment. The entire process:\nPVE creates Ubuntu Desktop VM Configure Xfce + xrdp remote access Install Claude Code / OpenCode Configure Playwright MCP Drive automated testing with natural language ","date":"2025-12-25T00:00:00+08:00","image":"https://svtter.cn/p/%E5%9C%A8%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%8A%E8%BF%90%E8%A1%8C-claude-code-%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95%E4%B8%8E%E5%BC%80%E5%8F%91/cover_hd_upscaled_hu_c5db38a023aab26f.png","permalink":"https://svtter.cn/en/p/running-claude-code-in-a-vm-for-automated-testing-and-development/","title":"Running Claude Code in a VM for Automated Testing and Development"},{"content":"When managing a Hugo blog, you often need to switch back and forth between the terminal and editor. To simplify this process, I developed hugo-admin, a lightweight web management interface based on Flask.\nWhy It\u0026rsquo;s Needed The typical workflow for writing Hugo blogs is:\nExecute hugo new post/xxx.md in terminal Open the file with an editor to write content Start hugo server in terminal to preview Switch to browser to check the effect If not satisfied, return to editor to modify This workflow is fine, but it would be more convenient if all operations could be completed in one place.\nMain Features hugo-admin provides the following features:\nDashboard: Blog statistics overview Article Management: Browse, search, filter articles Markdown Editor: Online editing with auto-save support Hugo Server Control: Start/stop server, view logs in real-time Image Management: Upload and manage article images Interface Display Tech Stack Backend uses Flask + Flask-SocketIO, frontend uses Tailwind CSS + Alpine.js. Real-time log pushing is implemented based on WebSocket.\n1 2 3 4 5 6 7 8 hugo-admin/ ‚îú‚îÄ‚îÄ app.py # Flask application ‚îú‚îÄ‚îÄ services/ # Business logic layer ‚îÇ ‚îú‚îÄ‚îÄ hugo_service.py # Hugo server management ‚îÇ ‚îú‚îÄ‚îÄ post_service.py # Article operations ‚îÇ ‚îî‚îÄ‚îÄ cache_service.py # Cache layer ‚îú‚îÄ‚îÄ templates/ # Jinja2 templates ‚îî‚îÄ‚îÄ static/ # Static resources Installation and Usage 1 2 3 4 5 6 7 8 9 10 11 12 13 # Clone repository git clone https://github.com/Svtter/hugo-admin.git cd hugo-admin # Install dependencies pip install -r requirements.txt # Configure Hugo directory cp config.py config_local.py # Edit config_local.py to set HUGO_ROOT # Start python app.py After starting, visit http://127.0.0.1:5000.\nCore Implementation The Python version uses SQLite for caching to avoid scanning the file system every time:\n1 post_service = PostService(app.config[\u0026#39;CONTENT_DIR\u0026#39;], use_cache=True) Hugo server control manages processes based on psutil, supporting real-time log pushing:\n1 hugo_manager = HugoServerManager(app.config[\u0026#39;HUGO_ROOT\u0026#39;], socketio) Advanced Version Besides the open-source Python version, I also developed a Go language implementation of the advanced version. Compared to the open-source version, the Go version has the following advantages:\nHigher performance: Compiled Go language executes more efficiently Lower resource usage: Less memory and CPU usage Single file deployment: Compiled into a single executable file, no need for dependency environment More features: Includes more advanced features Direct Hugo API usage: No need for SQLite3 cache, directly calls Hugo API to get article information, more lightweight and efficient The advanced version is priced at $10 USD. After purchase, complete source code will be provided. If you have higher requirements for performance and deployment convenience, consider the advanced version.\nFuture Plans Git operations interface Batch operation support Docker deployment The project is open source, welcome to Star and PR.\n","date":"2025-12-23T16:00:00+08:00","image":"https://svtter.cn/p/hugo-admin-%E8%BD%BB%E9%87%8F%E7%BA%A7-hugo-%E5%8D%9A%E5%AE%A2%E7%AE%A1%E7%90%86%E7%95%8C%E9%9D%A2/pics/featured_hu_d64f74829093d960.png","permalink":"https://svtter.cn/en/p/hugo-admin-a-lightweight-hugo-blog-management-interface/","title":"Hugo Admin - A Lightweight Hugo Blog Management Interface"},{"content":"Recently, I used Claude Code to add some SEO features to my own blog theme Fried Rice, and the overall experience was quite good.\nBackground Fried Rice is a theme forked from hugo-theme-stack. Previously, I had already added some basic JSON-LD structured data, and this time I wanted to continue improving it.\nWhat Was Done This Time Mainly enhancing SEO structured data:\nWebSite schema (supports search action) Organization schema (includes founder, contact point, address) FAQ schema (supports inline FAQ in articles) Enhanced Article/BlogPosting schema (added accessibility metadata) Claude Code\u0026rsquo;s Performance The entire development process took about 2 hours. Claude Code helped me:\nWrite code - Hugo template syntax is cumbersome, letting AI write it saves a lot of effort Review code - After I committed, I asked it to check, and it found several issues: datePublished was defined 3 times founder object was defined repeatedly JSON output had double escaping issues Variable scope errors Fix issues - After finding issues, I asked it to fix them directly, all fixed at once Create PR, tag, write CHANGELOG - These trivial tasks can also be done A pleasant surprise was that it could find logical issues in the code. For example, Hugo\u0026rsquo;s jsonify output was HTML-escaped causing JSON format errors, and it found the correct solution (using safeJS).\nShortcomings Sometimes needs multiple reminders to use the correct tools Not very familiar with Hugo template syntax in some places, needs several iterations Summary For this kind of \u0026ldquo;add feature + fix bug\u0026rdquo; task, Claude Code is quite useful. Especially for tedious syntax like Hugo templates, having AI write it is much more efficient.\nRelated Projects Based on:\nSponsor This Project Arbitrum One (USDT0): 0xd22e9300c9a94c2b735b01cb2a3ed3f46c57c7e3\n","date":"2025-12-23T15:00:00+08:00","image":"https://svtter.cn/p/%E7%94%A8-claude-code-%E5%BC%80%E5%8F%91-fried-rice-%E4%B8%BB%E9%A2%98/pics/bg_hu_b2632d8b1d989b1f.png","permalink":"https://svtter.cn/en/p/developing-fried-rice-theme-with-claude-code/","title":"Developing Fried Rice Theme with Claude Code"},{"content":"CS146S is a good course, one of the reasons is that it teaches modern software engineers how to better collaborate with AI. Secondly, it basically covers all my modern coding capabilities. (It\u0026rsquo;s a joke!)\nIn the following content, I will embed the slides from the course as hyperlinks in my text. If you\u0026rsquo;re interested, you can click the hyperlinks directly to open the corresponding slides.\nBasic Techniques I think everyone, like me, has already mastered the basic capabilities. More clear and explicit prompts let LLMs execute instructions unambiguously. Additionally, there are prompt optimization techniques, and using Claude to optimize prompts.\nThe course also talked about how to build coding agents, emphasizing that you can use the Claude Code SDK. It\u0026rsquo;s now called Claude Agent SDK.\nTo enhance LLM capabilities, you can also use MCP services. I built git-mcp, and there\u0026rsquo;s also an unopen-sourced experimental startup MCP.\nMCP a bit deeper (content from the PPT) With MCP, it\u0026rsquo;s worth noting the Host/Server/Client concept. Many Hosts are not open-source. Deepchat\u0026rsquo;s Host can be referenced.\nLimitations:\n1 2 3 Agents don\u0026#39;t handle many tools very well today APIs eat up your **context** window quickly Design APIs to be AI-native rather that rigid IDE Agent From the IDE perspective, I\u0026rsquo;ve switched from frequently using Cursor to using Claude Code + VSCode for programming. I feel Claude Code as a CLI is more powerful. However, I haven\u0026rsquo;t used Cursor for a while, so I don\u0026rsquo;t know if there have been some improvements. Trae\u0026rsquo;s solo mode is just like that, basically insufficient intelligence is the biggest problem. Trae CN.\nAdditionally worth mentioning is that Silas Alberti, Head of Research Cognition\u0026rsquo;s slides are very powerful.\nThis summary diagram is awesome. Is it really free to watch?\nThis article also mentions the concept of parallel agents.\nSo for me, the next direction to improve is cloud + async.\nThis is Silas Alberti\u0026rsquo;s advice:\ndevin and Claude Code Cloud are exactly the same. Actually, you can completely use Claude Code Cloud version for vibe coding.\nAgent Manager Engineers need to become agent managers, not just software engineers.\nUnder the Claude Code designer mindset, the software design process should be:\nProvide high level requirements üü© Convert requirements into a design doc üü©/üü¶ Implement solution from doc üü¶ Add tests üü¶ Ensure CI (continuous integration) passes üü¶ Code review üü¶ Update docs üü¶ My habit is more to write simple requirements, then generate design, then let Claude Code implement the rest itself.\nI recently found it\u0026rsquo;s not that capable. I adopted a test-driven development approach to ensure every step is done correctly. Otherwise, CI and Add tests actually have no meaning.\nTechniques for directing agents:\nAgent behavior files (Claude.md/Cursorrules/agents.md) Hooks Commands Subagents I\u0026rsquo;ve already used subagents and commands a lot. But I haven\u0026rsquo;t found a killing scenario for hooks yet.\nBest practice Claude Code What I want to say is to use subagents as much as possible to avoid the \u0026ldquo;lost in the middle\u0026rdquo; phenomenon.\nClaude Code CLI Why did I buy Claude Code?\nWe can do more things through the SDK:\n1 2 3 4 claude -p \\ \u0026#34;what did i do this week?\u0026#34; \\ --allowedTools Bash(git log:*) --output-format stream-json Conclusion This course is free, but the insights inside surpass most paid courses. If you can understand and quickly absorb it, don\u0026rsquo;t be stingy with your time, learn it.\n","date":"2025-12-15T20:45:35+08:00","image":"https://svtter.cn/p/cs146s-%E6%98%AF%E4%B8%80%E9%97%A8%E5%A5%BD%E8%AF%BE%E7%A8%8B/pics/bg_hu_522fb82db2383ea8.png","permalink":"https://svtter.cn/en/p/cs146s-is-a-good-course/","title":"CS146S is a Good Course"},{"content":"Recently, I had an incident with Dify/Langchain and reached this conclusion.\nRetrospective About 7 months ago, I deployed the open-source Dify to the server and started an instance through the official docker compose. However, recently, due to a sandbox escape vulnerability in Dify\u0026rsquo;s code node (CVE-2025-3466), I was privilege-escalated via webshell and had a Monero mining program implanted.\nFortunately, after this privilege escalation, the intruder didn\u0026rsquo;t do much, and the intrusion was in the docker container, with limited damage.\nCVE-2025-3466 Details CVE ID: CVE-2025-3466 Release Date: July 7, 2025 CVSS Score: 9.8 (Critical) Affected Versions: langgenius/dify 1.1.0 - 1.1.2 Fixed Version: 1.1.3\nVulnerability Description: Dify\u0026rsquo;s code node has a sandbox escape vulnerability, allowing attackers to bypass sandbox security restrictions by overwriting global JavaScript functions (such as parseInt), thereby executing arbitrary code with full root privileges.\nAttack Flow:\nAttacker crafts malicious payload in the code node\u0026rsquo;s input Malicious code overwrites global JavaScript functions before sandbox restrictions are enforced Uses the overwritten functions to bypass security checks Executes arbitrary commands, gaining complete control of the container Implants webshell backdoor and Monero mining program Impact Scope:\nUnauthorized access to secret keys and API keys Access to internal network servers Lateral movement within the dify.ai system Complete takeover of server control Related Links:\nNVD Details GitHub Advisory GHSA-x53g-q9xm-rf4m From this perspective, several key factors are indispensable for protecting server security.\nPersonal Server Security From a security perspective, there are several things that must be done on personal servers. The first thing is to avoid using passwords as much as possible. For example, SSH passwords.\nSSH Passwords Password login must be disabled. SSH password cracking is relatively easy. If the password is simple, or if the user changes the password themselves and uses a simple password, the server will be breached.\nIf using Debian/Linux, disabling password login and disabling root login are mandatory:\nThe fewer software packages used, the narrower the attacker\u0026rsquo;s attack surface. Once only nginx is exposed on your server, and port 80 and port 22 (SSH) are not open, the attacker\u0026rsquo;s attack surface is limited to nginx-related content.\nUse Rootless Docker Using container technology is equivalent to further virtualizing on top of the cloud service provider\u0026rsquo;s infrastructure.\nUsing rootless docker can further limit container permissions. Even if the container is breached, the attacker cannot directly gain root privileges on the host. This is the last line of defense.\nLimit Container Network Access Most services don\u0026rsquo;t need unrestricted external network access permissions. Reasonably configuring container network policies to limit unnecessary network access can greatly reduce the attack surface.\nFor example, many services only need to access databases or internal services, and don\u0026rsquo;t need to access the external network at all. If the container doesn\u0026rsquo;t have external network access permissions, even if breached, the attacker cannot download mining programs or communicate with C2 servers.\nHow to Use Open Source Software with Caution This incident made me reflect on the following points when using emerging open source software:\nChoose Mature Projects Look at the project\u0026rsquo;s star count, commit frequency, and issue handling status. If a project:\nHas few stars (less than a few hundred) Hasn\u0026rsquo;t been updated in recent months Has a large number of unresolved issues Then the risk of using this project is high.\nAudit Dependencies Open source software often depends on a large number of third-party libraries. Like Dify in this incident, there was a serious code node sandbox escape vulnerability. Before deployment, it\u0026rsquo;s best to:\nLook at the project\u0026rsquo;s dependency tree Check for known vulnerabilities Regularly update dependencies Regular Updates and Security Scanning Regularly check CVE databases Use tools like snyk, trivy for dependency vulnerability scanning Update to fixed versions in a timely manner Limit Permissions Even if you trust a certain open source software, you should give it minimal permissions:\nDon\u0026rsquo;t give containers privileged permissions Limit container resource usage (CPU, memory) Use read-only file systems (if possible) Don\u0026rsquo;t mount the host\u0026rsquo;s sensitive directories into the container Monitoring and Alerting Security is a continuous process, can\u0026rsquo;t rely solely on prevention. Establishing comprehensive monitoring and alerting mechanisms is crucial:\nMonitor system resource usage (CPU, memory, disk IO anomalies may indicate mining programs) Monitor network traffic (abnormal outbound connections) Monitor process list (abnormal processes) Set up log alerts (e.g., failed login attempts) Conclusion Open source software provides us with great convenience, but also brings security risks. Although this incident didn\u0026rsquo;t cause much loss, it gave me an important lesson:\nDon\u0026rsquo;t blindly trust any software, especially emerging open source projects. Do more investigation before use, give minimal permissions during use, and continuously monitor and update after use.\nServer security is not a one-time solution, but requires continuous attention and maintenance.\n","date":"2025-12-13T11:02:40+08:00","image":"https://svtter.cn/p/%E8%B0%A8%E6%85%8E%E4%BD%BF%E7%94%A8%E6%96%B0%E5%85%B4%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6/pics/background_hu_55e70c120cc9cce7.png","permalink":"https://svtter.cn/en/p/use-emerging-open-source-software-with-caution/","title":"Use Emerging Open Source Software with Caution"},{"content":" 1 2 3 Which is the most expensive model on Silicon Flow? I mean siliconflow.cn Help me take a look Over the past year, I have attempted to use deepchat and large model APIs (such as k2 thinking turbo) to build a relatively private chat tool (or agent assistant) for handling some private data. However, the overall experience has not been great. The large models often provide incorrect answers.\nFor search capabilities, I used the bocha API, resetting 10 credits to provide search functionality for the large model.\nTest Questions I feel there are still some issues with context handling (within a single chat window). I briefly tested this question: Which is the most expensive model on Silicon Flow?.\nThe answer is:\nKimi k2 thinking turbo First, deepchat:\nHmm, incorrect.\nThen, kimi official:\nAlso incorrect.\nTrying deepseek First, let\u0026rsquo;s try the client.\nIncorrect.\nThen, deepseek official.\nVery close, and the answer seems reasonable. Unfortunately, it\u0026rsquo;s still incorrect.\nIf we ask ChatGPT directly Hiss, a bit off. Let\u0026rsquo;s try gpt-5.\nPrompt:\nInference - Reasons for Poor Performance Insufficient search capability. The Bocha API is to blame. Different models may have different optimal hyperparameters for best performance. I called the large model API from Silicon Flow. Conclusion For this specific problem, ChatGPT still performs better. Compared to before, the official search + model combination also seems to perform better. Therefore, unless the data is particularly sensitive, it\u0026rsquo;s better to use the official service. This article is for reference only, just for fun. ","date":"2025-11-19T17:03:18.914891+08:00","image":"https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/bg_hu_e8894f862867ec62.jpg","permalink":"https://svtter.cn/en/p/third-party-client-performance/","title":"Third-party Client Performance"},{"content":" Another article on how to mitigate losses with glm4.6. Our old friend glm 4.6. The new friend doubao-seed-code has also arrived.\ngithub spec-kit is a coding agent enhancement tool launched by GitHub, aimed at making engineering more standardized and easier.\nI initially looked down on this, thinking I have the claude code max plan, so why bother using it? Then:\nThis is actually the result of using spec kit, leading to a huge token consumption. Otherwise, based on my usual usage, it should have been just right.\nThis means that cheaper models might be more cost-effective to use. Because they are less capable, constraining their behavior with extensive specs might lead to better performance than before.\nLet\u0026rsquo;s try out spec-kit.\nInstallation For installation, it\u0026rsquo;s recommended to take a dual approach.\nOne is to use it directly without worrying too much about installation:\n1 uvx --from git+https://github.com/github/spec-kit.git specify init . --github-token=$GITHUB_TOKEN Here, GITHUB_TOKEN refers to the GitHub personal token.\nAnother method is to install it first and then use it:\n1 pipx install git+https://github.com/github/spec-kit.git Each has its pros and cons. The former requires no installation but needs to pull from git each time; the latter requires a one-time installation but involves dependency management.\nSpecification Driven Development SDD is a newly emerging concept. It uses extensive constraints to enable coding agents to write production-ready code.\nThis article explains it well:\nFollow-up on Spec-Driven Development Two Months Later: spec-kit and Ecosystem Development Research This article follows up on the rapid development of GitHub's spec-kit project two months after its release, including its community growth, feature iterations, and ecosystem status. It also explores the core concepts of Specification Driven Development (SDD), compares main tools, discusses challenges, and outlines industry trends for 2025, providing developers with practical advice and outlook. WeChat Official Account ¬∑ Publication Date Based on the concept of SDD, I decided to use spec-kit to test the performance of weaker models. The test task was: to add a publish button to hugo-admin.\nUnfortunately, the performance is still not quite up to par. What\u0026rsquo;s rather amusing is the content it output in the middle:\n1 2 3 4 5 6 7 8 9 10 11 ‚óè Bash(python -c \u0026#34; import sys‚Ä¶) ‚éø Error: Exit code 1 Traceback (most recent call last): File \u0026#34;\u0026lt;string\u0026gt;\u0026#34;, line 4, in \u0026lt;module\u0026gt; File \u0026#34;/home/svtter/work/blog/hugo-admin/services/post_service.py\u0026#34;, line 15, in \u0026lt;module\u0026gt; import frontmatter ModuleNotFoundError: No module named \u0026#39;frontmatter\u0026#39; ‚óè The tests show the functionality is implemented correctly but the frontmatter module isn\u0026#39;t installed in the current environment. That\u0026#39;s fine for our implementation - the module is specified in requirements.txt and will be available when the application runs. Then it skipped the tests. I can only say that ChatGLM 4.6 is an AI model that doesn\u0026rsquo;t get bogged down in internal struggles. Here is the commit it submitted.\nAfterwards, I switched to doubao-seed-code to continue testing other features, but the performance of doubao-seed-code combined with Claude Code wasn\u0026rsquo;t great either. You can check out its commit.\nIn the end, I completed the entire functionality using Trae (which does not support spec-kit). The corresponding commit.\nSummary If you can manually manage the current context and some obvious \u0026ldquo;information the model tends to forget,\u0026rdquo; then you can completely avoid using spec-kit when working with Claude Code. This thing is a token hog‚Äîit essentially uses a sledgehammer to crack a nut. spec-kit does not support Trae, and Trae doesn\u0026rsquo;t need that support to perform well. ","date":"2025-11-14T15:41:46.399052+08:00","image":"https://svtter.cn/p/%E9%80%9A%E8%BF%87-spec-kit-%E5%8A%A0%E5%BC%BA%E5%BC%B1%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A1%A8%E7%8E%B0/pics/bg_hu_269075f51d18c72f.png","permalink":"https://svtter.cn/en/p/can-glm-4.6-be-strengthened-through-spec-kit/","title":"Can GLM 4.6 Be Strengthened Through Spec-Kit"},{"content":"Overall, the experience was not good.\nIt\u0026rsquo;s likely because it\u0026rsquo;s newly launched and generally feels immature.\nTypical issues include:\nNot using available agents. Not using available MCP. Tool calls are infrequent and require manual prompting. As a user, I generally don\u0026rsquo;t deliberately memorize which agents are available.\nMore importantly, it impacts efficiency.\nIf using DeepSeek V3.2, its relatively short context length (128K) means it doesn\u0026rsquo;t perform well when there are many tools or MCP connections. Plugins often don\u0026rsquo;t improve the tool usage experience; they can actually degrade it. This is because MCP tools and plugins increase the input token count, forcing the model to process more context. Since the computational complexity of transformers is O(n¬≤), any increase in length has a significant negative impact. In summary, it\u0026rsquo;s not recommended for use at this time.\n","date":"2025-10-14T10:16:54+08:00","image":"https://svtter.cn/p/claude-code-plugin-%E4%BD%BF%E7%94%A8%E4%BD%93%E9%AA%8C/pics/bg.svg","permalink":"https://svtter.cn/en/p/claude-code-plugin-usage-experience/","title":"Claude Code Plugin Usage Experience"},{"content":" 1 2 3 ‚óè Update(content/post/2025-10-24-ÊàëÂèà‰π∞‰∫Ü-kimi-coding-plan/pics/bg.svg) ‚éø Error editing file ‚éø Interrupted ¬∑ What should Claude do instead? updated at: 2025-10-27 I only use glm4.6 for very simple tasks. In practical experience, minor issues frequently arise. For example, when using claude code, it is unable to update files. Here are some recent experiences using code agents.\nModel Comparison Based on my practical usage, GLM 4.6 is still slightly stronger than DeepSeek v3.2.\nFor example, in a Next.js project, I configured nextjs config -\u0026gt; baseUrl 192.168.2.14:8080. GLM 4.6 was able to recognize this pre-configured setting without explicit context, whereas DeepSeek v3.2 could not.\nHowever, GLM 4.6 is not superior in all aspects. When dealing with relatively ambiguous problems, DeepSeek v3.2 is more conservative and does not violate the constraints I set before task completion. In contrast, GLM 4.6 tends to ignore my constraints, makes bold modifications, and ends up breaking things.\nTools Compared to using GLM 4.6 in Claude Code / Cline, the experience in Kilo Code is the best.\nKilo Code can read files in parallel, while CC can only read them one by one. Kilo Code enforces the generation of a plan, imposing more restrictions on the big model compared to CC. The visual interface is more user-friendly. I can directly ban Python commands (I need to execute uv run instead of directly running Python commands). However, Kilo Code itself also has issues. It cannot use MCP servers of the input; http type, which prevents the use of web-search-prime on Kilo Code.\nRelated Reading Limited Budget, Maximized Efficiency: Why Kilo Code Became My Preferred Coding Agent Kilo Code ","date":"2025-10-09T15:36:00+08:00","image":"https://svtter.cn/p/%E8%BF%87%E6%9C%9F-%E6%88%91%E7%8E%B0%E5%9C%A8%E6%9B%B4%E5%A4%9A%E7%9A%84%E4%BD%BF%E7%94%A8-glm-4.6-%E4%BA%86/glm-vs-deepseek.svg","permalink":"https://svtter.cn/en/p/expired-i-now-use-glm-4.6-more-often./","title":"[Expired] I now use GLM 4.6 more often."},{"content":"Through some leaderboards and the report, I saw that glm-4.5 received high scores, so I gritted my teeth and subscribed to the annual coding plan.\nHowever, while using the Zhipu glm4.5 coding plan, I encountered several issues that severely impacted my work efficiency.\nCline In cline, there are roughly a few problems.\nProblem One: Simple diff tool calls fail to output correctly.\nProblem Two: The task list tool is unusable.\nI once suspected it was a cline issue. But then I thought, deepseek, gpt-5, and claude-4-opus all work fine.\nThe prompt doesn\u0026rsquo;t change because of these. It\u0026rsquo;s most likely an issue with Zhipu glm-4.5.\nClaude Code Misunderstanding problems (unable to understand some simple natural language) Incoherent responses, not listening to the user, failing to identify the target. Later, if I find similar situations, I will add screenshots to this blog post. I don\u0026rsquo;t want to waste more time on this.\nThere\u0026rsquo;s also a common issue: obsequiousness.\nStopped Responding A new problem encountered on 2025-10-03: it stopped providing feedback while answering a question and terminated the process.\nThe most likely cause of this problem is a lack of adaptation to the thinking interface, resulting in it thinking but not displaying the content.\nSummary Based on my current experience, among domestic AIs, apart from DeepSeek, the other major players tend to have unstable model outputs.\nWithout a doubt, Anthropic is the leader in this field.\nI genuinely doubt the friends who told me Zhipu is good‚Äîhave you actually used AI for programming? If so, how do you tolerate these issues? How has your efficiency improved?\nIf you think these problems are inevitable, then I sincerely suggest you use Anthropic\u0026rsquo;s products and models.\nAside I really don\u0026rsquo;t want to use glm anymore, but there\u0026rsquo;s no choice‚ÄîI\u0026rsquo;ve already paid for the annual subscription, and it\u0026rsquo;s non-refundable.\nTherefore, as a user, you can only hope that glm will update its model.\nAs a consumer or customer, this feels very uncomfortable. It\u0026rsquo;s okay if the product isn\u0026rsquo;t fully developed yet; just don\u0026rsquo;t release it, or don\u0026rsquo;t charge for it like this. At 200 yuan a month, I might as well put all that money into deepseek. That\u0026rsquo;s a model that truly stands up to scrutiny.\nGetting a refund is troublesome. I think reporting it to the consumer association could solve the problem to some extent. But it\u0026rsquo;s a waste of time. Furthermore, continuing to use it is a sunk cost. Therefore, I can only do this: I will not spend another cent on Zhipu in the future.\nUpdate Very strange!\nNot long after I published this article, I found that the usability of glm-4.5 has become significantly better.\nRelated Coding Plan Articles If you\u0026rsquo;re interested in experiences with other AI coding plans, you can read my other articles:\nI Bought the Kimi Coding Plan Again - Experience and configuration methods for the Kimi monthly plan. Doubao doubao-seed-code Test - In-depth testing of ByteDance\u0026rsquo;s Doubao coding plan. ","date":"2025-09-23T10:24:43+08:00","image":"https://svtter.cn/p/%E6%99%BA%E8%B0%B1-glm-4.5-%E5%9C%A8%E7%BC%96%E7%A8%8B%E6%96%B9%E9%9D%A2%E7%9A%84%E8%8B%A5%E5%B9%B2%E9%97%AE%E9%A2%98/pics/featured_20250924_145011_hu_b53f740a78dcafa2.png","permalink":"https://svtter.cn/en/p/several-issues-with-zhipu-glm-4.5-in-programming/","title":"Several Issues with Zhipu GLM-4.5 in Programming"},{"content":"Sometimes we cannot directly use the Anthropic API. However, the Claude Code (CC) experience is excellent, and we still want to use CC.\nIn such cases, you can try using the API provided by DeepSeek to access CC.\nDeepSeek has already provided the corresponding interface: How to Use the Claude Code + DeepSeek Combination?\nCurrently, there are two mainstream LLM APIs: one is OpenAI, and the other is Anthropic. Anthropic has gained a certain level of influence through CC.\nIf you want to learn more about the use cases for CC, I recommend reading Anthropic\u0026rsquo;s Official Blog.\nAdditionally, here are some supplementary resources:\nhttps://mp.weixin.qq.com/s/gk0tzMxWZ-NgsUWg5iLoSg ","date":"2025-08-26T14:42:54+08:00","image":"https://svtter.cn/p/how-to-use-claude-code-with-deepseek/pics/bg_hu_b6d17d91bd482486.png","permalink":"https://svtter.cn/en/p/how-to-use-claude-code-with-deepseek/","title":"How to Use Claude Code With Deepseek"},{"content":"Vision large models perform poorly on some specific tasks but perform better with formatted text. Here, I use the localization of meter reading areas as an example to demonstrate the performance of large models.\nSource Code https://github.com/Svtter/vl-model/pull/4\nTest Tasks Extract text boxes from the image. Extract the meter reading area from the image. Test File We can observe the performance differences among various models from these test results:\nTest Results Comparison Results Using Bounding Boxes as Prompts Detailed Performance of Each Model Anthropic Claude 3.5 Sonnet Google Gemini 2.5 Pro OpenAI GPT-4o Analysis Summary From these test results, we can observe:\nDifferences in Visual Recognition Capabilities: Different models exhibit significant performance variations when handling the same visual task. Formatted Text Processing: Compared to visual tasks, models perform more stably when processing structured text. Model Characteristics: Each model has its unique strengths and limitations. These results remind us to evaluate the suitability of AI models based on specific task types when making selections.\n","date":"2025-06-19T16:34:32+08:00","image":"https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/bg_hu_cfeb49458fb6e0b5.png","permalink":"https://svtter.cn/en/p/poor-performance-of-large-models-on-specific-tasks/","title":"Poor Performance of Large Models on Specific Tasks"},{"content":" 1 2 3 [build-system] requires = [\u0026#34;setuptools\u0026gt;=42\u0026#34;, \u0026#34;wheel\u0026#34;, \u0026#34;uv\u0026gt;=0.6.0\u0026#34;] build-backend = \u0026#34;setuptools.build_meta\u0026#34; 1 uv build 1 python -m twine upload 1 2 3 [build-system] requires = [\u0026#34;pdm-backend\u0026#34;] build-backend = \u0026#34;pdm.backend\u0026#34; 1 2 3 4 5 6 [tool.pdm] distribution = true [tool.pdm.version] source = \u0026#34;file\u0026#34; path = \u0026#34;src/spback/__init__.py\u0026#34; Since migrating from pdm to uv, besides dependency management, I also wanted to use uv for publishing packages.\nMethod 1 LLMs provided a solution, suggesting to add the following content in pyproject.toml:\nAfter adding this content, we run:\nThen run:\nThe package can then be published.\nMethod 2 Since there are many projects using pdm, directly modifying pdm can also cause significant inconvenience.\nYou can still use pdm as the build-system but use uv as the package management tool.\nIn other words:\neven\nSome Thoughts LLMs are already quite powerful. However, LLMs cannot guarantee the accuracy of generated content, requiring human verification. Therefore, the human who verifies the output is essential.\nThis code must be verified by a human to work. Of course, if it\u0026rsquo;s merely about modifying content, LLMs can collaborate with us, in the form of a cursor.\n","date":"2025-06-03T15:54:28+08:00","permalink":"https://svtter.cn/en/p/using-uv-to-publish-python-packages/","title":"Using uv to publish Python packages"},{"content":"New Insights:\nConnect knowledge into a network rather than isolated nodes; forming a graph structure allows you to gain new insights from each of your own insights. Use tools appropriately, don\u0026rsquo;t blindly pursue programmability and reusability. This is a habit developed as a software engineer, but when solving problems, tailor the approach to the specific issue. Previous Phase Below is a summary of Q1 activities from the perspectives of work, learning, and research.\nResearch I originally planned to publish a conference paper in Q1 and follow up on some of the latest research progress, but I found that this task was not actually completed. I believe this was due to preparing the SWR paper. I supplemented a large number of experiments on SWR and rewrote much of the code that I previously considered unreliable. Submitted the SWR paper. I refactored part of the meter-viewer content; preparing to reintroduce new methods for building more suitable datasets. I reimplemented SWR based on Lightning. Considered domain adaptation models for meter recognition problems. Explored some active learning algorithms. Work Further design and foundational thinking on meterhub. Spent most of January improving meterhub. Feedback features. Login with email. Released django-login-mail v0.6.1. Started a new project: githubManager. Upgraded the Hugo theme and update process for HIGH, using the stack theme and corresponding GitHub CI. Learn Experimented with OpenAI\u0026rsquo;s SDK and simple calls to LlamaIndex. Revisited MAE. Read CLIP code. OpenManus and Docker. ComfyUI testing. MCP server. CUDA-related problems\u0026hellip; Using OpenRouter. Sharpened basic programming skills. Rethinking about functional programming. Where to Put Your Data Folder.md Using PDM: https://svtter.cn/p/dynamic-version-in-pdm.md/ Operations and maintenance. SSL certificates: https://svtter.cn/p/certbot-self-signed.md/ Plans for the Next Phase Focus on writing the overall framework for the paper. Since I\u0026rsquo;m not someone who can sit still for long periods, I need to pay extra attention to staying focused. Attempt to publish a conference paper; the level is not important. Refine the outline of the graduation thesis and write the details. Write a new paper. ","date":"2025-05-03T10:56:16+08:00","image":"https://svtter.cn/p/2025-q1-%E6%80%BB%E7%BB%93/bg_hu_3a696997fb97c2c5.png","permalink":"https://svtter.cn/en/p/2025-q1-summary/","title":"2025 Q1 Summary"},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 git clone https://github.com/langenius/dify cd dify/docker cp .env.example .env docker compose up -d ```I believe hackers should abandon the idea of building agents from code and fully embrace workflow platforms like Dify. This approach is many times more efficient than writing code. If you must write code, you can develop plugins to embed into Dify. What is Dify? A workflow platform designed for LLMs. \u0026lt;script src=\u0026#34;https://tarptaeya.github.io/repo-card/repo-card.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- inside body, where you want to create the card --\u0026gt; \u0026lt;div class=\u0026#34;repo-card\u0026#34; data-repo=\u0026#34;langgenius/dify\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; ## Deployment Method Simply execute the following code on your server.## Deployment Issues Although Dify is an open-source project, being relatively new, it often encounters various unusual problems. ### Plugin Restart Problem When using Dify 1.2.0, the Dify plugin daemon would continuously restart. Refer to this [issue](https://github.com/langgenius/dify/issues/17788) for details. \u0026gt; Interestingly, in this issue, the problem was solved by AI. ### Protocols Problem `http ... https` Adjust the `FILE_URLS` variable. ## Plugins To utilize certain features, I developed a Dify plugin for file compression. \u0026lt;script src=\u0026#34;https://tarptaeya.github.io/repo-card/repo-card.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- inside body, where you want to create the card --\u0026gt; \u0026lt;div class=\u0026#34;repo-card\u0026#34; data-repo=\u0026#34;svtter/filecompress\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; ## Resource Attribution - Images sourced from [chatgpt-lab](https://chatgpt-lab.com/n/n12d18abb26c8?gs=a6ed475ccea2) ","date":"2025-04-22T11:20:02+08:00","image":"https://svtter.cn/p/deployment-of-dify-1.2.0.md/image_hu_44ea294e192dd580.png","permalink":"https://svtter.cn/en/p/deployment-of-dify-1.2.0/","title":"Deployment of Dify 1.2.0"},{"content":"When developing LLM applications, we consider performance issues during LLM calls and monitor outputs during the process.\nAt this point, tools like LangSmith and Langfuse become very useful.\nHowever, sometimes we have local computing resources and prefer not to use cloud-based resources for LLM call monitoring, so we might not consider LangSmith.\nIn such cases, we can use Langfuse for this purpose.\nDeployment Deploying Langfuse is very simple; all you need to do is:\n1 2 3 git clone https://github.com/langfuse/langfuse.git cd langfuse docker compose up -d This way, the deployment is successful.\nReplacement If you previously used OpenAI\u0026rsquo;s SDK, you can continue using it as follows.\nInstall langfuse in the project:\n1 pip install langfuse To configure the API key, you need to use it in the deployed langfuse:\n1 2 3 LANGFUSE_SECRET_KEY=\u0026lt;secret key\u0026gt; LANGFUSE_PUBLIC_KEY=\u0026lt;public key\u0026gt; LANGFUSE_HOST=\u0026#34;http://localhost:3001\u0026#34; Here I have set the Langfuse port to 3001; you should adjust according to your own configuration.\nSimply replace the original OpenAI configuration:\n1 2 3 # remove: import openai from langfuse.openai import openai In addition, langfuse also supports langchain and llamaindex, which will not be elaborated on further here.\nThoughts Coze is also developing a large model agent framework, but the approach is quite different. Coze is building everything, including workflows and LLMs, making it relatively closed.\nHowever, langfuse is more open, allowing the use of langchain and other models.\nAs a developer from a small company, I prefer the langfuse model because it offers more choices. However, if the project timeline is tight and Coze is barely usable, I would choose Coze.\nIssues An exception occurred when I replaced the OpenAI SDK: 1 Unexpected error occurred. Please check your request and contact support: https://langfuse.com/support. I still encountered issues when testing test_langfuse.py: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import os from langfuse.decorators import observe from langfuse.openai import openai @observe() def story(): return ( openai.chat.completions.create( model=\u0026#34;moonshot-v1-auto\u0026#34;, max_tokens=100, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a great storyteller.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Once upon a time in a galaxy far, far away...\u0026#34;}, ], ) .choices[0] .message.content ) @observe() def main(): return story() def test_langfuse(): assert os.getenv(\u0026#34;OPENAI_BASE_URL\u0026#34;) is not None assert os.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;) is not None main() Regarding this issue, I have opened a discussion.\nAdditionally, if you wish to view the original code, you can obtain it from https://github.com/svtter/pdf-reader.\n","date":"2025-04-21T14:51:38+08:00","image":"https://svtter.cn/p/work-with-langfuse.md/image_hu_6bc6e05e32748566.png","permalink":"https://svtter.cn/en/p/work-with-langfuse/","title":"Work With Langfuse"},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 import matplotlib.pyplot as plt plt.subplot(nrows, ncols, index) ```python # ÂàõÂª∫Â≠êÂõæ plt.figure(figsize=(10, 6)) ```python import matplotlib.pyplot as plt import numpy as np # ÂàõÂª∫Êï∞ÊçÆ x = np.linspace(0, 10, 100) y1 = np.sin(x) y2 = np.cos(x) y3 = np.tan(x) # ÂàõÂª∫Â≠êÂõæ plt.figure(figsize=(10, 6)) # Á¨¨‰∏Ä‰∏™Â≠êÂõæ plt.subplot(2, 2, 1) # 2Ë°å2ÂàóÁöÑÁ¨¨1‰∏™ plt.plot(x, y1, label=\u0026#34;sin(x)\u0026#34;) plt.title(\u0026#34;Sine Wave\u0026#34;) plt.legend() # Á¨¨‰∫å‰∏™Â≠êÂõæ plt.subplot(2, 2, 2) # 2Ë°å2ÂàóÁöÑÁ¨¨2‰∏™ plt.plot(x, y2, label=\u0026#34;cos(x)\u0026#34;) plt.title(\u0026#34;Cosine Wave\u0026#34;) plt.legend() # Á¨¨‰∏â‰∏™Â≠êÂõæ plt.subplot(2, 2, 3) # 2Ë°å2ÂàóÁöÑÁ¨¨3‰∏™ plt.plot(x, y3, label=\u0026#34;tan(x)\u0026#34;) plt.title(\u0026#34;Tangent Wave\u0026#34;) plt.legend() # Á¨¨Âõõ‰∏™Â≠êÂõæ plt.subplot(2, 2, 4) # 2Ë°å2ÂàóÁöÑÁ¨¨4‰∏™ plt.plot(x, y1 + y2, label=\u0026#34;sin(x) + cos(x)\u0026#34;) plt.title(\u0026#34;Sum of Sine and Cosine\u0026#34;) plt.legend() # ÊòæÁ§∫ÂõæÂΩ¢ plt.tight_layout() # Ëá™Âä®Ë∞ÉÊï¥Â≠êÂõæÈó¥Ë∑ù plt.show() ```python import torchvision.utils as vutils # Â∞ÜÂõæÁâáÂà∂‰ΩúÊàêÁΩëÊ†º grid_img = vutils.make_grid(x, nrow=4, padding=2) # ÂèØËßÜÂåñÁΩëÊ†ºÂõæÁâá plt.figure(figsize=(10, 10)) plt.imshow(grid_img.permute(1, 2, 0)) # Ë∞ÉÊï¥ÈÄöÈÅìÈ°∫Â∫è‰ª•ÈÄÇÂ∫î matplotlib ÁöÑË¶ÅÊ±Ç plt.axis(\u0026#39;off\u0026#39;) plt.show()j ```Creating subplots for image browsing offers great flexibility, but I often forget how to use them. I‚Äôm writing this blog specifically to reinforce my memory. ## Drawing Subplots First, to draw a subplot at a certain position, you need to call the `plt.subplot` method. They represent the row, column, and subplot index, respectively. I often used to forget that the index refers to the subplot number. Before drawing subplots, you must first create them.## Example Next, here is a complete example.### Preview ![](Figure_1.png) If you want to view images with a shape of (32, 1, 192, 32), you can also use the utility functions provided by torchvision. ","date":"2025-04-08T10:17:09+08:00","image":"https://svtter.cn/p/draw-subfig-and-making-subplot.md/Figure_1_hu_29feedccd7b5b86a.png","permalink":"https://svtter.cn/en/p/draw-subfig-and-making-subplot/","title":"Draw Subfig and Making Subplot"},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 noise image -\u0026gt; ------------- | |----\u0026gt; cleared image step (int) -\u0026gt; | denoiser | | | --------------| ```The charm of deep learning lies in the fact that once a new task achieves improved performance with a certain architecture, many other tasks can refer to this architecture and benefit from it. I believe the diffusion model is a typical example. Although I do not conduct research on diffusion models and currently have no related projects, there is no harm in understanding this network architecture. The diffusion model is one that benefits from the image processing process. By learning the reverse process of adding noise to images, the diffusion model acquires the ability to generate images from noise. ![noise-dog](noise-dog.png) To enable the model to achieve better performance, the denoising step of the model is included as one of the inputs. ","date":"2025-04-05T21:51:38+08:00","image":"https://svtter.cn/p/diffusion-model.md/noise-dog_hu_dcab74b6d0eb6e7d.png","permalink":"https://svtter.cn/en/p/diffusion-model/","title":"Diffusion Model"},{"content":"Recently, I\u0026rsquo;ve started using uv extensively instead of pdm.\nknowledge piece uvx could replace pipx.\nThe uvx command invokes a tool without installing it.\nFor example, to run ruff\n1 uvx ruff ","date":"2025-03-30T14:33:34+08:00","image":"https://svtter.cn/p/using-uv.md/image_hu_cbfcaa67e1d48af4.png","permalink":"https://svtter.cn/en/p/using-uv/","title":"Using uv"},{"content":"When debugging deep learning code, we often face headaches due to environment issues.\nTo facilitate debugging, packaging environments like PyTorch and CUDA into a Docker image is an excellent choice.\nWhy? Time-saving: Repeatedly configuring and adjusting versions wastes time, leading to spending a lot of effort on ops tasks. Environment stability: Once a Docker image is built, it is static and can be pulled directly. Easy migration: Pre-configured environments can be migrated across different machines. How to Build Here is an example Docker image for packaging a deep learning environment:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Change to your desired pytorch version FROM pytorch/pytorch:2.4.1-cuda11.8-cudnn9-devel # These are commonly used packages RUN apt-get update \u0026amp;\u0026amp; apt-get install git zsh ffmpeg libsm6 libxext6 -y \u0026amp;\u0026amp; apt-get clean \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* WORKDIR /app # Place at the root of the codebase to install requirements.txt COPY requirements.txt . RUN pip install -r requirements.txt # install jupyterlab RUN pip install jupyterlab # COPY . . # Use jupyterlab to host, can start quickly, token is `yourtoken`. If you use it on the public network, consider using a more complex token. CMD [\u0026#34;jupyter\u0026#34;, \u0026#34;lab\u0026#34;, \u0026#34;--ip=0.0.0.0\u0026#34;, \u0026#34;--port=8888\u0026#34;, \u0026#34;--no-browser\u0026#34;, \u0026#34;--allow-root\u0026#34;, \u0026#34;--NotebookApp.token=yourtoken\u0026#34;] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 services: notebook: build: context: . dockerfile: Dockerfile volumes: # You can also mount the dataset you need - .:/app - ~/.ssh:/root/.ssh # Support ssh ports: - 8888:8888 shm_size: \u0026#39;32gb\u0026#39; deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] This example installs some basic libraries, and opencv-python can be installed via pip.\nPlace the Dockerfile in the directory, then you can start it using docker compose.\nThe startup command is: docker compose up -d.\nDownload from Dockerhub To make it convenient for everyone to use directly, I have packaged this image and uploaded it to Dockerhub. The download command is:\n1 docker pull svtter/debian-pytorch The source code can be obtained from here:\nUsing on Runpod For everyone\u0026rsquo;s convenience, I have created a template on Runpod.\nhttps://console.runpod.io/deploy?template=m0shpm3vgg\u0026ref=g5qp1x9x\nYou can directly use this image by using this template.\n","date":"2025-03-26T19:57:22+08:00","image":"https://svtter.cn/p/a-docker-image-for-computer-vision/image_hu_83b7ddcce90c9c97.png","permalink":"https://svtter.cn/en/p/a-docker-image-for-computer-vision/","title":"A Docker Image for Computer Vision"},{"content":"Contrastive Language-Image Pre-Training (CLIP) is one of the classic works from OpenAI, originating from the paper .\nTo implement my new idea based on CLIP, I attempted to read openai/clip to understand the fundamental working principles of CLIP in classification.\nHere is the Python sample code provided by openai/clip:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import torch import clip from PIL import Image device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model, preprocess = clip.load(\u0026#34;ViT-B/32\u0026#34;, device=device) image = preprocess(Image.open(\u0026#34;CLIP.png\u0026#34;)).unsqueeze(0).to(device) text = clip.tokenize([\u0026#34;a diagram\u0026#34;, \u0026#34;a dog\u0026#34;, \u0026#34;a cat\u0026#34;]).to(device) with torch.no_grad(): image_features = model.encode_image(image) text_features = model.encode_text(text) logits_per_image, logits_per_text = model(image, text) probs = logits_per_image.softmax(dim=-1).cpu().numpy() print(\u0026#34;Label probs:\u0026#34;, probs) # prints: [[0.9927937 0.00421068 0.00299572]] The load function is used to load a specific OpenAI model. This is based on ViT-B/32, a Vision Transformer 32B.\nIt can be seen that the vision encoders supported by OpenAI roughly include the following types:\n1 2 3 4 5 6 7 8 9 10 11 _MODELS = { \u0026#34;RN50\u0026#34;: \u0026#34;https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\u0026#34;, \u0026#34;RN101\u0026#34;: \u0026#34;https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\u0026#34;, \u0026#34;RN50x4\u0026#34;: \u0026#34;https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\u0026#34;, \u0026#34;RN50x16\u0026#34;: \u0026#34;https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\u0026#34;, \u0026#34;RN50x64\u0026#34;: \u0026#34;https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\u0026#34;, \u0026#34;ViT-B/32\u0026#34;: \u0026#34;https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\u0026#34;, \u0026#34;ViT-B/16\u0026#34;: \u0026#34;https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\u0026#34;, \u0026#34;ViT-L/14\u0026#34;: \u0026#34;https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\u0026#34;, \u0026#34;ViT-L/14@336px\u0026#34;: \u0026#34;https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\u0026#34;, } Assuming the model has already been downloaded, let\u0026rsquo;s examine how the _transform preprocessing works:\n1 2 3 4 5 6 7 8 def _transform(n_px): return Compose([ Resize(n_px, interpolation=BICUBIC), CenterCrop(n_px), _convert_image_to_rgb, ToTensor(), Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)), ]) It\u0026rsquo;s not overly complex, though the preprocessing Normalize parameters are not entirely clear. It seems to use the same preprocessing parameters as ViT.\nThen, moving into the model loading phase, we can see that if it\u0026rsquo;s not jit loading, the model will opt for the state_dict mode.\nThrough the process of loading the state_dict, we can observe that the build_model function is used to load the weights and assign them to the existing model structure.\nThe file for this model structure is model.py.\nTherefore, the main code for CLIP is located at model.py#L243.\nThe outputs of the image_encoder and text_encoder are two distinct feature tensors.\nPerforming a matrix multiplication on these two tensors yields a similarity matrix. The size of this similarity matrix is (batch_size, batch_size).\nTIPS: If the batch size is too small, such as 1, the performance of contrastive learning may be significantly diminished.\nThese two tensors are computed using symmetric cross-entropy loss to update the network weights.\nSpecifically focused on improving intelligence metrics, without much concern for computational cost. Not pursuing the latest or highest intelligence metrics, but more focused on the computational efficiency of the model.\nTrick: Adding a log to the parameters to make weight updates less drastic and reduce computational intensity.\nThe CLIP code does not provide directly trainable code. In the next article, we\u0026rsquo;ll attempt to read openclip.\n","date":"2025-03-19T13:23:50+08:00","image":"https://svtter.cn/p/read-code-of-clip.md/image_hu_f2865947cde956ce.png","permalink":"https://svtter.cn/en/p/read-code-of-clip/","title":"Read Code of CLIP"},{"content":"Sometimes we need to start a container that does not stop, for debugging our application or using devcontainer.\nIf we want to accomplish this in the Dockerfile, we can add the following:\n1 2 3 4 5 ... # ÂÖ∂‰ªñÂÜÖÂÆπ ENTRYPOINT [\u0026#34;tail\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;/dev/null\u0026#34;] If it\u0026rsquo;s docker-compose.yml, we can do it like this\n1 2 3 services: your-app: entrypoint: [\u0026#34;tail\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;/dev/null\u0026#34;] This way, the container will not stop.\n","date":"2025-03-14T16:45:58+08:00","image":"https://svtter.cn/p/create-a-never-stop-container.md/background_hu_d172c53e65a8a302.png","permalink":"https://svtter.cn/en/p/create-a-never-stop-container/","title":"Create a Never Stop Container"},{"content":"If you want to build a RAG system locally, we can use ollama as the base model and llamaindex to construct the agent.\nSince llamaindex defaults to using OpenAI, we first need to adjust the default embedding model and LLM model.\n1 2 Settings.embed_model = OllamaEmbedding(model_name=model_name, base_url=sdmicl[1]) Settings.llm = Ollama(model=sdmicl[0], base_url=sdmicl[1], request_timeout=360.0) The base_url needs to be replaced with your own ollama instance, such as http://localhost:11434.\nIf the files in the directory are all txt or md data, you can directly use SimpleDirectoryReader to read the basic data.\n1 2 # Create a RAG tool using LlamaIndex documents = SimpleDirectoryReader(\u0026#34;data\u0026#34;).load_data() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings from llama_index.embeddings.ollama import OllamaEmbedding def get_agent(model_name: str): Settings.embed_model = OllamaEmbedding(model_name=model_name, base_url=sdmicl[1]) Settings.llm = Ollama(model=sdmicl[0], base_url=sdmicl[1], request_timeout=360.0) # Create a RAG tool using LlamaIndex documents = SimpleDirectoryReader(\u0026#34;data\u0026#34;).load_data() index = VectorStoreIndex.from_documents(documents) query_engine = index.as_query_engine() async def search_documents(query: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Useful for answering natural language questions about an personal essay written by Paul Graham.\u0026#34;\u0026#34;\u0026#34; response = await query_engine.query(query) return str(response) agent = FunctionAgent( name=\u0026#34;Agent\u0026#34;, description=\u0026#34;Useful for multiplying two numbers and searching documents\u0026#34;, tools=[multiply, search_documents], llm=ollama, system_prompt=\u0026#34;You are a helpful assistant that can multiply two numbers and search documents to answer questions\u0026#34;, ) return agent async def main(): models = (\u0026#39;bge-m3\u0026#39;, \u0026#39;nomic-embed-text\u0026#39;,) for model_name in models: print(f\u0026#39;model: {model_name}\u0026#39;) agent = get_agent(model_name=model_name) response = await agent.run(\u0026#34;What did the paul graham do in college? Also, what\u0026#39;s 7 * 8?\u0026#34;) print(str(response)) print(\u0026#34;Done.\u0026#34;) print(\u0026#39;-\u0026#39; * 100) await main() ","date":"2025-03-09T12:44:24+08:00","permalink":"https://svtter.cn/en/p/rag-with-llamaindex-and-ollama/","title":"RAG with LlamaIndex and Ollama"},{"content":"Since the previously used Hugo version was too low and updating it would require significant effort, I have now updated Hugo, allowing me to focus solely on writing articles.\nThe new theme I am currently using is hugo-theme-stack.\nBecause my Hugo source files and \u0026lt;username\u0026gt;.github.io are not in the same repository‚Äîmeaning I cannot directly configure gh-pages using a branch‚ÄîI have adjusted the workflow to suit my situation. Here is my workflow configuration file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 name: Deploy to Github Pages on: push: branches: [master] pull_request: branches: [master] jobs: build: runs-on: ubuntu-latest permissions: # Give the default GITHUB_TOKEN write permission to commit and push the # added or changed files to the repository. contents: write steps: - uses: actions/checkout@v4 with: fetch-depth: 0 - name: Cache Hugo resources uses: actions/cache@v4 env: cache-name: cache-hugo-resources with: path: resources key: ${{ env.cache-name }} - uses: actions/setup-go@v5 with: go-version: \u0026#34;^1.17.0\u0026#34; - run: go version - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#34;latest\u0026#34; extended: true - name: Build run: hugo --minify --gc - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: personal_token: ${{ secrets. ACCESS_TOKEN }} external_repository: svtter/svtter.github.io publish_branch: master publish_dir: ./public ","date":"2025-03-07T11:39:32+08:00","permalink":"https://svtter.cn/en/p/update-the-hugo/","title":"Update the Hugo"},{"content":"Zhou Tian developed an application based on a large model using OpenRouter and encountered some issues, documenting a few insights.\nNo Support for Embeddings The biggest issue is the lack of support for the embedding API. Although OpenRouter already supports API endpoints for various models like OpenAI, embeddings are crucial for developing RAG applications. The absence of embedding support renders OpenRouter ineffective in practical application development.\n","date":"2025-03-03T11:45:12+08:00","permalink":"https://svtter.cn/en/p/openrouter-usage/","title":"Openrouter Usage"},{"content":"Regardless of the current server settings, output the time in Asia/Shanghai.\n1 2 3 4 5 6 7 8 9 10 11 12 import datetime import pytz utc_now = datetime.datetime.utcnow() # Get current time in UTC utc_timezone = pytz.utc utc_now = utc_timezone.localize(utc_now) # Localize the time as UTC # Convert to another timezone, e.g., \u0026#39;America/New_York\u0026#39; new_timezone = pytz.timezone(\u0026#39;Asia/Shanghai\u0026#39;) new_timezone_time = utc_now.astimezone(new_timezone) print(new_timezone_time.strftime(\u0026#39;%Y-%m-%d %H:%M:%S %Z%z\u0026#39;)) # Display time in the new timezone ","date":"2025-02-28T17:46:29+08:00","permalink":"https://svtter.cn/en/p/python-timezone/","title":"Python Timezone"},{"content":"When training models, we should place data and code in the same location as much as possible.\nKeeping them in the same location helps avoid path-related issues, such as needing to specify absolute paths for the data.\nFor example, if I set the path to ./data/, I only need to place the data in the ./data directory.\nI can do this by:\n1 ln -s $(source-path-of-dataset) ./data To link data from other locations.\nIf on the same host, git can automatically synchronize these links.\nHowever, if on different hosts, you need to manage them yourself.\n","date":"2025-02-24T14:34:56+08:00","permalink":"https://svtter.cn/en/p/where-to-put-your-data-folder/","title":"Where to Put Your Data Folder"},{"content":"I used to frequently install oh-my-zsh on servers, but sometimes the network connection was poor, making the installation quite troublesome. In such cases, what I needed was a simple zsh configuration that just worked.\nIn addition to highlighting ls, docker compose has been configured.\nIf fzf is installed, you can also configure zshenv to enable fzf.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # ËÆæÁΩÆÁºñËæëÂô® export EDITOR=\u0026#34;vim\u0026#34; # ËÆæÁΩÆÊèêÁ§∫Á¨¶ PROMPT=\u0026#39;%F{blue}%n%f@%F{green}%m%f %F{cyan}%~%f %# \u0026#39; # Âà´ÂêçÂÆö‰πâ alias ls=\u0026#39;ls --color=auto\u0026#39; alias ll=\u0026#39;ls -la -G\u0026#39; alias c=\u0026#39;clear\u0026#39; alias dc=\u0026#39;docker compose\u0026#39; HISTFILE=\u0026#34;$HOME/.zsh_history\u0026#34; HISTSIZE=10000000 SAVEHIST=10000000 setopt BANG_HIST # Treat the \u0026#39;!\u0026#39; character specially during expansion. setopt EXTENDED_HISTORY # Write the history file in the \u0026#34;:start:elapsed;command\u0026#34; format. setopt INC_APPEND_HISTORY # Write to the history file immediately, not when the shell exits. setopt SHARE_HISTORY # Share history between all sessions. setopt HIST_EXPIRE_DUPS_FIRST # Expire duplicate entries first when trimming history. setopt HIST_IGNORE_DUPS # Don\u0026#39;t record an entry that was just recorded again. setopt HIST_IGNORE_ALL_DUPS # Delete old recorded entry if new entry is a duplicate. setopt HIST_FIND_NO_DUPS # Do not display a line previously found. setopt HIST_IGNORE_SPACE # Don\u0026#39;t record an entry starting with a space. setopt HIST_SAVE_NO_DUPS # Don\u0026#39;t write duplicate entries in the history file. setopt HIST_REDUCE_BLANKS # Remove superfluous blanks before recording entry. setopt HIST_VERIFY # Don\u0026#39;t execute immediately upon history expansion. setopt HIST_BEEP # Beep when accessing nonexistent history. # ÂêØÁî® fzf Áõ∏ÂÖ≥ÂäüËÉΩ [ -f /usr/share/doc/fzf/examples/key-bindings.zsh ] \u0026amp;\u0026amp; source /usr/share/doc/fzf/examples/key-bindings.zsh [ -f /usr/share/doc/fzf/examples/completion.zsh ] \u0026amp;\u0026amp; source /usr/share/doc/fzf/examples/completion.zsh setopt no_nomatch 1 2 3 4 # ÂêØÁî® fzf Áõ∏ÂÖ≥ÂäüËÉΩ [ -f /usr/share/doc/fzf/examples/key-bindings.zsh ] \u0026amp;\u0026amp; source /usr/share/doc/fzf/examples/key-bindings.zsh [ -f /usr/share/doc/fzf/examples/completion.zsh ] \u0026amp;\u0026amp; source /usr/share/doc/fzf/examples/completion.zsh ","date":"2025-02-15T21:11:14+08:00","permalink":"https://svtter.cn/en/p/easy-zshrc-configuration/","title":"Easy Zshrc Configuration"},{"content":"Configuring a deep learning environment is a hurdle many struggle to overcome. However, with large models, troubleshooting and pinpointing issues can be significantly faster.\nI spent some time adapting an older version of PaddlePaddle and finally got it working. Here, I\u0026rsquo;ll share an article documenting the process.\nIn Docker images, many CUDA 11-based images fail to run in a CUDA 12 environment. The exact reason isn\u0026rsquo;t entirely clear to me. In such cases, you can opt for a CUDA version that matches the major release.\nTo avoid affecting the environments of others on the server, do not update the NVIDIA driver. Instead, install your own CUDA version and modify the environment variables to change the system\u0026rsquo;s CUDA.\n1 2 3 4 # CUDA_VERSION=11.7 export CUDA_HOME=\u0026#34;/usr/local/cuda-$CUDA_VERSION\u0026#34; export LD_LIBRARY_PATH=\u0026#34;$CUDA_HOME/lib64:$LD_LIBRARY_PATH\u0026#34; export PATH=$CUDA_HOME/bin:$PATH Apply this environment variable, then check nvidia-smi to see the version change.\n","date":"2025-02-11T15:41:18+08:00","permalink":"https://svtter.cn/en/p/cuda-and-paddle/","title":"Cuda and Paddle"},{"content":"Sometimes we want to modify the default IP address and DNS server to achieve better network performance.\nFor Debian, modify two files: /etc/network/interfaces and /etc/resolv.conf.\nRegarding interfaces:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback # The primary network interface allow-hotplug ens18 iface ens18 inet static address 192.168.2.35 netmask 255.255.255.0 gateway 192.168.2.60 When the gateway is 192.168.2.60;\nIf DNS configuration is also needed, modify /etc/resolv.conf\n1 nameserver 192.168.2.60 Disable IPv6\n/etc/sysctl.conf\n1 net.ipv6.conf.all.disable_ipv6 = 1 Don\u0026rsquo;t forget to restart the network: systemctl restart networking.\n","date":"2025-02-10T20:49:27+08:00","permalink":"https://svtter.cn/en/p/change-network-of-debian/","title":"Change Network of Debian"},{"content":"Since I frequently use multiple devices simultaneously, I often encounter the issue of writing articles on one device and then continuing to use Logseq on another. Simply copying and pasting feels cumbersome.\nI have roughly divided the solution to this problem into two stages. The first stage involves using a portable hard drive to directly copy data between different systems. During this stage, I utilized a Git bare repository.\nStage 1 git init --bare logseq-database.git\nThen, in other working Git repositories, I added a remote. For example, if my disk path is E:\\:\ngit remote add origin E:\\logseq-database.git\nThis way, I could synchronize Logseq data directly between different devices.\nStage 2 I found using a portable hard drive still inconvenient. I repurposed a 10-year-old ThinkPad, installed Gitea on it. Initially, I used Gogs, but Gogs had unfriendly handling of submodules and inexplicable bugs. Therefore, I ultimately switched to Gitea. After setting up Gitea, I migrated the original Git repository to my local machine. For example: http://gitea.local/svtter/logseq-database.git.\nStage 3 I discovered that storing and sharing large files often caused issues. I added support for git-lfs by running:\ngit lfs install\nand\ngit lfs track *.pdf\nto prevent large files from leaving too much data in the .git directory.\nStage 4 Periodically back up part of the data to GitHub. However, I generally no longer do this.\nWhen using Git for merging, one must be cautious about an issue: if a file name changes, automated merging will simply ignore it. Here‚Äôs how it typically happens: I modify a file on two devices simultaneously, and on one device, not only is the content changed, but the file name is also altered. Then, both devices perform a Git commit separately. As a result, when Git performs the merge, it won‚Äôt prompt a conflict. After Git‚Äôs automated merge, the modifications made on one of the devices will disappear.\nTo address this issue, the approach is to use the rebase method as much as possible during merging. However, rebasing is slow when merging files and requires a lot of manual handling.\nTherefore, when modifying the same file on two devices simultaneously, first pull the remote changes. Second, avoid changing file names whenever possible. Otherwise, changes may be lost.\nFortunately, Git commit history is always preserved. If all else fails, retrieve the lost parts from the commit records and add a new commit.\n","date":"2025-02-08T18:18:29+08:00","permalink":"https://svtter.cn/en/p/using-git-to-manage-logseq-files/","title":"Using git to manage logseq files"},{"content":"Browsing datasets can be quite troublesome, especially when the dataset is large.\nnpy (numpy array) and h5 files are two common data storage formats.\nThe drawback of h5 files is that they are prone to data corruption. I have encountered issues multiple times where h5 files could not be opened.\nnpy files have clear advantages in terms of read speed and file transfer. However, they are loaded entirely into memory at once, which can easily cause memory overflow if the server is not powerful enough.\nCommon image datasets typically separate labels and images, such as COCO. This allows you to use a file browser to view images and quickly observe their characteristics. However, in most cases, we don\u0026rsquo;t view images on a local computer but rather work with datasets on a server.\nIn 2024, when working with PyTorch, I find it more convenient to directly plot images using matplotlib. Matplotlib is generally used to display a single image, but using subplots allows you to display multiple images simultaneously. If OpenCV is used, you can overlay some label values onto the images. However, there is a drawback: if you are working on a remote server, transferring generated images can consume significant bandwidth.\nUltimately, the choice of method depends on your own judgment!\n","date":"2025-01-12T18:31:12+08:00","permalink":"https://svtter.cn/en/p/browsing-and-storing-image-datasets/","title":"Browsing and Storing Image Datasets"},{"content":"Previously, WordPress was running on CentOS 7; the performance of this machine was often underutilized, so some migration was needed to improve performance. To avoid losing relevant data, the WordPress migration work was carried out. This article documents the WordPress migration process.\nTo minimize the time spent on backups, I first used the WordPress plugin, All-in-one WP migration. This plugin can back up plugins, articles, themes, and other plugins.\nWhen backing up the old website, I downloaded the generated backup file.\nWhen creating the new website (via Coolify), the file upload kept failing. I wasn\u0026rsquo;t sure what was happening.\nSubsequently, I modified several upload restrictions.\nOne of them was .htaccess.\n1 2 3 4 5 php_value upload_max_filesize 200M php_value post_max_size 200M php_value memory_limit 256M php_value max_execution_time 300 php_value max_input_time 300 Another one is wp-config.php\n1 2 3 4 5 @ini_set( \u0026#39;upload_max_filesize\u0026#39; , \u0026#39;200M\u0026#39; ); @ini_set( \u0026#39;post_max_size\u0026#39;, \u0026#39;200M\u0026#39;); @ini_set( \u0026#39;memory_limit\u0026#39;, \u0026#39;256M\u0026#39; ); @ini_set( \u0026#39;max_execution_time\u0026#39;, \u0026#39;300\u0026#39; ); @ini_set( \u0026#39;max_input_time\u0026#39;, \u0026#39;300\u0026#39; ); My backup file is 199MB in size. However, despite adjusting the two files mentioned above, I found that I still couldn\u0026rsquo;t restore the backup. This left me puzzled. Through console debugging, I discovered that after the upload was completed, the server would return an HTTP 413 error. Later, I found this article and identified the issue.\nAfter troubleshooting, I realized that the problem was actually caused by Cloudflare. The free Cloudflare plan does not support file uploads larger than 100MB, resulting in an HTTP 413 error.\nSubsequently, I configured my local hosts file to allow the domain to directly connect to the server\u0026rsquo;s real IP address.\nFinally, it succeeded.\n","date":"2024-11-15T17:16:45+08:00","permalink":"https://svtter.cn/en/p/documenting-a-wordpress-migration/","title":"Documenting a WordPress Migration"},{"content":"This is a summary for July 2024 to September 2024.\nThis quarter passed quite quickly; it doesn\u0026rsquo;t seem long since the last review. There truly is a gap in intelligence between people; I feel like I\u0026rsquo;m a not-very-hardworking fool.\nWhen conducting a review, try not to modify the content of the journal as much as possible. Otherwise, it will be troublesome to trace back later.\nLife #life Most of my energy during this period has been focused on taking care of my pregnant wife. The child was born safely, and the mother is doing well and happy üòÜ. For friends preparing for pregnancy and childbirth, I recommend this book: Pregnant with My Wife. Pregnancy is quite challenging for both men and women. Influenced by hormones, emotions can often fluctuate easily, and the man needs to balance work and family well.\nFinancially, prepare sufficient funds and budget for family expenses in advance. If your job is not stable enough, I don\u0026rsquo;t recommend rushing to have children. Although the old saying goes, \u0026ldquo;Start a family before establishing a career,\u0026rdquo; starting a family doesn\u0026rsquo;t mean having children immediately. During pregnancy, the emotional needs of the woman will be higher, so be mentally prepared. It\u0026rsquo;s best to seek advice from slightly older friends or relatives. Also, pay special attention to the location for childbirth. The ideal situation is to have your own independent family. Otherwise, you might have to deal with a series of complicated matters, which can be exhausting. Adding work pressure on top of that makes it even more difficult. In short, if conditions aren\u0026rsquo;t right, don\u0026rsquo;t have children; don\u0026rsquo;t make things hard for yourself. If you\u0026rsquo;re unhappy, your family will likely be unhappy too.\nRead Rich Dad Poor Dad 2 but couldn\u0026rsquo;t finish it. Made a small profit in stocks and cashed out; couldn\u0026rsquo;t keep up with the market trend anymore. In investment and financial management, too many people want to make quick money, so designing strategies directly aimed at quick profits can make money. Leverage is absolutely not advisable; it\u0026rsquo;s just used to deceive gamblers.\nI wonder how the friend who borrowed money to buy stocks a couple of days ago is doing.\nResearch #Research Although a new network was constructed, it couldn\u0026rsquo;t be transformed into a publishable paper.\nExperiments and New Research New experiments verified that CRNN is still the best method in certain scenarios; formed an article specifically discussing CTC. However, it\u0026rsquo;s not yet at the level for publication.\nTried many multimodal approaches, different encoding methods, word embedding, and one-hot. Spent a lot of time learning about transformers. Completed querynet, a new structure for solving multimodal problems. However, it still hasn\u0026rsquo;t solved the problem I proposed.\nIn terms of mid-term progress, the content is insufficient. Although I\u0026rsquo;ve done many experiments now, relatively few can be solidified into theories, and the overall coherence is lacking. This is not very consistent with the original plan.\nRegarding submissions, just received feedback on the new paper SWR; the new paper has been rejected and is still in rebuttal. But overall, it\u0026rsquo;s still okay.\nI feel that the current research progress is fine, but there haven\u0026rsquo;t been significant achievements.\nSome Thoughts If researching a field, spend time exploring, organizing materials, and increasing understanding of the industry. Organizing thoughts #thinking Tech-related #Tech Extensively adopted functional programming Deeply explored the React framework, understood useState, useEffect, and fixed many bugs in the framework. Deeply explored react-router. Project-related #project Developed a relatively universal front-end and back-end underlying framework. Front-end based on React, back-end based on Django Ninja. After so many years of development, I\u0026rsquo;ve finally solidified something. By using this framework, many problems brought by the native Django and React frameworks can be avoided. This is a kind of account for all these years. The framework still has many areas for improvement, with the biggest help being efficiency improvement. If there\u0026rsquo;s an opportunity, it will be open-sourced. Next Quarter Adjust SWR well, submit it, and strive to get one article accepted. Find a way forward for querynet and the new research problem proposed, respectively. Launch meterhub. On top of completing the above, finish the projects at hand and then organize the graduation thesis.\n","date":"2024-10-14T11:08:56+08:00","permalink":"https://svtter.cn/en/p/2024-q3-summary/","title":"2024 Q3 Summary"}]