<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Llm on Svtter's Blog</title><link>https://svtter.cn/en/tags/llm/</link><description>Recent content in Llm on Svtter's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 05 Jan 2026 16:00:00 +0800</lastBuildDate><atom:link href="https://svtter.cn/en/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Efficient and Cost-Effective: My AI Agent Workflow Choice</title><link>https://svtter.cn/en/p/efficient-and-cost-effective-my-ai-agent-workflow-choice/</link><pubDate>Mon, 05 Jan 2026 16:00:00 +0800</pubDate><guid>https://svtter.cn/en/p/efficient-and-cost-effective-my-ai-agent-workflow-choice/</guid><description>&lt;img src="https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/featured-image.jpg" alt="Featured image of post Efficient and Cost-Effective: My AI Agent Workflow Choice" /&gt;&lt;p&gt;Claude Code&amp;rsquo;s $100/month price tag is a bit steep for many. To address this, I&amp;rsquo;ve been experimenting with a more practical and affordable workflow.&lt;/p&gt;
&lt;p&gt;In terms of models, my recommendation is to use &lt;strong&gt;Gemini 3 Flash&lt;/strong&gt; on an as-needed (pay-as-you-go) basis as a replacement.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; Gemini 3 Flash offers incredible value. It&amp;rsquo;s fast, efficient, and costs a fraction of what you&amp;rsquo;d pay for Opus or Sonnet. For the vast majority of tasks, Flash is more than enough.&lt;/p&gt;
&lt;h2 id="the-cost-saving-workflow"&gt;The Cost-Saving Workflow
&lt;/h2&gt;&lt;p&gt;Here is my current &amp;ldquo;budget&amp;rdquo; workflow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Planning &amp;amp; Proposals&lt;/strong&gt;: Use Gemini 3 Flash.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Execution &amp;amp; Building&lt;/strong&gt;: Use the free &lt;strong&gt;GLM 4.7&lt;/strong&gt; (or MiniMax M2.1) via OpenCode. If you have a &lt;a class="link" href="https://svtter.cn/p/2025-10-09-%e6%88%91%e7%8e%b0%e5%9c%a8%e6%9b%b4%e5%a4%9a%e7%9a%84%e4%bd%bf%e7%94%a8-GLM-4.6-%e4%ba%86/" &gt;Zhipu Coding Plan&lt;/a&gt;, that works perfectly too.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speaking of Gemini 3, we have to talk about &lt;strong&gt;GPT-5.2&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Many engineers still rely on ChatGPT.com directly instead of using a proper coding agent. Regardless of the efficiency debate, the reliability is concerning. From my experience, GPT-5.2&amp;rsquo;s default tone has been tuned to be overly &amp;ldquo;people-pleasing,&amp;rdquo; which might not be ideal for professional developers seeking direct technical feedback.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/pics/image_1767597061665_0.png"
width="1023"
height="930"
srcset="https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/pics/image_1767597061665_0_hu_c015f16c1d892d52.png 480w, https://svtter.cn/p/%E9%AB%98%E6%95%88%E7%9C%81%E9%92%B1%E6%88%91%E7%9A%84-ai-agent-%E5%B7%A5%E4%BD%9C%E6%B5%81%E9%80%89%E6%8B%A9/pics/image_1767597061665_0_hu_474d285f9e3620e9.png 1024w"
loading="lazy"
alt="GPT-5.2 Response Tone"
class="gallery-image"
data-flex-grow="110"
data-flex-basis="264px"
&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, while GPT-5.2 scored impressively on &lt;strong&gt;SWE-bench Verified&lt;/strong&gt;, my real-world experience has been mixed. It&amp;rsquo;s worth looking at the history of SWE-bench:&lt;/p&gt;
&lt;p&gt;Originally proposed by a team from &lt;strong&gt;Princeton University&lt;/strong&gt; (ICLR 2024), it evaluates a model&amp;rsquo;s ability to solve real GitHub issues. However, in August 2024, OpenAI&amp;rsquo;s Preparedness team collaborated with the original authors to create &lt;strong&gt;SWE-bench Verified&lt;/strong&gt; (a subset of 500 manually verified issues). Since OpenAI was involved in the design of this benchmark, their models&amp;rsquo; performance on it should be taken with a grain of salt. While not necessarily a deliberate manipulation, the risk of inherent bias is significant.&lt;/p&gt;
&lt;p&gt;Ultimately, as I often say, &amp;ldquo;Codex&amp;rdquo; models don&amp;rsquo;t always deliver the most practical results in everyday coding.&lt;/p&gt;
&lt;h2 id="opencode-tips"&gt;OpenCode Tips
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Leveraging Agents&lt;/strong&gt;: OpenCode supports launching SubAgents. When debugging complex projects, you can have OpenCode launch agents in different directories to handle front-end and back-end tasks separately, which also helps avoid permission issues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OpenSpec: Cross-Agent Collaboration&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;span class="lnt"&gt;2
&lt;/span&gt;&lt;span class="lnt"&gt;3
&lt;/span&gt;&lt;span class="lnt"&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;1. OpenCode + Gemini 3 Flash → Generate proposal
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;2. Codex → Code Review
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;3. Claude Code → Secondary Review
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;4. OpenSpec Apply → Final Execution
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;OpenSpec generates reliable specs, but sometimes cheaper models produce lower-quality code. In such cases, you can generate multiple times using the spec and select the best result.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="final-thoughts"&gt;Final Thoughts
&lt;/h2&gt;&lt;p&gt;As AI Agent engineers, we need to adapt to these ongoing trends:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Models are becoming smarter.&lt;/li&gt;
&lt;li&gt;Execution is becoming faster.&lt;/li&gt;
&lt;li&gt;Prices are dropping.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While these trends are promising, we still need to balance speed, cost, and quality for every task. We might soon see agent systems that automate this balancing act, but for now, it&amp;rsquo;s a crucial part of the engineer&amp;rsquo;s role.&lt;/p&gt;</description></item><item><title>Third-party Client Performance</title><link>https://svtter.cn/en/p/third-party-client-performance/</link><pubDate>Wed, 19 Nov 2025 17:03:18 +0800</pubDate><guid>https://svtter.cn/en/p/third-party-client-performance/</guid><description>&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/bg.jpg" alt="Featured image of post Third-party Client Performance" /&gt;&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;span class="lnt"&gt;2
&lt;/span&gt;&lt;span class="lnt"&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Which is the most expensive model on Silicon Flow?
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;I mean siliconflow.cn
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Help me take a look
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Over the past year, I have attempted to use &lt;a class="link" href="https://github.com/ThinkInAIXYZ/deepchat" target="_blank" rel="noopener"
&gt;deepchat&lt;/a&gt; and large model APIs (such as k2 thinking turbo) to build a relatively private chat tool (or agent assistant) for handling some private data. However, the overall experience has not been great. The large models often provide incorrect answers.&lt;/p&gt;
&lt;p&gt;For search capabilities, I used the bocha API, resetting 10 credits to provide search functionality for the large model.&lt;/p&gt;
&lt;h2 id="test-questions"&gt;Test Questions
&lt;/h2&gt;&lt;p&gt;I feel there are still some issues with context handling (within a single chat window). I briefly tested this question: &lt;code&gt;Which is the most expensive model on Silicon Flow?&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The answer is:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/answ.png"
width="1200"
height="832"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/answ_hu_b24caa1450c981f6.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/answ_hu_c39dd9490283c1be.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
&gt;&lt;/p&gt;
&lt;h2 id="kimi-k2-thinking-turbo"&gt;Kimi k2 thinking turbo
&lt;/h2&gt;&lt;p&gt;First, deepchat:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171204_032.png"
width="2091"
height="1587"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171204_032_hu_7db667b7f9d9df7b.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171204_032_hu_88d3f5743cb65223.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="131"
data-flex-basis="316px"
&gt;&lt;/p&gt;
&lt;p&gt;Hmm, incorrect.&lt;/p&gt;
&lt;p&gt;Then, kimi official:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171256_940.png"
width="2163"
height="1911"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171256_940_hu_c84b877eeaffa92d.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ScreenShot_2025-11-19_171256_940_hu_434f4701e97c5241.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="271px"
&gt;&lt;/p&gt;
&lt;p&gt;Also incorrect.&lt;/p&gt;
&lt;h2 id="trying-deepseek"&gt;Trying deepseek
&lt;/h2&gt;&lt;p&gt;First, let&amp;rsquo;s try the client.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds-dc.png"
width="2001"
height="1509"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds-dc_hu_4b496410369e3b3a.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds-dc_hu_e711c6410482af7e.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="132"
data-flex-basis="318px"
&gt;&lt;/p&gt;
&lt;p&gt;Incorrect.&lt;/p&gt;
&lt;p&gt;Then, deepseek official.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds.png"
width="1260"
height="1538"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds_hu_449973d3bc8fb8ab.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/ds_hu_a4b89e87924b34eb.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="81"
data-flex-basis="196px"
&gt;&lt;/p&gt;
&lt;p&gt;Very close, and the answer seems reasonable. Unfortunately, it&amp;rsquo;s still incorrect.&lt;/p&gt;
&lt;h2 id="if-we-ask-chatgpt-directly"&gt;If we ask ChatGPT directly
&lt;/h2&gt;&lt;p&gt;Hiss, a bit off. Let&amp;rsquo;s try gpt-5.&lt;/p&gt;
&lt;p&gt;Prompt:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/wechat_2025-11-19_171536_131.png"
width="1275"
height="1616"
srcset="https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/wechat_2025-11-19_171536_131_hu_479ab0fea5b53f5f.png 480w, https://svtter.cn/p/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B-api-%E7%BB%93%E5%90%88--%E6%80%A7%E8%83%BD%E5%B0%8F%E6%B5%8B/pics/wechat_2025-11-19_171536_131_hu_33adf58eab061c65.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="78"
data-flex-basis="189px"
&gt;&lt;/p&gt;
&lt;h2 id="inference---reasons-for-poor-performance"&gt;Inference - Reasons for Poor Performance
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Insufficient search capability. The Bocha API is to blame.&lt;/li&gt;
&lt;li&gt;Different models may have different optimal hyperparameters for best performance. I called the large model API from Silicon Flow.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="conclusion"&gt;Conclusion
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;For this specific problem, ChatGPT still performs better. Compared to before, the official search + model combination also seems to perform better. Therefore, unless the data is particularly sensitive, it&amp;rsquo;s better to use the official service.&lt;/li&gt;
&lt;li&gt;This article is for reference only, just for fun.&lt;/li&gt;
&lt;/ul&gt;</description></item></channel></rss>