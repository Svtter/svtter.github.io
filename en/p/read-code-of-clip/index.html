<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Gaining New Inspiration from CLIP Code"><title>Read Code of CLIP</title><link rel=canonical href=https://svtter.cn/en/p/read-code-of-clip/><link rel=stylesheet href=/scss/style.min.922ed5da1826abed3ff2115253a79406aeb687c10fc0b8923d83062915d75d21.css><meta property='og:title' content="Read Code of CLIP"><meta property='og:description' content="Gaining New Inspiration from CLIP Code"><meta property='og:url' content='https://svtter.cn/en/p/read-code-of-clip/'><meta property='og:site_name' content="Svtter's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='CLIP'><meta property='article:tag' content='ViT'><meta property='article:tag' content='Deep Learning'><meta property='article:published_time' content='2025-03-19T13:23:50+08:00'><meta property='article:modified_time' content='2025-03-19T13:23:50+08:00'><meta property='og:image' content='https://svtter.cn/p/read-code-of-clip.md/image.png'><meta name=twitter:title content="Read Code of CLIP"><meta name=twitter:description content="Gaining New Inspiration from CLIP Code"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://svtter.cn/p/read-code-of-clip.md/image.png'><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=https://cdn.jsdelivr.net><link rel=preconnect href=https://latest.cactus.chat crossorigin><link rel=dns-prefetch href=https://latest.cactus.chat><link rel=preconnect href=https://fonts.googleapis.com crossorigin><link rel=dns-prefetch href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=https://fonts.gstatic.com><link rel=preconnect href=https://utteranc.es crossorigin><link rel=dns-prefetch href=https://utteranc.es><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","inLanguage":"en","name":"Svtter's Blog","potentialAction":{"@type":"SearchAction","query-input":"required name=search_term_string","target":{"@type":"EntryPoint","urlTemplate":"https://svtter.cn/search/?q={search_term_string}"}},"url":"https://svtter.cn/"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","logo":"https://svtter.cn/img/avatar.png","name":"Svtter's Blog","url":"https://svtter.cn/"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","accessMode":"textual,visual","accessibilityFeature":"alternativeText,readingOrder","accessibilitySummary":"The content is accessible for screen readers and keyboard navigation.","articleSection":68,"author":{"@type":"Person","name":"Svtter's Blog","url":"https://svtter.cn/"},"dateModified":"2025-03-19T13:23:50+08:00","datePublished":"2025-03-19T13:23:50+08:00","description":"Gaining New Inspiration from CLIP Code","headline":"Read Code of CLIP","image":"https://svtter.cn/image.png","inLanguage":"en","keywords":["CLIP","ViT","Deep Learning","Deep Learning"],"mainEntityOfPage":{"@id":"https://svtter.cn/en/p/read-code-of-clip/","@type":"WebPage"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://svtter.cn/img/avatar.png"},"name":"Svtter's Blog"},"timeRequired":"PT3M","wordCount":454}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://svtter.cn/en/","name":"Svtter's Blog","position":1},{"@type":"ListItem","item":"https://svtter.cn/en/post/","name":"Posts","position":2},{"@type":"ListItem","item":"https://svtter.cn/en/p/read-code-of-clip/","name":"Read Code of CLIP","position":3}]}</script><link rel="shortcut icon" href=/favicon.png><script async src="https://www.googletagmanager.com/gtag/js?id=G-65DJGXT4VC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-65DJGXT4VC")}</script><script src=https://app.rybbit.io/api/script.js data-site-id=382 defer></script></head><body class=article-page><script>(function(){const e="FriedRiceColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="FriedRiceColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><header class=site-header><nav class=site-nav><div class=nav-container><a href=/en class=site-brand><span class=brand-name>Svtter's Blog</span></a><div class=nav-links><a href=/en/about/ class=nav-link>About
</a><a href=/en/archives/ class=nav-link>Archives
</a><a href=/en/search/ class=nav-link>Search
</a><a href=/en/links/ class=nav-link>Links
</a><a href=/en/llms/ class=nav-link>LLMs</a></div><div class=nav-actions><div class=nav-social><a href=https://github.com/svtter target=_blank rel="me noopener" title=GitHub class=social-link><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg>
</a><a href=https://twitter.com/crack_svtter target=_blank rel="me noopener" title=Twitter class=social-link><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></div><a href=/en/index.xml target=_blank rel=alternate type=application/rss+xml title=RSS class="social-link rss-link"><svg class="icon icon-tabler icon-tabler-rss" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="5" cy="19" r="1" fill="currentColor"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg>
</a><a href=https://svtter.cn/p/read-code-of-clip.md/ class=language-toggle aria-label="Switch language" title="Switch language"><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
</a><button id=dark-mode-toggle class=theme-toggle aria-label="Toggle dark mode" title="Dark Mode">
<svg class="icon icon-tabler icon-tabler-sun" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="4"/><path d="M3 12h1m8-9v1m8 8h1m-9 8v1M5.6 5.6l.7.7m12.1-.7-.7.7m0 11.4.7.7m-12.1-.7-.7.7"/></svg>
<svg class="icon icon-tabler icon-tabler-moon" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M12 3c.132.0.263.0.393.0a7.5 7.5.0 007.92 12.446A9 9 0 1112 2.992z"/></svg>
</button>
<button class=mobile-menu-toggle id=toggle-menu aria-label="Toggle Menu" aria-expanded=false>
<span class=hamburger-line></span>
<span class=hamburger-line></span>
<span class=hamburger-line></span></button></div></div><div class=mobile-menu id=mobile-menu aria-hidden=true><a href=/en/about/ class=mobile-nav-link>About
</a><a href=/en/archives/ class=mobile-nav-link>Archives
</a><a href=/en/search/ class=mobile-nav-link>Search
</a><a href=/en/links/ class=mobile-nav-link>Links
</a><a href=/en/llms/ class=mobile-nav-link>LLMs</a><div class=mobile-social><a href=https://github.com/svtter target=_blank rel="me noopener" title=GitHub class=mobile-social-link><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg>
<span>GitHub</span>
</a><a href=https://twitter.com/crack_svtter target=_blank rel="me noopener" title=Twitter class=mobile-social-link><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg>
<span>Twitter</span>
</a><a href=/en/index.xml target=_blank rel=alternate type=application/rss+xml title=RSS class=mobile-social-link><svg class="icon icon-tabler icon-tabler-rss" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="5" cy="19" r="1" fill="currentColor"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg>
<span>RSS</span>
</a><a href=https://svtter.cn/p/read-code-of-clip.md/ class="mobile-social-link language-link"><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<span>Switch language</span></a></div></div></nav></header><div class=main-container><main class=main-content><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/en/p/read-code-of-clip/><img src=/p/read-code-of-clip.md/image_hu_bed4f8999dd62dd3.png srcset="/p/read-code-of-clip.md/image_hu_bed4f8999dd62dd3.png 800w, /p/read-code-of-clip.md/image_hu_10e1dc7e15822af9.png 1600w" width=800 height=282 loading=lazy alt="Featured image of post Read Code of CLIP"></a></div><div class=article-details><header class=article-category><a href=/en/categories/deep-learning/>Deep Learning</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/p/read-code-of-clip/>Read Code of CLIP</a></h2><h3 class=article-subtitle>Gaining New Inspiration from CLIP Code</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2025-03-19T13:23:50+08:00>Mar 19, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>3 minute read</time></div></footer><footer class=article-translations><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><div><a href=https://svtter.cn/p/read-code-of-clip.md/ class=link>简体中文</a></div></footer></div></header><section class=article-content><p>Contrastive Language-Image Pre-Training (CLIP) is one of the classic works from OpenAI, originating from the paper <a class=link href=https://arxiv.org/abs/2103.00020 target=_blank rel=noopener></a>.</p><p>To implement my new idea based on CLIP, I attempted to read <a class=link href=https://github.com/openai/CLIP target=_blank rel=noopener>openai/clip</a> to understand the fundamental working principles of CLIP in classification.</p><p>Here is the Python sample code provided by <a class=link href=https://github.com/openai/CLIP target=_blank rel=noopener>openai/clip</a>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>clip</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>PIL</span> <span class=kn>import</span> <span class=n>Image</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s2>&#34;cuda&#34;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&#34;cpu&#34;</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=p>,</span> <span class=n>preprocess</span> <span class=o>=</span> <span class=n>clip</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&#34;ViT-B/32&#34;</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>preprocess</span><span class=p>(</span><span class=n>Image</span><span class=o>.</span><span class=n>open</span><span class=p>(</span><span class=s2>&#34;CLIP.png&#34;</span><span class=p>))</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>clip</span><span class=o>.</span><span class=n>tokenize</span><span class=p>([</span><span class=s2>&#34;a diagram&#34;</span><span class=p>,</span> <span class=s2>&#34;a dog&#34;</span><span class=p>,</span> <span class=s2>&#34;a cat&#34;</span><span class=p>])</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>image_features</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode_image</span><span class=p>(</span><span class=n>image</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>text_features</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode_text</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>logits_per_image</span><span class=p>,</span> <span class=n>logits_per_text</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>image</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>probs</span> <span class=o>=</span> <span class=n>logits_per_image</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Label probs:&#34;</span><span class=p>,</span> <span class=n>probs</span><span class=p>)</span>  <span class=c1># prints: [[0.9927937  0.00421068 0.00299572]]</span>
</span></span></code></pre></td></tr></table></div></div><p>The load function is used to load a specific OpenAI model. This is based on <code>ViT-B/32</code>, a Vision Transformer 32B.</p><p>It can be seen that the vision encoders supported by OpenAI roughly include the following types:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>_MODELS</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;RN50&#34;</span><span class=p>:</span> <span class=s2>&#34;https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;RN101&#34;</span><span class=p>:</span> <span class=s2>&#34;https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;RN50x4&#34;</span><span class=p>:</span> <span class=s2>&#34;https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;RN50x16&#34;</span><span class=p>:</span> <span class=s2>&#34;https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;RN50x64&#34;</span><span class=p>:</span> <span class=s2>&#34;https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;ViT-B/32&#34;</span><span class=p>:</span> <span class=s2>&#34;https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;ViT-B/16&#34;</span><span class=p>:</span> <span class=s2>&#34;https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;ViT-L/14&#34;</span><span class=p>:</span> <span class=s2>&#34;https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;ViT-L/14@336px&#34;</span><span class=p>:</span> <span class=s2>&#34;https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>Assuming the model has already been downloaded, let&rsquo;s examine how the _transform preprocessing works:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_transform</span><span class=p>(</span><span class=n>n_px</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>        <span class=n>Resize</span><span class=p>(</span><span class=n>n_px</span><span class=p>,</span> <span class=n>interpolation</span><span class=o>=</span><span class=n>BICUBIC</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>CenterCrop</span><span class=p>(</span><span class=n>n_px</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>_convert_image_to_rgb</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>ToTensor</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>        <span class=n>Normalize</span><span class=p>((</span><span class=mf>0.48145466</span><span class=p>,</span> <span class=mf>0.4578275</span><span class=p>,</span> <span class=mf>0.40821073</span><span class=p>),</span> <span class=p>(</span><span class=mf>0.26862954</span><span class=p>,</span> <span class=mf>0.26130258</span><span class=p>,</span> <span class=mf>0.27577711</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>    <span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>It&rsquo;s not overly complex, though the preprocessing <code>Normalize</code> parameters are not entirely clear. It seems to use the same preprocessing parameters as ViT.</p><p>Then, moving into the model loading phase, we can see that if it&rsquo;s not <a class=link href=https://chenglu.me/blogs/pytorch-jit target=_blank rel=noopener>jit loading</a>, the model will opt for the state_dict mode.<br>Through the process of loading the state_dict, we can observe that the <code>build_model</code> function is used to load the weights and assign them to the existing model structure.</p><p>The file for this model structure is <a class=link href=https://github.com/openai/CLIP/blob/main/clip/model.py target=_blank rel=noopener>model.py</a>.<br>Therefore, the main code for CLIP is located at <a class=link href=https://github.com/openai/CLIP/blob/main/clip/model.py#L243 target=_blank rel=noopener>model.py#L243</a>.</p><p>The outputs of the <code>image_encoder</code> and <code>text_encoder</code> are two distinct feature tensors.</p><p>Performing a matrix multiplication on these two tensors yields a similarity matrix. The size of this similarity matrix is <code>(batch_size, batch_size)</code>.</p><blockquote><p>TIPS: If the batch size is too small, such as 1, the performance of contrastive learning may be significantly diminished.</p></blockquote><p>These two tensors are computed using symmetric cross-entropy loss to update the network weights.</p><p>Specifically focused on improving intelligence metrics, without much concern for computational cost. Not pursuing the latest or highest intelligence metrics, but more focused on the computational efficiency of the model.</p><blockquote><p>Trick: Adding a log to the parameters to make weight updates less drastic and reduce computational intensity.</p></blockquote><p>The <a class=link href=https://github.com/openai/CLIP target=_blank rel=noopener>CLIP</a> code does not provide directly trainable code. In the next article, we&rsquo;ll attempt to read openclip.</p></section><footer class=article-footer><section class=article-tags><a href=/en/tags/clip/>CLIP</a>
<a href=/en/tags/vit/>ViT</a>
<a href=/en/tags/deep-learning/>Deep Learning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/en/p/browsing-and-storing-image-datasets/><div class=article-details><h2 class=article-title>Browsing and Storing Image Datasets</h2></div></a></article><article class=has-image><a href=/en/p/diffusion-model/><div class=article-image><img src=/p/diffusion-model.md/noise-dog.4e980902d7fa4a82647bf2182f2c811f_hu_22709112f80b82c0.png width=250 height=150 loading=lazy alt="Featured image of post Diffusion Model" data-hash="md5-TpgJAtf6SoJke/IYLyyBHw=="></div><div class=article-details><h2 class=article-title>Diffusion Model</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=Svtter/svtter.github.io issue-term=pathname label=blog crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Svtter's Blog</section><section class=powerby>Theme <b>Fried Rice</b> designed by <a href=https://svtter.cn target=_blank rel=noopener>svtter</a><br>Source code on <a href=https://github.com/Svtter/Fried-Rice target=_blank rel=noopener>GitHub</a></section></footer><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.acc9fdfa3fa1412c783325ef28fd814b0d8fe9d21eaad163e4c8a97c862fbcff.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>