<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>机器学习 on Svtter's Blog</title><link>https://svtter.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 机器学习 on Svtter's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Mon, 04 Sep 2023 14:07:19 +0800</lastBuildDate><atom:link href="https://svtter.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>生成模型和判别模型</title><link>https://svtter.cn/p/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 04 Sep 2023 14:07:19 +0800</pubDate><guid>https://svtter.cn/p/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/</guid><description>&lt;p&gt;在李航的书中讲到，监督学习方法可以分为生成方法（generative approach）和判别方法 (discriminative approach) 。所学习到的模型分别称为生成模型和判别模型。&lt;/p&gt;
&lt;p&gt;之前一直搞不清楚这俩模型区别到底是什么，课程中也只是学习了一下条件概率和联合概率分布，理解总是有些偏差。
对于生成方法，模型通过对数据进行学习，得到联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型。
也就是说:&lt;/p&gt;
$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$&lt;p&gt;这个方法之所以称之为生成方法，是因为模型表示了&lt;strong&gt;给定输入$X$产生输出$Y$的生成关系&lt;/strong&gt;。
典型的生成模型有朴素贝叶斯法和隐马尔可夫模型。&lt;/p&gt;
&lt;p&gt;判别方法由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测模型，即判别模型。
判别方法关心的是给定输入$X$，应该预测什么样的输出$Y$。&lt;/p&gt;
&lt;p&gt;典型的判别模型包括：k近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。&lt;/p&gt;
&lt;p&gt;隐变量是不可观测的随机变量。&lt;/p&gt;
&lt;h2 id="区别"&gt;区别
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;对生成方法而言，学习收敛的速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型。如果存在隐变量，判别方法就不能用。&lt;/li&gt;
&lt;li&gt;对于判别方法而言，直接学习条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率更高。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我的理解：区别是一个是学习$P(X,Y)$，因此如果输入的样本发生变化，即$P(X)$发生变化，那么$P(Y|X)$只需要进行简单计算。相比之下，
直接使用$P(Y|X)$的模型则可能需要重新训练。&lt;/p&gt;
&lt;h2 id="附录"&gt;附录
&lt;/h2&gt;&lt;p&gt;决策函数指的是应用监督学习的方法：$Y=f(X)$，或者条件概率分布$P(Y|X)$。&lt;/p&gt;</description></item><item><title>My Keras tricks</title><link>https://svtter.cn/p/my-keras-tricks/</link><pubDate>Mon, 07 Jan 2019 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/my-keras-tricks/</guid><description>&lt;p&gt;记录了一些使用 keras 的技巧。&lt;/p&gt;
&lt;h2 id="categorical_crossentropy-vs-sparse_categorical_crossentropy"&gt;categorical_crossentropy vs sparse_categorical_crossentropy.
&lt;/h2&gt;&lt;h4 id="3-the-answerin-a-nutshell"&gt;3. The Answer, In a Nutshell
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If your targets are one-hot encoded, use categorical_crossentropy.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Examples of &lt;a href="https://jovianlin.io/keras-one-hot-encode-decode-sequence-data/" target="_blank" rel="noreferrer noopener"&gt;one-hot encodings&lt;/a&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[1,0,0]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[0,1,0]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[0,0,1]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But if your targets are integers, use sparse_categorical_crossentropy.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Examples of integer encodings (&lt;em&gt;for the sake of completion&lt;/em&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1, 2, 3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="clip-norm"&gt;&lt;strong&gt;clip norm&lt;/strong&gt;
&lt;/h2&gt;&lt;p style="text-align:center"&gt;
&lt;br /&gt;&lt;a rel="noreferrer noopener" href="https://wulc.me/2018/05/01/%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA%E5%8F%8A%E5%85%B6%E4%BD%9C%E7%94%A8/" target="_blank"&gt;https://wulc.me/2018/05/01/%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA%E5%8F%8A%E5%85%B6%E4%BD%9C%E7%94%A8/&lt;/a&gt;&lt;br /&gt;可以加速 RNN 训练
&lt;/p&gt;
&lt;h2 id="multiple-gpu"&gt;&lt;strong&gt;Multiple GPU&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;# &lt;a class="link" href="https://keras.io/utils/#multi" target="_blank" rel="noopener"
&gt;https://keras.io/utils/#multi&lt;/a&gt;_gpu_model#&lt;/p&gt;
&lt;p&gt;使用多GPU，注意使用 save 的时候，传参传 model (multi_gpu_model的model参数)&lt;/p&gt;
&lt;h2 id="encode-labels"&gt;&lt;strong&gt;Encode Labels&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;可以把不同的字符[‘aa’, ‘bb’, ‘cc’, ‘aa’] 编码成 [0, 1, 2, 0]&lt;/p&gt;
&lt;table class="wp-block-table"&gt;
&lt;tr&gt;
&lt;td&gt;
1&lt;br /&gt;2&lt;br /&gt;3&lt;br /&gt;4&lt;br /&gt;5&lt;br /&gt;6
&lt;/td&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;td&amp;gt;
# encode class values as integers&amp;lt;br /&amp;gt;encoder&amp;amp;nbsp;=&amp;amp;nbsp;LabelEncoder()&amp;lt;br /&amp;gt;encoder.fit(Y)&amp;lt;br /&amp;gt;encoded_Y&amp;amp;nbsp;=&amp;amp;nbsp;encoder.transform(Y)&amp;lt;br /&amp;gt;# convert integers to dummy variables (i.e. one hot encoded)&amp;lt;br /&amp;gt;dummy_y&amp;amp;nbsp;=&amp;amp;nbsp;np_utils.to_categorical(encoded_Y)
&amp;lt;/td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id="训练中存在的问题"&gt;训练中存在的问题
&lt;/h2&gt;&lt;p&gt;训练性能低了别急着调参，首先看看数据预处理有没有问题，评价指标是不是写错了。再一个，batch norm 要勤快点加上。&lt;/p&gt;
&lt;p&gt;相似的文章还有：&lt;a class="link" href="https://svtter.github.io/2018/02/01/keras%e5%9d%91/" target="_blank" rel="noopener"
&gt;https://svtter.github.io/2018/02/01/keras%e5%9d%91/&lt;/a&gt;&lt;/p&gt;</description></item><item><title>beam search – 一个搜索策略</title><link>https://svtter.cn/p/beam-search-%E4%B8%80%E4%B8%AA%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5/</link><pubDate>Fri, 23 Nov 2018 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/beam-search-%E4%B8%80%E4%B8%AA%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5/</guid><description>&lt;p&gt;这篇文章不建议读，2018年写的，不知所云。&lt;/p&gt;
&lt;p&gt;beam search 是一个近似搜索策略，用于在候选可能中选择最好的结果。&lt;a class="link" href="https://hackernoon.com/beam-search-a-search-strategy-5d92fb7817f" target="_blank" rel="noopener"
&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i2.wp.com/cdn-images-1.medium.com/max/1760/1*HmPwWf0VJV3TeaGKoly4Xw.png?w=625&amp;amp;#038;ssl=1"
loading="lazy"
alt="img"
&gt;&lt;/p&gt;
&lt;!-- &lt;img src="" alt="" data-recalc-dims="1" /&gt;&lt;/figure&gt; --&gt;
&lt;p&gt;一个常用例子，BS(beam search) 用于获得与机器翻译等价的结果。对于那些不了解机器翻译的人，也肯定知道 Google Translate。&lt;/p&gt;
&lt;p&gt;这就是为啥要讲这个。这些系统都用 BS 技术来找到与结果最等价的翻译。阅读这个 Wiki 来了解相同文件的定义。&lt;/p&gt;
&lt;p&gt;让我们讨论一下这个使用机器翻译案例的策略。如果你是一个喜欢研究现象背后原理的人，一定要读一下 google encoder-decoder 网络架构。这个东西我就不讲了，有很多人讲。例如，如果你不知道这个架构，看看这个 quora 上的回答。&lt;/p&gt;
&lt;h2 id="一个视角"&gt;一个视角
&lt;/h2&gt;&lt;p&gt;机器翻译模型可以被认为是一种 “条件语言模型”，对于…&lt;/p&gt;
&lt;h2 id="让我们看一下8230"&gt;让我们看一下…
&lt;/h2&gt;&lt;p&gt;BS B(beam 宽度) 是唯一一个调整翻译结果的超参。 B 在一般情况决定了，在每一步，要记忆的单词的个数，来变换概率。&lt;/p&gt;
&lt;h2 id="不翻译了这里有更直接的结果"&gt;不翻译了。。这里有更直接的结果
&lt;/h2&gt;&lt;p&gt;beam search 时在每一个时间点选择 beam_width 个最大的可能类别，然后在每个时间点 beam_width 个类别组成的空间里寻找整体概率最大的一条路径，得到最后得识别输出。而 greedy search 则直接在每个时间点寻找概率最大的类别，然后依次组成这个路径。也就是说，greedy search 是 beam_width=1 版本的 beam search。上图是 CTC 论文里 greedy search 示意图。&lt;/p&gt;</description></item><item><title>对于 CTC 的一个直观理解与解释</title><link>https://svtter.cn/p/%E5%AF%B9%E4%BA%8E-ctc-%E7%9A%84%E4%B8%80%E4%B8%AA%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E4%B8%8E%E8%A7%A3%E9%87%8A/</link><pubDate>Thu, 22 Nov 2018 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/%E5%AF%B9%E4%BA%8E-ctc-%E7%9A%84%E4%B8%80%E4%B8%AA%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E4%B8%8E%E8%A7%A3%E9%87%8A/</guid><description>&lt;blockquote&gt;
&lt;p&gt;这篇文章是一个翻译：&lt;a class="link" href="https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c" target="_blank" rel="noopener"
&gt;towardsdatascience-ctc&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通过 CTC loss 以及编码操作进行文字识别。&lt;/p&gt;
&lt;p&gt;如果想用使用计算机识别文字，神经网络很好用。使用一些列 CNN 从序列中提取特征，使用 RNN 来传播需略的信息。它会输出字符得分，给每个序列元素，通过一个简单的矩阵表示。现在，有两个事情我们想要对矩阵进行处理。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;训练：计算损失值来训练神经网络&lt;/li&gt;
&lt;li&gt;推理：解码矩阵来获得图片中的字符&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;两个任务可以同时被 CTC 操作完成。对于手写数字系统的描述，可以参见图像 1.&lt;/p&gt;
&lt;p&gt;我们更进一步看看 CTC 操作，并且讨论一下它如完成的，以及它背后的公式是如此巧妙。最后，我将会指点你来找到 Python 代码以及不复杂的公式，如果你感兴趣的话。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i0.wp.com/cdn-images-1.medium.com/max/1760/1*i2OG4hu9EjsyWcVMc4OOvA.png?w=625&amp;amp;#038;ssl=1"
loading="lazy"
alt="img"
&gt;&lt;/p&gt;
&lt;h2 id="为什么我们使用-ctc"&gt;为什么我们使用 CTC
&lt;/h2&gt;&lt;p&gt;当然我们可以创建一个数据集，这个数据集有文本行，然后指出每列属于哪一个字符，就像图 2 中展示的那样。然后，我们可以训练一个神经网络来输出每一列的得分。而然，对于这个简单的解法，这里有两个问题。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;这个十分的耗时（以及无聊）来在字符层面上标注数据&lt;/li&gt;
&lt;li&gt;我们仅仅能够得到字符的得分，因此还需要一些操作来获取最终的文本。一个简单地字符可以跨越多个位置，比如，我们得到 “ttooo”，是因为 “o” 是一个比较宽的字符。我们已经删除了多余的 “t” and “o”，但是，如果要识别的字符是 “too”，我们应该怎么办？如果删除了多余的 “o”，将会给我们错误的答案。我们应该如何处理呢？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;CTC 解决了几个问题&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们只需要告诉 CTC loss function，文本在图像中出现了，因此我们忽略位置和宽度文中在图像中。&lt;/li&gt;
&lt;li&gt;不需要更多的文本识别处理&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src="https://i0.wp.com/cdn-images-1.medium.com/max/1760/1*rfeNj9aNfUSqA-SXB_i4Bw.png"
loading="lazy"
alt="img"
&gt;&lt;/p&gt;
&lt;h2 id="ctc-如何工作的"&gt;CTC 如何工作的
&lt;/h2&gt;&lt;p&gt;就像是我们已经讨论的，我们不希望在图像的每一列标注数据（这曾经被我们成为时间步）。神经网络的训练将会被 CTC 损失函数所指引。我们只需要把数据矩阵给 CTC 函数，以及对应的真实值即可。但是它是怎么知道每一个字符出现的呢？他不知道。相对而言，它尝试了图片中所有的真实文本，以及计算了所有的加和。通过这个方式， This way, the score of a GT text is high if the sum over the alignment-scores has a high value.&lt;/p&gt;
&lt;h2 id="编码文本"&gt;编码文本
&lt;/h2&gt;&lt;p&gt;如何编码重复的文本曾经是一个问题。这个问题通过引入一个虚假字符来解决了（称为空，但是不要把它和真正的 space 混淆。）。这个特殊的字符被标记为 “-”，在下面的文本中。我们使用了一个聪明的编码策略来解决重复字符的问题。当编码一个文本的时候，我们可以随机加入许多空在任何位置中，当我们解码的时候，我们将会把的这些删除。但是，我们必须在重复字符串中加入空，例如 “he&lt;strong&gt;ll&lt;/strong&gt;o”，如此一来，重复字符就不是问题了。&lt;/p&gt;
&lt;p&gt;Let’s look at some examples:&lt;/p&gt;
&lt;p&gt;“to” → “—ttttttooo”, or “-t-o-”, or “to” “too” → “—ttttto-o”, or “-t-o-o-”, or “to-o”, but not “too”&lt;/p&gt;
&lt;p&gt;正如你所见，这些模式也允许我们简单的创建一些相同文本串的不同对取，比如 “t-o”, 以及 “too”，以及 “-to”，所有的表示都是同一个文本 “to”，但是通过对图片不同对其获得的。神经网络被训练于输出一个编码的文本（在神经网络的矩阵中编码）。&lt;/p&gt;
&lt;h2 id="损失计算"&gt;损失计算
&lt;/h2&gt;&lt;p&gt;我们需要计算每一个损失值，这个损失值是由图像核真实文本给出来训练 NN 的。你已经知道 NN 输出一个矩阵，包含一个得分，为每个文本在每个时间步上。一个小矩阵在图三中展示：有许多的时间步（t0, t1），以及三个字符（”a”, “b”, 以及 blank “-“).&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i1.wp.com/cdn-images-1.medium.com/max/1760/1*BFQYgGofh6HOxnGdkJnO-w.png?w=625&amp;amp;#038;ssl=1"
loading="lazy"
alt="img"
&gt;&lt;/p&gt;
&lt;p&gt;此外，你已经知道，loss 是通过加和所有积分来进行计算的，通过这个方式，字符出现在图片的哪个位置不重要。&lt;/p&gt;
&lt;p&gt;对于一个 alignment 的得分（或者 path；在文学中一般这么称呼）通过将相应的字符相乘。在上面的例子中，path”aa” 的得分是：0.4*0.4=0.16，”a-” 的得分是 0.4*0.6=0.24，”-a” 的得分是 0.6*0.4=0.24. 为了获得 GT 文本的得分，我们加和这个文本的所有 path 的得分。我们假设，GT 文本是 “a”，我们已经计算了所有长度为 “2” 的 path，分别是 “aa”，“a-”，“-a”，我们已经计算了这些 path 所有的得分，所以我们只需要把他们加起来，得到 0.4×0.4 + 0.4×0.6 + 0.6×0.4 = 0.64。如 GT 文本可能是 “”，我们可以看到只有一种相关的 path，那么就是 “–”，获得的得分是 0.6×0.6 =0.36.&lt;/p&gt;
&lt;p&gt;如果仔细看，你已经发现我们计算了 GT 的可能性，但是不是 loss 值，而然，loss 知识概率的负对数。这个 loss 值是反向传播算法以及 NN 的参数更新使用的，我这里没有进行详细的讨论。&lt;/p&gt;
&lt;h2 id="解码"&gt;解码
&lt;/h2&gt;&lt;p&gt;当我们训练一个 NN，我们想要使用它来识别那些之前没有看到的图像。或者更多在更多的技术术语：我们想要计算，NN 输出的矩阵最可能是什么。你已经知道一个方法来计算给出文本的得分，但是现在，我们没有被给出任何文本，事实上，它正是我们正在寻找的文本。尝试所有可能的文本，如果他们只有很少的时间步以及字符，但是对于练习用例而言，这不可行。&lt;/p&gt;
&lt;p&gt;一个简单而快速的算法，是最佳 path 解码，包含两个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;它计算了最佳 path，通过获取最可能的每一个时间步的字符&lt;/li&gt;
&lt;li&gt;首先删除重复的字符，然后删除 path 里面所有的空。这仍然表示了识别的文本。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;正如 FIG4 所展示的，字符是 “a”，“b” 以及 “-”（空），一共有 5 个时间步。让我们应用最佳 path decoder 来处理这个矩阵。在 t0，最可能的是“a”，同样应用于 t1，t2. 空字符在 t3 是最可能的。最后，“b” 是 t4 时刻最可能的。这将给出我们一个 path“aaa-b”，我们删除了重复的字符，这将会返回“a-b”，然后我们删除 path 中的空，这给我们一个“ab”，作为我们输出的识别结果。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i0.wp.com/cdn-images-1.medium.com/max/1600/1*1_5KnLvaTkGUFoyat2jHcQ.png?w=625&amp;amp;#038;ssl=1"
loading="lazy"
alt="img"
&gt;&lt;/p&gt;
&lt;p&gt;最佳 path 解析是，当容纳，仅仅是一个近似。构建样例容易给出错误的结果，比如用这个方法构建 FIG3，将会得到 “”，作为识别文本。但是，我们已经知道“” 结果的概率是 0.36，而 “a” 的概率是 0.64。而然，近似算法经常在练习的情景下给出比较好的结果。也有许多其他的比较好的 decoder，例如 beam-search，prefix-search 以及 token passing，这些关于语言结构的方法，都有利于提升结果。&lt;/p&gt;
&lt;h2 id="结论以及展望"&gt;结论以及展望
&lt;/h2&gt;&lt;p&gt;首先我们看得是，神经网络如何解决这个问题；然后，我们展示了 CTC 如何解决这些问题，然后，我们解释了 CTC 为啥能够工作，如何计算的 loss，以及如何解码 CTC 训练的 NN。&lt;/p&gt;
&lt;p&gt;This should give you a good understanding of what is happening behind the scenes when you e.g. call functions like ctc_loss or ctc_greedy_decoder in TensorFlow. However, when you want to implement CTC yourself, you need to know some more details, especially to make it run fast. Graves et al. &lt;/p&gt;
\[1] introduce the CTC operation, the paper also shows all the relevant math. If you are interested in how to improve&amp;nbsp;decoding,&amp;nbsp;take a look&amp;nbsp;at the articles about beam search&amp;nbsp;decoding&amp;nbsp;[2\]\[3\]&lt;p&gt;. I implemented some decoders and the loss function in Python and C++, which you can find on github &lt;/p&gt;
\[4\]\[5\]&lt;p&gt;. Finally, if you want to look at the bigger picture of how to recognize (handwritten) text, look at my article on how to build a handwritten text recognition system [6].&lt;/p&gt;</description></item><item><title>使用主动学习加速机器学习</title><link>https://svtter.cn/p/%E4%BD%BF%E7%94%A8%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link><pubDate>Tue, 20 Nov 2018 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/%E4%BD%BF%E7%94%A8%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</guid><description>&lt;blockquote class="wp-block-quote"&gt;
&lt;p&gt;
一篇 medium 文章的渣翻
&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;&lt;a class="link" href="https://becominghuman.ai/accelerate-machine-learning-with-active-learning-96cea4b72fdb" target="_blank" rel="noopener"
&gt;https://becominghuman.ai/accelerate-machine-learning-with-active-learning-96cea4b72fdb&lt;/a&gt;&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;让我们讨论一下主动学习。我相信这个方法可以极大的增速，以及减少许多机器学习工程的花费。这篇文章我将从两个部分说明这个问题。在第一部分，我给出了一个极高的层级的主动学习的说明，以及如何把它利用到机器学习工程中。在第二部分，深入到一个主动学习 demo 中。&lt;/p&gt;
&lt;h2 id="第一部分"&gt;第一部分
&lt;/h2&gt;&lt;h3 id="主动学习是如何工作的"&gt;主动学习是如何工作的
&lt;/h3&gt;&lt;p&gt;让我们通过一个很简单的概览，来看看机器学习是如何工作地。&lt;figure class="wp-block-image"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i2.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdd3awts2j218g0j4wj2.jpg?w=625" alt="Repeat thousands of times and you get a trained model!" data-recalc-dims="1" /&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;许多机器学习模型是巨大的猜疑机器——他们看了许多数据，计算出一个猜测的结果，检查他们的答案，微调一下，然后再试试。在许多数据之后，模型将会变得十分准确。&lt;/p&gt;
&lt;h3 id="标记数据"&gt;标记数据
&lt;/h3&gt;&lt;p&gt;…&lt;/p&gt;
&lt;h3 id="主动学习"&gt;主动学习
&lt;/h3&gt;&lt;p&gt;主动学习是一种方法，有时可以极大减少标记样本的数量。它通过专家标记样本来完成这个工作。&lt;/p&gt;
&lt;p&gt;不使用全部的数据一次标注所有数据，主动学习优先处理那些让模型感到困惑的数据，并且仅仅需要好那些数据的标签。模型在小样本数据上进行训练，然后根据那些最令模型疑惑的数据，请求更多的标签。&lt;/p&gt;
&lt;p&gt;通过优先处理那些最迷惑的样本，模型可以专注于提供一些最有价值的信息。这帮助模型训练的更快，并且让专家跳过那些对于模型帮助不是很大的数据。结果是，我们可以很大程度上减少标记样本的数量，并且我们仍然得到一个很好的模型。这意味着节省时间和金钱！&lt;/p&gt;
&lt;h2 id="第二部分"&gt;第二部分
&lt;/h2&gt;&lt;h3 id="mnist-例子"&gt;MNIST 例子
&lt;/h3&gt;&lt;p&gt;让我们看一下实际的主动学习样本。&lt;/p&gt;
&lt;p&gt;使用文档良好的 MNIST 数据集，以及经典的 Tensorflow 卷积神经完了过。一个聪明的模型和架构可以做的更好，但是我们想要直接使用这个模型。&lt;/p&gt;
&lt;p&gt;MNIST 数据集是公开可获取得的数据集，包含了大量的手写数字，以及数值标签。它经常被使用于机器学习入门教程，因为他的标记数据质量很高，并且简单地模型也可以表现的不错。&lt;figure class="wp-block-image"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i1.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdd4dewm6j20ho04fq38.jpg?w=625" alt="" data-recalc-dims="1" /&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id="设计"&gt;设计
&lt;/h3&gt;&lt;p&gt;这个工程包含两个部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在训练模型的时候，模仿主动学习&lt;/li&gt;
&lt;li&gt;在严格的模型上确定主动学习的效率&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="训练一个模型"&gt;训练一个模型
&lt;/h3&gt;&lt;p&gt;我们使用 mini-batch 训练。这个模型仅仅在训练集中，看一个小数量样本，通过小数量样本进行学习。&lt;/p&gt;
&lt;p&gt;这里，我们可以看到一个正常的（非 – 主动学习）的训练过程，模型在一个随机结合的小批次上进行训练。每在小 – 批次训练中的迭代，都在测试记上运行模型（不作为训练集的一部分）来追踪模型是怎样增长的。我提供了准确率以及 cross-entropy 损失（就像是平均误差一样）。在这里，每一个小批次有个 10 个例子，我运行了 2000 批次（20000 个标注）。&lt;figure class="wp-block-image"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i1.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdd4pv3b2j218g0fhjtx.jpg?w=625" alt="" data-recalc-dims="1" /&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;对于这个分类任务，我们试图把 0~9 的数字进行分类，意味着随机猜测仅有 10% 的准确率。简单的神经网络已经做的不错了。&lt;/p&gt;
&lt;h3 id="模拟主动学习"&gt;模拟主动学习
&lt;/h3&gt;&lt;p&gt;获得主动学习结果有一点小技巧。我们不在数据集中的随机选择数据，相反，模型将会评估许多在训练集中的例子，然后将置信度最小的数据作为小 – 批次（在这个工程中，我查看了 1000 个在训练集中的随机样本，来确定置信度至少为 10）。在那里，模型将会像处理小 – 批次数据一样处理进行训练过程，它将会重复这个过程来更新模型。就像是在 “非 – 主动学习” 样例中，每经过一些迭代，我将会在测试记上运行模型，追踪模型的训练过程。&lt;figure class="wp-block-image"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i2.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdd5z4a18j218g0ljjwd.jpg?w=625" alt="" data-recalc-dims="1" /&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;有许多很好的文章来说明如何实现 主动学习。在这里，我仅仅想要把事情做的简单一些。这个模型使用一个 “softmax” 来生成概率——在这个例子中，是数字 0~9。” 置信度” 通过选择” 最大的概率减去最小的概率 “。模型越自信，这个差值越大。（置信度不意味着准确率）。&lt;figure class="wp-block-image"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i0.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdd96i561j218g0ejte0.jpg?w=625" alt="" data-recalc-dims="1" /&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;主动学习过程，使用了那些置信度比较低的数据，并且在上面进行训练。并且当然的，当模型改变了，它的置信度也一样会改变。&lt;/p&gt;
&lt;p&gt;MNIST 数据集已经有了我们需要的标签，但是这个过程，在 mini-batch 中，模拟询问了专家，来获取标签。在通常情况下，专家会随机被提问，来获取数据。在主动学习的例子中，模型会选择那些数据，希望专家进行标注。&lt;/p&gt;
&lt;p&gt;让我们来看一下主动学习结果 VS 一般的结果。注意 y-axis。&lt;figure class="wp-block-image"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i1.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxddharrn6j218g0h378k.jpg?w=625" alt="" data-recalc-dims="1" /&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;通过 mini-batch（8000 标签），主动学习的方法匹配了 2000 mini-batch（20000label）数据的准确率。所以，使用一接近一半的数据，主动学习可以达到同样地准确率。&lt;/p&gt;
&lt;h3 id="倾斜数据的二分类任务"&gt;倾斜数据的二分类任务
&lt;/h3&gt;&lt;p&gt;主动学习可以大施拳脚的地方是，数据的强烈偏差。&lt;/p&gt;
&lt;p&gt;训练一个模型的时候，重要的不仅仅是标记的数据，还有不同数据的，合理的不同表示。如果我们尝试在 MNIST 上训练一个模型，而没有任何包含 3 的数据，收集多少数据并不重要，重要的是我们的模型不可能区分 3。如果我们仅仅含有一小部分 3，我们仍然会面临一个问题，就是模型仅仅会准确的区分其他数字，也就是那些有更好表达的数据。&lt;/p&gt;
&lt;p&gt;数据的偏差，不均衡对于 MNIST 数据集中不存在，但是它的确是一个真实世界的问题。如果我们训练一个模型来识别 CT 中的脑瘤，大多数 CT 图像不会含有肿瘤图像，所以标注 “肿瘤” 的数据将会远不均衡于 “非肿瘤” 的样本数据。因为主动学习优先考虑的例子不那么自信，因此主动学习可能有助于识别 “异常 “或代表性不足的数据并且确定优先级。&lt;/p&gt;
&lt;p&gt;我们在 MNIST 上模拟一下 skew 的问题。重新定义 MNIST 的问题，定义成 3 或者非 3，然后，非 3 的数据有 90%，而 3 的数据仅有 10%。所以愚蠢的策略将会在” 非 3“上达到 90% 的准确率，让我们看一下主动学习是如何做的：&lt;figure class="wp-block-image"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i2.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdf9177f8j218g0hdwkb.jpg?w=625" alt="" data-recalc-dims="1" /&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;在使用主动学习的时候，在 500mini-batch（5000labels）我们就达到，甚至更好地准确率。相比之下，cross-entropy 算法，通过 2000 mini-batch。主动学习减少了 4 倍的数据量。主动学习是如何做到的？看下图。&lt;figure class="wp-block-image"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://i2.wp.com/ww1.sinaimg.cn/large/c53b1907ly1fxdfe5ahvnj20ua0nkjv0.jpg?w=625" alt="" data-recalc-dims="1" /&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id="后续"&gt;后续
&lt;/h2&gt;&lt;p&gt;未完，后面翻不翻看心情。。也不知道工业界玩 active learning 的多不多。&lt;/p&gt;</description></item><item><title>批量转换ipynb</title><link>https://svtter.cn/p/%E6%89%B9%E9%87%8F%E8%BD%AC%E6%8D%A2ipynb/</link><pubDate>Fri, 20 Apr 2018 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/%E6%89%B9%E9%87%8F%E8%BD%AC%E6%8D%A2ipynb/</guid><description>&lt;p&gt;一段脚本将&lt;code&gt;ipython notebook&lt;/code&gt;转化为&lt;code&gt;py&lt;/code&gt;文件。&lt;/p&gt;
&lt;p&gt;It’s hard to make &lt;code&gt;notebook&lt;/code&gt; file to &lt;code&gt;import&lt;/code&gt; so it’s important to make &lt;code&gt;notebook&lt;/code&gt; importable.&lt;/p&gt;
&lt;div class="codehilite"&gt;
&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class="c1"&gt;# coding: utf-8&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;nbformat&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nbconvert&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PythonExporter&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;convertNotebook&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;notebookPath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;modulePath&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;notebookPath&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;fh&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;nb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nbformat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fh&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;nbformat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;NO_CONVERT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;exporter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PythonExporter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;meta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exporter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_notebook_node&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;modulePath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w+&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;fh&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;fh&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writelines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;trans_all&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;list_dirs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;listdir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;list_dirs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endswith&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.ipynb&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;py&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;convertNotebook&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;py&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;trans_all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;</description></item><item><title>Keras坑</title><link>https://svtter.cn/p/keras%E5%9D%91/</link><pubDate>Thu, 01 Feb 2018 01:00:00 +0800</pubDate><guid>https://svtter.cn/p/keras%E5%9D%91/</guid><description>&lt;p&gt;使用Keras做分类的时候踩了一个坑，也是拿来主义的锅，估计也有不少同志遇到。&lt;/p&gt;
&lt;p&gt;在进行分类的时候，往往使用&lt;code&gt;categorical_crossentropy&lt;/code&gt;，有时候萌新（像我）会用&lt;code&gt;binary_crossentropy&lt;/code&gt;，虽然结果可能上浮30%，但是这个结果是不对的。&lt;code&gt;model.fit&lt;/code&gt;以及&lt;code&gt;model.evaluate&lt;/code&gt;给出的&lt;code&gt;acc&lt;/code&gt;的值都是有问题的，正确的计算方法应该是：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;span class="lnt"&gt;2
&lt;/span&gt;&lt;span class="lnt"&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Actual accuracy calculated manually:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;其中&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt;1
&lt;/span&gt;&lt;span class="lnt"&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 返回沿轴axis最大值的索引。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;使用测试样本的数量代替&lt;code&gt;10000&lt;/code&gt;，输出的acc才是正确的结果。&lt;/p&gt;
&lt;p&gt;这个方法是使用二分类的时候才能使用的，label的个数多于2就不能使用。&lt;/p&gt;
&lt;h2 id="参考"&gt;参考
&lt;/h2&gt;&lt;p&gt;&lt;a class="link" href="https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance" target="_blank" rel="noopener"
&gt;https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>