<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Svtter&#39;s Blog</title>
    <link>https://svtter.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Svtter&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 23 Nov 2018 01:00:00 +0800</lastBuildDate><atom:link href="https://svtter.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>beam search – 一个搜索策略</title>
      <link>https://svtter.cn/2018/11/23/beam-search-%E4%B8%80%E4%B8%AA%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5/</link>
      <pubDate>Fri, 23 Nov 2018 01:00:00 +0800</pubDate>
      
      <guid>https://svtter.cn/2018/11/23/beam-search-%E4%B8%80%E4%B8%AA%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5/</guid>
      <description>一个常用例子，BS(beam search) 用于获得与机器翻译等价的结果。对于那些不了解机器翻译的人，也肯定知道 Google Translate。
这就是为啥要讲这个。这些系统都用 BS 技术来找到与结果最等价的翻译。阅读这个 Wiki 来了解相同文件的定义。
让我们讨论一下这个使用机器翻译案例的策略。如果你是一个喜欢研究现象背后原理的人，一定要读一下 google encoder-decoder 网络架构。这个东西我就不讲了，有很多人讲。例如，如果你不知道这个架构，看看这个 quora 上的回答。
一个视角 机器翻译模型可以被认为是一种 “条件语言模型”，对于…
让我们看一下… BS B(beam 宽度) 是唯一一个调整翻译结果的超参。 B 在一般情况决定了，在每一步，要记忆的单词的个数，来变换概率。
不翻译了。。这里有更直接的结果 beam search 时在每一个时间点选择 beam_width 个最大的可能类别，然后在每个时间点 beam_width 个类别组成的空间里寻找整体概率最大的一条路径，得到最后得识别输出。而 greedy search 则直接在每个时间点寻找概率最大的类别，然后依次组成这个路径。也就是说，greedy search 是 beam_width=1 版本的 beam search。上图是 CTC 论文里 greedy search 示意图。</description>
    </item>
    
    <item>
      <title>对于 CTC 的一个直观理解与解释</title>
      <link>https://svtter.cn/2018/11/22/%E5%AF%B9%E4%BA%8E-ctc-%E7%9A%84%E4%B8%80%E4%B8%AA%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E4%B8%8E%E8%A7%A3%E9%87%8A/</link>
      <pubDate>Thu, 22 Nov 2018 01:00:00 +0800</pubDate>
      
      <guid>https://svtter.cn/2018/11/22/%E5%AF%B9%E4%BA%8E-ctc-%E7%9A%84%E4%B8%80%E4%B8%AA%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E4%B8%8E%E8%A7%A3%E9%87%8A/</guid>
      <description>通过 连接主义时间分类 loss 以及编码操作
如果想用使用计算机识别文字，神经网络很好用。使用一些列 CNN 从序列中提取特征，使用 RNN 来传播需略的信息。它会输出字符得分，给每个序列元素，通过一个简单的矩阵表示。现在，有两个事情我们想要对矩阵进行处理。
训练：计算损失值来训练神经网络 推理：解码矩阵来获得图片中的字符 两个任务可以同时被 CTC 操作完成。对于手写数字系统的描述，可以参见图像 1.
我们更进一步看看 CTC 操作，并且讨论一下它如完成的，以及它背后的公式是如此巧妙。最后，我将会指点你来找到 Python 代码以及不复杂的公式，如果你感兴趣的话。
为什么我们使用 CTC 当然我们可以创建一个数据集，这个数据集有文本行，然后指出每列属于哪一个字符，就像图 2 中展示的那样。然后，我们可以训练一个神经网络来输出每一列的得分。而然，对于这个简单的解法，这里有两个问题。
这个十分的耗时（以及无聊）来在字符层面上标注数据 我们仅仅能够得到字符的得分，因此还需要一些操作来获取最终的文本。一个简单地字符可以跨越多个位置，比如，我们得到 “ttooo”，是因为 “o” 是一个比较宽的字符。我们已经删除了多余的 “t” and “o”，但是，如果要识别的字符是 “too”，我们应该怎么办？如果删除了多余的 “o”，将会给我们错误的答案。我们应该如何处理呢？ CTC 解决了几个问题
我们只需要告诉 CTC loss function，文本在图像中出现了，因此我们忽略位置和宽度文中在图像中。 不需要更多的文本识别处理 CTC 如何工作的 就像是我们已经讨论的，我们不希望在图像的每一列标注数据（这曾经被我们成为时间步）。神经网络的训练将会被 CTC 损失函数所指引。我们只需要把数据矩阵给 CTC 函数，以及对应的真实值即可。但是它是怎么知道每一个字符出现的呢？他不知道。相对而言，它尝试了图片中所有的真实文本，以及计算了所有的加和。通过这个方式， This way, the score of a GT text is high if the sum over the alignment-scores has a high value.</description>
    </item>
    
    <item>
      <title>使用主动学习加速机器学习</title>
      <link>https://svtter.cn/2018/11/20/%E4%BD%BF%E7%94%A8%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Tue, 20 Nov 2018 01:00:00 +0800</pubDate>
      
      <guid>https://svtter.cn/2018/11/20/%E4%BD%BF%E7%94%A8%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;https://becominghuman.ai/accelerate-machine-learning-with-active-learning-96cea4b72fdb&#34;&gt;https://becominghuman.ai/accelerate-machine-learning-with-active-learning-96cea4b72fdb&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;让我们讨论一下主动学习。我相信这个方法可以极大的增速，以及减少许多机器学习工程的花费。这篇文章我将从两个部分说明这个问题。在第一部分，我给出了一个极高的层级的主动学习的说明，以及如何把它利用到机器学习工程中。在第二部分，深入到一个主动学习 demo 中。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>批量转换ipynb</title>
      <link>https://svtter.cn/2018/04/20/%E6%89%B9%E9%87%8F%E8%BD%AC%E6%8D%A2ipynb/</link>
      <pubDate>Fri, 20 Apr 2018 01:00:00 +0800</pubDate>
      
      <guid>https://svtter.cn/2018/04/20/%E6%89%B9%E9%87%8F%E8%BD%AC%E6%8D%A2ipynb/</guid>
      <description>一段脚本将ipython notebook转化为py文件。
It’s hard to make notebook file to import so it’s important to make notebook importable.</description>
    </item>
    
  </channel>
</rss>
