<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Prompt on Svtter's Blog</title><link>https://svtter.cn/categories/prompt/</link><description>Recent content in Prompt on Svtter's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 04 Apr 2025 11:20:42 +0800</lastBuildDate><atom:link href="https://svtter.cn/categories/prompt/index.xml" rel="self" type="application/rss+xml"/><item><title>Prompt Is a Soft Contrain.md</title><link>https://svtter.cn/p/prompt-is-a-soft-contrain.md/</link><pubDate>Fri, 04 Apr 2025 11:20:42 +0800</pubDate><guid>https://svtter.cn/p/prompt-is-a-soft-contrain.md/</guid><description>&lt;img src="https://svtter.cn/p/prompt-is-a-soft-contrain.md/ds.jpg" alt="Featured image of post Prompt Is a Soft Contrain.md" />&lt;p>大语言模型的提示词本质上是一种软约束（soft contrain）。&lt;/p>
&lt;p>我们经常会发现，尽管我们对大模型提出了要求，这种要求往往是通过提示词实现的，但大模型仍然会输出超出我们要求的内容。&lt;/p>
&lt;p>因此，这种要求不是硬性的，永远都会有极小的可能性会出现问题。例如 CRNN，也一定会出现识别的字符串超出原本字符串长度的情况。&lt;/p>
&lt;p>进而，工程师在设计基于 LLMs 的应用时，应该将这部分考虑进去。毕竟，如果&lt;code>int a = 10; print(a);&lt;/code> 的输出一般就是&lt;code>10&lt;/code>。&lt;/p></description></item></channel></rss>