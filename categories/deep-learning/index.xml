<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on Svtter's Blog</title><link>https://svtter.cn/categories/deep-learning/</link><description>Recent content in Deep Learning on Svtter's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sat, 05 Apr 2025 21:51:38 +0800</lastBuildDate><atom:link href="https://svtter.cn/categories/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Diffusion Model.md</title><link>https://svtter.cn/p/diffusion-model.md/</link><pubDate>Sat, 05 Apr 2025 21:51:38 +0800</pubDate><guid>https://svtter.cn/p/diffusion-model.md/</guid><description>&lt;img src="https://svtter.cn/p/diffusion-model.md/noise-dog.png" alt="Featured image of post Diffusion Model.md" />&lt;p>深度学习的魅力在于，一旦有新的任务在某些架构上获得了新的性能，许多其他任务可以参考这个架构，进而从中受益。&lt;/p>
&lt;p>我想，diffusion model 是一个典型的模型。虽然我不进行 diffusion model 的相关研究，目前也没有相关的项目与之有关，但是了解一下这个网络架构并没有坏处。&lt;/p>
&lt;p>diffusion model 是一个从图像处理过程中受益的模型。&lt;/p>
&lt;p>diffusion model 通过学习图像增加噪声的逆过程，从而学习到了从噪声中生成图像的能力。&lt;/p>
&lt;p>&lt;img src="https://svtter.cn/p/diffusion-model.md/noise-dog.png"
width="2992"
height="668"
srcset="https://svtter.cn/p/diffusion-model.md/noise-dog_hu_c99a3a942e0e9506.png 480w, https://svtter.cn/p/diffusion-model.md/noise-dog_hu_ad0c92868aa71fcd.png 1024w"
loading="lazy"
alt="noise-dog"
class="gallery-image"
data-flex-grow="447"
data-flex-basis="1074px"
>&lt;/p></description></item><item><title>Read Code of CLIP.md</title><link>https://svtter.cn/p/read-code-of-clip.md/</link><pubDate>Wed, 19 Mar 2025 13:23:50 +0800</pubDate><guid>https://svtter.cn/p/read-code-of-clip.md/</guid><description>&lt;img src="https://svtter.cn/p/read-code-of-clip.md/image.png" alt="Featured image of post Read Code of CLIP.md" />&lt;p>Contrastive Language-Image Pre-Training (CLIP) 是 openai 的经典工作之一。出自论文&lt;a class="link" href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener"
>&lt;/a>&lt;/p>
&lt;p>为了能够在 CLIP 上完成我的新 idea，我尝试阅读 &lt;a class="link" href="https://github.com/openai/CLIP" target="_blank" rel="noopener"
>openai/clip&lt;/a> 来理解 clip 在 classifier 上的基本工作原理。&lt;/p>
&lt;p>这是 &lt;a class="link" href="https://github.com/openai/CLIP" target="_blank" rel="noopener"
>openai/clip&lt;/a> 给出的 python 样例代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">clip&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">PIL&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Image&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">device&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;cuda&amp;#34;&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_available&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="s2">&amp;#34;cpu&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">preprocess&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">clip&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;ViT-B/32&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">preprocess&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Image&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;CLIP.png&amp;#34;&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">clip&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tokenize&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="s2">&amp;#34;a diagram&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;a dog&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;a cat&amp;#34;&lt;/span>&lt;span class="p">])&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">no_grad&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">image_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode_image&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">text_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode_text&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits_per_image&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">logits_per_text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">probs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">logits_per_image&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cpu&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">numpy&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Label probs:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">probs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># prints: [[0.9927937 0.00421068 0.00299572]]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>load 函数用于加载特定的 openai 模型。这里是基于&lt;code>ViT-B/32&lt;/code>，一个 Vision Transformer 32B。&lt;/p>
&lt;p>可以看到，如果 openai 支持的 vision encoder 大概有如下几种：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">_MODELS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;RN50&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;RN101&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;RN50x4&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;RN50x16&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;RN50x64&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;ViT-B/32&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;ViT-B/16&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;ViT-L/14&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;ViT-L/14@336px&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们假设模型已经下载完成，让我们看看 _tranform 预处理工作是如何进行的：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">_transform&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_px&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">Compose&lt;/span>&lt;span class="p">([&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Resize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_px&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">interpolation&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">BICUBIC&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">CenterCrop&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_px&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_convert_image_to_rgb&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ToTensor&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Normalize&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="mf">0.48145466&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.4578275&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.40821073&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mf">0.26862954&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.26130258&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.27577711&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>也不是很复杂，预处理&lt;code>Normalize&lt;/code>参数虽然不太明白。似乎是用的 ViT 同样的预处理参数。&lt;/p>
&lt;p>然后进入模型加载阶段，我们可以看到，如果不是 &lt;a class="link" href="https://chenglu.me/blogs/pytorch-jit" target="_blank" rel="noopener"
>jit 加载&lt;/a> ，那么模型会选择 state_dict 的模式。
通过加载 state_dict 的过程，我们可以看到 build_model 函数用于加载权重，将权重赋值给已有的模型结构。&lt;/p>
&lt;p>这个模型结构的文件是&lt;a class="link" href="https://github.com/openai/CLIP/blob/main/clip/model.py" target="_blank" rel="noopener"
>model.py&lt;/a>。
因此，CLIP 的主要代码位于&lt;a class="link" href="https://github.com/openai/CLIP/blob/main/clip/model.py#L243" target="_blank" rel="noopener"
>model.py#L243&lt;/a>。&lt;/p>
&lt;p>image_encoder 和 text_encoder 的输出，分别是两个不同的特征 tensor。&lt;/p>
&lt;p>将两个 tensor 进行矩阵乘积，分别得到一个相似性矩阵。这个相似性矩阵的大小是 &lt;code>(batch_size, batch_size)&lt;/code>。&lt;/p>
&lt;blockquote>
&lt;p>TIPS: 如果说 batch-size 太小，为1，那么对比学习的性能可能就大打折扣了。&lt;/p>&lt;/blockquote>
&lt;p>这两个 tensor 使用 symmetric cross-entropy loss 进行计算，来用于更新网络权重。&lt;/p>
&lt;p>专门做智能指标的提升，不太在意计算量。不追求最新最高的智能指标，更加关注模型计算的运行效率。&lt;/p>
&lt;blockquote>
&lt;p>trick: 给参数加一个 log 来使得权重更新没有那么剧烈，计算起来没有那么大。&lt;/p>&lt;/blockquote>
&lt;p>&lt;a class="link" href="https://github.com/openai/CLIP" target="_blank" rel="noopener"
>CLIP&lt;/a> 代码中没有给出能够直接进行训练的代码。下一篇文章，尝试阅读一下 openclip。&lt;/p></description></item><item><title>Torchvision Cuda Problem.md</title><link>https://svtter.cn/p/torchvision-cuda-problem.md/</link><pubDate>Tue, 04 Mar 2025 18:07:13 +0800</pubDate><guid>https://svtter.cn/p/torchvision-cuda-problem.md/</guid><description>&lt;p>记录一下 torchvision 遇到的问题&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">torch.cuda.DeferredCudaCallError: CUDA call failed lazily at initialization with error: device &amp;gt;= 0 &amp;amp;&amp;amp; device &amp;lt; num_gpus INTERNAL ASSERT FAILED at &amp;#34;/opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/cuda/CUDAContext.cpp&amp;#34;:50, please report a bug to PyTorch. device=, num_gpus=
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我调整了代码，遇到了这个问题，结果是因为引入了 &lt;code>from torchvision import transforms&lt;/code>。&lt;/p>
&lt;p>这个问题可能是 nvidia 驱动问题导致的。解决这个问题，最好的方法是改一下 torch 的版本，找到一个和驱动接近的。&lt;/p>
&lt;p>重新安装 nvidia 驱动付出的代价比较大。&lt;/p></description></item><item><title>CNN Size Computing</title><link>https://svtter.cn/p/cnn-size-computing/</link><pubDate>Fri, 11 Oct 2024 16:51:14 +0800</pubDate><guid>https://svtter.cn/p/cnn-size-computing/</guid><description>&lt;p>CNNs (Convolutional Neural Networks) is a amazing component of neural network theory. However, to use it efficiently, we need to compute the output shape.&lt;/p>
&lt;h3 id="ask-for-chatgpt">ask for chatgpt
&lt;/h3>&lt;ul>
&lt;li>&lt;a class="link" href="https://chatgpt.com/share/19be811d-e750-45de-b5bd-ad391c9dba80" target="_blank" rel="noopener"
>https://chatgpt.com/share/19be811d-e750-45de-b5bd-ad391c9dba80&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="input-shape">input shape
&lt;/h3>&lt;p>Ensure that the input to your network is of the correct shape. The input tensor should have the dimensions &lt;code>[batch_size, channels, height, width]&lt;/code>. For example, if you&amp;rsquo;re using a batch size of 1, the input should be &lt;code>[1, 32, 32, 300]&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>1 is batch size.&lt;/li>
&lt;li>&lt;code>input_data = torch.randn(1, 32, 32, 300) # Example input tensor with the correct shape&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="compute-of-cnn-output">Compute of CNN output
&lt;/h3>&lt;ul>
&lt;li>reference: &lt;a class="link" href="https://www.baeldung.com/cs/convolutional-layer-size" target="_blank" rel="noopener"
>baeldung link&lt;/a>
&lt;img src="https://svtter.cn/assets/image_1717928679511_0.png"
loading="lazy"
alt="image.png"
>&lt;/li>
&lt;/ul>
&lt;p>The channels number is the CNN&amp;rsquo;s filters number.&lt;/p></description></item><item><title>Easy LSTM Training Tricks</title><link>https://svtter.cn/p/easy-lstm-training-tricks/</link><pubDate>Mon, 12 Aug 2024 15:35:03 +0800</pubDate><guid>https://svtter.cn/p/easy-lstm-training-tricks/</guid><description>&lt;p>This post introduces how to train the LSTM networks to get correct outputs.&lt;/p>
&lt;p>If you use this way, not work.&lt;/p>
&lt;ul>
&lt;li>Just use the last width, like &lt;code>x[-1, :, :]&lt;/code>, select the last piece of width.&lt;/li>
&lt;li>It&amp;rsquo;s normal way. However, sometimes it will &lt;strong>NOT WORK&lt;/strong>.&lt;/li>
&lt;li>&lt;img src="https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-1.png"
width="744"
height="1140"
srcset="https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-1_hu_ac7f1b95cef41c2a.png 480w, https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-1_hu_90a5c97aafe0bd87.png 1024w"
loading="lazy"
alt="image.png"
class="gallery-image"
data-flex-grow="65"
data-flex-basis="156px"
>&lt;/li>
&lt;/ul>
&lt;p>This way will work.&lt;/p>
&lt;ul>
&lt;li>Flatten all the output&lt;/li>
&lt;li>&lt;img src="https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-2.png"
width="702"
height="1140"
srcset="https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-2_hu_15f30d5916691925.png 480w, https://svtter.cn/p/easy-lstm-training-tricks/pics/lstm-2_hu_30536e8b9ab4dc3b.png 1024w"
loading="lazy"
alt="image.png"
class="gallery-image"
data-flex-grow="61"
data-flex-basis="147px"
>&lt;/li>
&lt;li>It always works.&lt;/li>
&lt;/ul>
&lt;p>Have fun.&lt;/p>
&lt;p>Besides, I found &lt;a class="link" href="https://marimo.io/use-cases#examples" target="_blank" rel="noopener"
>marimo&lt;/a>, which is a replacement for jupyter notebooks.&lt;/p></description></item></channel></rss>