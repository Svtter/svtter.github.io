<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>VLM on Svtter's Blog</title><link>https://svtter.cn/categories/vlm/</link><description>Recent content in VLM on Svtter's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 19 Jun 2025 16:34:32 +0800</lastBuildDate><atom:link href="https://svtter.cn/categories/vlm/index.xml" rel="self" type="application/rss+xml"/><item><title>Poor Performance of Large Models on Specific Tasks</title><link>https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/</link><pubDate>Thu, 19 Jun 2025 16:34:32 +0800</pubDate><guid>https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/</guid><description>&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/bg.png" alt="Featured image of post Poor Performance of Large Models on Specific Tasks" /&gt;&lt;p&gt;视觉大模型在一些具体任务上比较糟糕，对于格式化的文本比较友好。这里我以仪表识别区域的定位作为例子，展示大模型的效果。&lt;/p&gt;
&lt;h2 id="源代码"&gt;源代码
&lt;/h2&gt;&lt;p&gt;&lt;a class="link" href="https://github.com/Svtter/vl-model/pull/4" target="_blank" rel="noopener"
&gt;https://github.com/Svtter/vl-model/pull/4&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="测试的任务"&gt;测试的任务
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;将图片中的文本 boxes 提取出来。&lt;/li&gt;
&lt;li&gt;将图片中的仪表读数区域提取出来。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="测试的文件"&gt;测试的文件
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/meter-2.jpg"
width="1280"
height="1707"
srcset="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/meter-2_hu_c88be12cb06d2945.jpg 480w, https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/meter-2_hu_791a35b85a8e04bb.jpg 1024w"
loading="lazy"
alt="Original Meter"
class="gallery-image"
data-flex-grow="74"
data-flex-basis="179px"
&gt;&lt;/p&gt;
&lt;p&gt;我们可以从这个测试结果中看出不同模型的表现差异：&lt;/p&gt;
&lt;h2 id="测试结果对比"&gt;测试结果对比
&lt;/h2&gt;&lt;h3 id="bounding-boxes-作为提示词的结果"&gt;bounding boxes 作为提示词的结果
&lt;/h3&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image.png"
width="1280"
height="1707"
srcset="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_hu_21b7440b53011ad6.png 480w, https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_hu_e002cc344d0e25da.png 1024w"
loading="lazy"
alt="整体测试结果"
class="gallery-image"
data-flex-grow="74"
data-flex-basis="179px"
&gt;&lt;/p&gt;
&lt;h3 id="各模型详细表现"&gt;各模型详细表现
&lt;/h3&gt;&lt;h4 id="anthropic-claude-35-sonnet"&gt;Anthropic Claude 3.5 Sonnet
&lt;/h4&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_anthropic_claude-3.5-sonnet.png"
width="187"
height="56"
srcset="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_anthropic_claude-3.5-sonnet_hu_21e838496ec13e0c.png 480w, https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_anthropic_claude-3.5-sonnet_hu_98d3433b78049bde.png 1024w"
loading="lazy"
alt="Claude 3.5 Sonnet 测试结果"
class="gallery-image"
data-flex-grow="333"
data-flex-basis="801px"
&gt;&lt;/p&gt;
&lt;h4 id="google-gemini-25-pro"&gt;Google Gemini 2.5 Pro
&lt;/h4&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_google_gemini-2.5-pro.png"
width="690"
height="142"
srcset="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_google_gemini-2.5-pro_hu_eb7a36a86550fa2e.png 480w, https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_google_gemini-2.5-pro_hu_c6c68f3c0be13424.png 1024w"
loading="lazy"
alt="Gemini 2.5 Pro 测试结果"
class="gallery-image"
data-flex-grow="485"
data-flex-basis="1166px"
&gt;&lt;/p&gt;
&lt;h4 id="openai-gpt-4o"&gt;OpenAI GPT-4o
&lt;/h4&gt;&lt;p&gt;&lt;img src="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_openai_gpt-4o.png"
width="120"
height="60"
srcset="https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_openai_gpt-4o_hu_66da02220ce9dd8e.png 480w, https://svtter.cn/p/poor-performance-of-large-models-on-specific-tasks/pics/cropped_image_openai_gpt-4o_hu_895477318e72aaf7.png 1024w"
loading="lazy"
alt="GPT-4o 测试结果"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
&gt;&lt;/p&gt;
&lt;h2 id="分析总结"&gt;分析总结
&lt;/h2&gt;&lt;p&gt;从这些测试结果可以看出：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;视觉识别能力差异&lt;/strong&gt;：不同模型在处理相同视觉任务时表现出明显的性能差异&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;格式化文本处理&lt;/strong&gt;：相比视觉任务，模型在处理结构化文本时表现更加稳定&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型特性&lt;/strong&gt;：每个模型都有其独特的优势和局限性&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些结果提醒我们在选择 AI 模型时需要根据具体任务类型来评估其适用性。&lt;/p&gt;</description></item></channel></rss>